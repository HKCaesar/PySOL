{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyopencl as cl\n",
    "import numpy\n",
    "import numpy.linalg as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:1: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:2: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 745.42346)\n"
     ]
    }
   ],
   "source": [
    "a = numpy.random.rand(5e6).astype(numpy.float32)\n",
    "b = numpy.random.rand(5e6).astype(numpy.float32)\n",
    "\n",
    "ctx = cl.create_some_context()\n",
    "queue = cl.CommandQueue(ctx)\n",
    "\n",
    "mf = cl.mem_flags\n",
    "a_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=a)\n",
    "b_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=b)\n",
    "dest_buf = cl.Buffer(ctx, mf.WRITE_ONLY, b.nbytes)\n",
    "\n",
    "prg = cl.Program(ctx, \"\"\"\n",
    "    __kernel void sum(__global const float *a,\n",
    "    __global const float *b, __global float *c)\n",
    "    {\n",
    "      int gid = get_global_id(0);\n",
    "      c[gid] = a[gid] * b[gid];\n",
    "    }\n",
    "    \"\"\").build()\n",
    "\n",
    "prg.sum(queue, a.shape, None, a_buf, b_buf, dest_buf)\n",
    "\n",
    "a_plus_b = numpy.empty_like(a)\n",
    "cl.enqueue_copy(queue, a_plus_b, dest_buf)\n",
    "\n",
    "print(la.norm(a_plus_b - (a*b)), la.norm(a_plus_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU push+compute+pull total [s]: 0.0129519701004\n",
      "GPU push [s]: 0.00250697135925\n",
      "GPU pull [s]: 0.00178384780884\n",
      "GPU compute (host-timed) [s]: 0.00866115093231\n",
      "GPU compute (event-timed) [s]:  0.008645344\n",
      "\n",
      "GFlops/s: 236.458181598\n",
      "\n",
      "GPU==CPU: True\n",
      "\n",
      "CPU time (s) 0.137889862061\n",
      "\n",
      "GPU speedup (with transfer):  10.6462461689\n",
      "GPU speedup (without transfer):  15.9205009979\n"
     ]
    }
   ],
   "source": [
    "# example provided by Eilif Muller\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "KERNEL_CODE = \"\"\"\n",
    "\n",
    "// Thread block size\n",
    "#define BLOCK_SIZE %(block_size)d\n",
    "\n",
    "// Matrix dimensions\n",
    "// (chosen as multiples of the thread block size for simplicity)\n",
    "#define WA %(w_a)d // Matrix A width\n",
    "#define HA %(h_a)d // Matrix A height\n",
    "#define WB %(w_b)d // Matrix B width\n",
    "#define HB WA  // Matrix B height\n",
    "#define WC WB  // Matrix C width\n",
    "#define HC HA  // Matrix C height\n",
    "\n",
    "\n",
    "/*\n",
    " * Copyright 1993-2009 NVIDIA Corporation.  All rights reserved.\n",
    " *\n",
    " * NVIDIA Corporation and its licensors retain all intellectual property and\n",
    " * proprietary rights in and to this software and related documentation.\n",
    " * Any use, reproduction, disclosure, or distribution of this software\n",
    " * and related documentation without an express license agreement from\n",
    " * NVIDIA Corporation is strictly prohibited.\n",
    " *\n",
    " * Please refer to the applicable NVIDIA end user license agreement (EULA)\n",
    " * associated with this source code for terms and conditions that govern\n",
    " * your use of this NVIDIA software.\n",
    " *\n",
    " */\n",
    "\n",
    "/* Matrix multiplication: C = A * B.\n",
    " * Device code.\n",
    " */\n",
    "\n",
    "#define AS(j, i) As[i + j * BLOCK_SIZE]\n",
    "#define BS(j, i) Bs[i + j * BLOCK_SIZE]\n",
    "\n",
    "////////////////////////////////////////////////////////////////////////////////\n",
    "//! Matrix multiplication on the device: C = A * B\n",
    "//! WA is A's width and WB is B's width\n",
    "////////////////////////////////////////////////////////////////////////////////\n",
    "__kernel __attribute__((reqd_work_group_size(BLOCK_SIZE,BLOCK_SIZE,1))) \n",
    "void\n",
    "matrixMul( __global float* C, __global float* A, __global float* B)\n",
    "{\n",
    "    __local float As[BLOCK_SIZE*BLOCK_SIZE];\n",
    "    __local float Bs[BLOCK_SIZE*BLOCK_SIZE];\n",
    "\n",
    "    // Block index\n",
    "    int bx = get_group_id(0);\n",
    "    int by = get_group_id(1);\n",
    "\n",
    "    // Thread index\n",
    "    int tx = get_local_id(0);\n",
    "    int ty = get_local_id(1);\n",
    "\n",
    "    // Index of the first sub-matrix of A processed by the block\n",
    "    int aBegin = WA * BLOCK_SIZE * by;\n",
    "\n",
    "    // Index of the last sub-matrix of A processed by the block\n",
    "    int aEnd   = aBegin + WA - 1;\n",
    "\n",
    "    // Step size used to iterate through the sub-matrices of A\n",
    "    int aStep  = BLOCK_SIZE;\n",
    "\n",
    "    // Index of the first sub-matrix of B processed by the block\n",
    "    int bBegin = BLOCK_SIZE * bx;\n",
    "\n",
    "    // Step size used to iterate through the sub-matrices of B\n",
    "    int bStep  = BLOCK_SIZE * WB;\n",
    "\n",
    "    // Csub is used to store the element of the block sub-matrix\n",
    "    // that is computed by the thread\n",
    "    float Csub = 0.0f;\n",
    "\n",
    "    // Loop over all the sub-matrices of A and B\n",
    "    // required to compute the block sub-matrix\n",
    "    for (int a = aBegin, b = bBegin;\n",
    "             a <= aEnd;\n",
    "             a += aStep, b += bStep) {\n",
    "\n",
    "        // Load the matrices from device memory\n",
    "        // to shared memory; each thread loads\n",
    "        // one element of each matrix\n",
    "        AS(ty, tx) = A[a + WA * ty + tx];\n",
    "        BS(ty, tx) = B[b + WB * ty + tx];\n",
    "\n",
    "        // Synchronize to make sure the matrices are loaded\n",
    "        barrier(CLK_LOCAL_MEM_FENCE);\n",
    "\n",
    "        // Multiply the two matrices together;\n",
    "        // each thread computes one element\n",
    "        // of the block sub-matrix\n",
    "        for (int k = 0; k < BLOCK_SIZE; ++k)\n",
    "            Csub += AS(ty, k) * BS(k, tx);\n",
    "\n",
    "        // Synchronize to make sure that the preceding\n",
    "        // computation is done before loading two new\n",
    "        // sub-matrices of A and B in the next iteration\n",
    "        barrier(CLK_LOCAL_MEM_FENCE);\n",
    "    }\n",
    "\n",
    "    // Write the block sub-matrix to device memory;\n",
    "    // each thread writes one element\n",
    "    C[get_global_id(1) * get_global_size(0) + get_global_id(0)] = Csub;\n",
    "\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pyopencl as cl\n",
    "from time import time\n",
    "import numpy\n",
    "\n",
    "block_size = 16\n",
    "\n",
    "ctx = cl.create_some_context()\n",
    "\n",
    "for dev in ctx.devices:\n",
    "    assert dev.local_mem_size > 0\n",
    "\n",
    "queue = cl.CommandQueue(ctx,\n",
    "        properties=cl.command_queue_properties.PROFILING_ENABLE)\n",
    "\n",
    "#queue = cl.CommandQueue(ctx)\n",
    "\n",
    "if False:\n",
    "    a_height = 4096*4\n",
    "    #a_height = 1024\n",
    "    a_width = 4096*4\n",
    "    #a_width = 256\n",
    "    #b_height == a_width\n",
    "    b_width = a_height\n",
    "\n",
    "elif False:\n",
    "    # like PyCUDA\n",
    "    a_height = 4096*4\n",
    "    a_width = 4096*4\n",
    "    b_height = a_width\n",
    "    b_width = 2144\n",
    "\n",
    "else:\n",
    "    # CL SDK\n",
    "    a_width = 50*block_size\n",
    "    a_height = 100*block_size\n",
    "    b_width = 50*block_size\n",
    "    b_height = a_width\n",
    "\n",
    "c_width = b_width\n",
    "c_height = a_height\n",
    "\n",
    "h_a = numpy.random.rand(a_height, a_width).astype(numpy.float32)\n",
    "h_b = numpy.random.rand(b_height, b_width).astype(numpy.float32)\n",
    "h_c = numpy.empty((c_height, c_width)).astype(numpy.float32)\n",
    "\n",
    "\n",
    "kernel_params = {\"block_size\": block_size,\n",
    "        \"w_a\":a_width, \"h_a\":a_height, \"w_b\":b_width}\n",
    "\n",
    "if \"NVIDIA\" in queue.device.vendor:\n",
    "    options = \"-cl-mad-enable -cl-fast-relaxed-math\"\n",
    "else:\n",
    "    options = \"\"\n",
    "prg = cl.Program(ctx, KERNEL_CODE % kernel_params,\n",
    "        ).build(options=options)\n",
    "kernel = prg.matrixMul\n",
    "#print prg.binaries[0]\n",
    "\n",
    "assert a_width % block_size == 0\n",
    "assert a_height % block_size == 0\n",
    "assert b_width % block_size == 0\n",
    "\n",
    "# transfer host -> device -----------------------------------------------------\n",
    "mf = cl.mem_flags\n",
    "\n",
    "t1 = time()\n",
    "\n",
    "d_a_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=h_a)\n",
    "d_b_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=h_b)\n",
    "d_c_buf = cl.Buffer(ctx, mf.WRITE_ONLY, size=h_c.nbytes)\n",
    "\n",
    "push_time = time()-t1\n",
    "\n",
    "# warmup ----------------------------------------------------------------------\n",
    "for i in range(5):\n",
    "    event = kernel(queue, h_c.shape[::-1], (block_size, block_size), \n",
    "            d_c_buf, d_a_buf, d_b_buf)\n",
    "    event.wait()\n",
    "\n",
    "queue.finish()\n",
    "\n",
    "# actual benchmark ------------------------------------------------------------\n",
    "t1 = time()\n",
    "\n",
    "count = 20\n",
    "for i in range(count):\n",
    "    event = kernel(queue, h_c.shape[::-1], (block_size, block_size),\n",
    "            d_c_buf, d_a_buf, d_b_buf)\n",
    "\n",
    "event.wait()\n",
    "\n",
    "gpu_time = (time()-t1)/count\n",
    "\n",
    "# transfer device -> host -----------------------------------------------------\n",
    "t1 = time()\n",
    "cl.enqueue_copy(queue, h_c, d_c_buf)\n",
    "pull_time = time()-t1\n",
    "\n",
    "# timing output ---------------------------------------------------------------\n",
    "gpu_total_time = gpu_time+push_time+pull_time\n",
    "\n",
    "print \"GPU push+compute+pull total [s]:\", gpu_total_time\n",
    "print \"GPU push [s]:\", push_time\n",
    "print \"GPU pull [s]:\", pull_time\n",
    "print \"GPU compute (host-timed) [s]:\", gpu_time\n",
    "print \"GPU compute (event-timed) [s]: \", (event.profile.end-event.profile.start)*1e-9\n",
    "\n",
    "gflop = h_c.size * (a_width * 2.) / (1000**3.)\n",
    "gflops = gflop / gpu_time\n",
    "\n",
    "print\n",
    "print \"GFlops/s:\", gflops\n",
    "\n",
    "# cpu comparison --------------------------------------------------------------\n",
    "t1 = time()\n",
    "h_c_cpu = numpy.dot(h_a,h_b)\n",
    "cpu_time = time()-t1\n",
    "\n",
    "print\n",
    "print \"GPU==CPU:\",numpy.allclose(h_c, h_c_cpu)\n",
    "print\n",
    "print \"CPU time (s)\", cpu_time\n",
    "print\n",
    "\n",
    "print \"GPU speedup (with transfer): \", cpu_time/gpu_total_time\n",
    "print \"GPU speedup (without transfer): \", cpu_time/gpu_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import reikna.cluda as cluda\n",
    "\n",
    "N = 512\n",
    "\n",
    "api = cluda.ocl_api()\n",
    "thr = api.Thread.create()\n",
    "\n",
    "program = thr.compile(\"\"\"\n",
    "KERNEL void multiply_them(\n",
    "    GLOBAL_MEM float *dest,\n",
    "    GLOBAL_MEM float *a,\n",
    "    GLOBAL_MEM float *b)\n",
    "{\n",
    "  const SIZE_T i = get_local_id(0);\n",
    "  dest[i] = a[i] * b[i];\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "multiply_them = program.multiply_them\n",
    "\n",
    "a = numpy.random.randn(N).astype(numpy.float32)\n",
    "b = numpy.random.randn(N).astype(numpy.float32)\n",
    "a_dev = thr.to_device(a)\n",
    "b_dev = thr.to_device(b)\n",
    "dest_dev = thr.empty_like(a_dev)\n",
    "\n",
    "multiply_them(dest_dev, a_dev, b_dev, local_size=N, global_size=N)\n",
    "print((dest_dev.get() - a * b == 0).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time of test without OpenCL:  0.0482218265533 s\n",
      "===============================================================\n",
      "Platform name: NVIDIA CUDA\n",
      "Platform profile: FULL_PROFILE\n",
      "Platform vendor: NVIDIA Corporation\n",
      "Platform version: OpenCL 1.1 CUDA 7.0.28\n",
      "---------------------------------------------------------------\n",
      "Device name: Tesla M2090\n",
      "Device type: GPU\n",
      "Device memory:  5375 MB\n",
      "Device max clock speed: 1301 MHz\n",
      "Device compute units: 16\n",
      "Device max work group size: 1024\n",
      "Device max work item sizes: [1024, 1024, 64]\n",
      "Data points: 8388608\n",
      "Workers: 256\n",
      "Preferred work group size multiple: 32\n",
      "Execution time of test: 0.000787584 s\n",
      "Results OK\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pyopencl as cl\n",
    "import numpy\n",
    "import numpy.linalg as la\n",
    "import datetime\n",
    "from time import time\n",
    "\n",
    "data_points = 2**23 # ~8 million data points, ~32 MB data\n",
    "workers = 2**8 # 256 workers, play with this to see performance differences\n",
    "               # eg: 2**0 => 1 worker will be non-parallel execution on gpu\n",
    "               # data points must be a multiple of workers\n",
    "\n",
    "a = numpy.random.rand(data_points).astype(numpy.float32)\n",
    "b = numpy.random.rand(data_points).astype(numpy.float32)\n",
    "c_result = numpy.empty_like(a)\n",
    "\n",
    "# Speed in normal CPU usage\n",
    "time1 = time()\n",
    "c_temp = (a+b) # adds each element in a to its corresponding element in b\n",
    "c_temp = c_temp * c_temp # element-wise multiplication\n",
    "c_result = c_temp * (a/2.0) # element-wise half a and multiply\n",
    "time2 = time()\n",
    "\n",
    "print(\"Execution time of test without OpenCL: \", time2 - time1, \"s\")\n",
    "\n",
    "\n",
    "for platform in cl.get_platforms():\n",
    "    for device in platform.get_devices():\n",
    "        print(\"===============================================================\")\n",
    "        print(\"Platform name:\", platform.name)\n",
    "        print(\"Platform profile:\", platform.profile)\n",
    "        print(\"Platform vendor:\", platform.vendor)\n",
    "        print(\"Platform version:\", platform.version)\n",
    "        print(\"---------------------------------------------------------------\")\n",
    "        print(\"Device name:\", device.name)\n",
    "        print(\"Device type:\", cl.device_type.to_string(device.type))\n",
    "        print(\"Device memory: \", device.global_mem_size//1024//1024, 'MB')\n",
    "        print(\"Device max clock speed:\", device.max_clock_frequency, 'MHz')\n",
    "        print(\"Device compute units:\", device.max_compute_units)\n",
    "        print(\"Device max work group size:\", device.max_work_group_size)\n",
    "        print(\"Device max work item sizes:\", device.max_work_item_sizes)\n",
    "\n",
    "        # Simnple speed test\n",
    "        ctx = cl.Context([device])\n",
    "        queue = cl.CommandQueue(ctx, \n",
    "                properties=cl.command_queue_properties.PROFILING_ENABLE)\n",
    "\n",
    "        mf = cl.mem_flags\n",
    "        a_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=a)\n",
    "        b_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=b)\n",
    "        dest_buf = cl.Buffer(ctx, mf.WRITE_ONLY, b.nbytes)\n",
    "\n",
    "        prg = cl.Program(ctx, \"\"\"\n",
    "            __kernel void sum(__global const float *a,\n",
    "            __global const float *b, __global float *c)\n",
    "            {\n",
    "                        int gid = get_global_id(0);\n",
    "                        float a_temp;\n",
    "                        float b_temp;\n",
    "                        float c_temp;\n",
    "\n",
    "                        a_temp = a[gid]; // my a element (by global ref)\n",
    "                        b_temp = b[gid]; // my b element (by global ref)\n",
    "                        \n",
    "                        c_temp = a_temp+b_temp; // sum of my elements\n",
    "                        c_temp = c_temp * c_temp; // product of sums\n",
    "                        c_temp = c_temp * (a_temp/2.0); // times 1/2 my a\n",
    "\n",
    "                        c[gid] = c_temp; // store result in global memory\n",
    "                }\n",
    "                \"\"\").build()\n",
    "\n",
    "        global_size=(data_points,)\n",
    "        local_size=(workers,)\n",
    "        preferred_multiple = cl.Kernel(prg, 'sum').get_work_group_info( \\\n",
    "            cl.kernel_work_group_info.PREFERRED_WORK_GROUP_SIZE_MULTIPLE, \\\n",
    "            device)\n",
    "\n",
    "        print(\"Data points:\", data_points)\n",
    "        print(\"Workers:\", workers)\n",
    "        print(\"Preferred work group size multiple:\", preferred_multiple)\n",
    "\n",
    "        if (workers % preferred_multiple):\n",
    "            print(\"Number of workers not a preferred multiple (%d*N).\" \\\n",
    "                    % (preferred_multiple))\n",
    "            print(\"Performance may be reduced.\")\n",
    "\n",
    "        exec_evt = prg.sum(queue, global_size, local_size, a_buf, b_buf, dest_buf)\n",
    "        exec_evt.wait()\n",
    "        elapsed = 1e-9*(exec_evt.profile.end - exec_evt.profile.start)\n",
    "\n",
    "        print(\"Execution time of test: %g s\" % elapsed)\n",
    "\n",
    "        c = numpy.empty_like(a)\n",
    "        cl.enqueue_read_buffer(queue, dest_buf, c).wait()\n",
    "        equal = numpy.all( c == c_result)\n",
    "\n",
    "        if not equal:\n",
    "                print(\"Results doesn't match!!\")\n",
    "        else:\n",
    "                print(\"Results OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# example by Roger Pau Monn'e\n",
    "import pyopencl as cl\n",
    "import numpy as np\n",
    "\n",
    "demo_r = np.empty( (500,5), dtype=np.uint32)\n",
    "ctx = cl.create_some_context()\n",
    "queue = cl.CommandQueue(ctx)\n",
    "\n",
    "mf = cl.mem_flags\n",
    "demo_buf = cl.Buffer(ctx, mf.WRITE_ONLY, demo_r.nbytes)\n",
    "\n",
    "prg = cl.Program(ctx,\n",
    "\"\"\"\n",
    "__kernel void demo(__global uint *demo)\n",
    "{\n",
    "    int i;\n",
    "    int gid = get_global_id(0);\n",
    "    for(i=0; i<5;i++)\n",
    "    {\n",
    "        demo[gid*5+i] = (uint) 1;\n",
    "    }\n",
    "}\"\"\")\n",
    "\n",
    "try:\n",
    "    prg.build()\n",
    "except:\n",
    "    print(\"Error:\")\n",
    "    print(prg.get_build_info(ctx.devices[0], cl.program_build_info.LOG))\n",
    "    raise\n",
    "\n",
    "prg.demo(queue, (500,), None, demo_buf)\n",
    "cl.enqueue_read_buffer(queue, demo_buf, demo_r).wait()\n",
    "\n",
    "for res in demo_r:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
