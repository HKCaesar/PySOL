{
 "metadata": {
  "name": "",
  "signature": "sha256:48567225d1b47f4bb7f2060d9d0d61d5f7834e58ad1ae49d6419f05b2adbfe63"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from SSMI import SSMIHandler\n",
      "from xml.dom import minidom\n",
      "import os\n",
      "import urllib\n",
      "import time\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "export_path = '/nfs/http/posada/quicklooks'\n",
      "# export_path = '/tmp/'\n",
      "product_name = 'SSMI_NC'\n",
      "daily = 'daily/data'\n",
      "proj = \"epsg_4326\"\n",
      "pp_names = ['atmosphere_cloud_liquid_water_content','wind_speed','atmosphere_water_vapor_content']\n",
      "pp_name = 'atmosphere_water_vapor_content'\n",
      "# callback_url = 'http://85.142.104.215:9000/'\n",
      "callback_url = None\n",
      "\n",
      "def parse():\n",
      "    import argparse\n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument('--year', type=unicode, dest='year', help=\"year to generate\")\n",
      "    parser.add_argument('--output', type=unicode, dest='export_path', help=\"export_path to generate\",\n",
      "                        default=export_path)\n",
      "    parser.add_argument('--offset', type=int, dest='offset', help=\"offset granules to generate\",\n",
      "                        default=0)\n",
      "    parser.add_argument('--f_name', type=str, dest='select_f', help=\"f_name granules to generate\",\n",
      "                        default='')\n",
      "    return parser\n",
      "\n",
      "parser = parse()\n",
      "args = parser.parse_args()\n",
      "\n",
      "years_gen = unicode(args.year).split(',')\n",
      "\n",
      "catalog = 'http://opendap.solab.rshu.ru:8080/opendap/hyrax/allData/%s' % product_name\n",
      "catalogxml = os.path.join(catalog, 'catalog.xml')\n",
      "print catalogxml\n",
      "\n",
      "xmldoc = minidom.parse(urllib.urlopen(catalogxml))\n",
      "itemlist = xmldoc.getElementsByTagName('thredds:catalogRef')\n",
      "print len(itemlist)\n",
      "\n",
      "handler_obj = SSMIHandler()\n",
      "job_id = 100\n",
      "count = 1\n",
      "glob_job_dict = None\n",
      "\n",
      "for f_item in itemlist:\n",
      "    f_name = f_item.attributes['name'].value\n",
      "\n",
      "    if (not f_name.startswith('f')) or (len(f_name) != 3):\n",
      "        continue\n",
      "\n",
      "    if unicode(args.select_f):\n",
      "        if (f_name != unicode(args.select_f)):\n",
      "            continue\n",
      "\n",
      "    print '\\nStart f: %s' %f_name\n",
      "    daily_cat = os.path.join(catalog, f_name, daily)\n",
      "    print daily_cat\n",
      "    daily_xml = os.path.join(daily_cat, 'catalog.xml')\n",
      "    xmldoc = minidom.parse(urllib.urlopen(daily_xml))\n",
      "    years_list = xmldoc.getElementsByTagName('thredds:catalogRef')\n",
      "\n",
      "    dataset_starts_with = \"%s_ssmi\" %f_name\n",
      "\n",
      "    for year in years_list:\n",
      "        year_name = year.attributes['name'].value\n",
      "        print year_name\n",
      "\n",
      "        if not year_name.isdigit():\n",
      "            continue\n",
      "\n",
      "        if not year_name in years_gen and years_gen:\n",
      "            continue\n",
      "\n",
      "        print '\\n\\nStart year: %s' %year_name\n",
      "        year_cat = os.path.join(daily_cat, year_name)\n",
      "        year_xml = os.path.join(year_cat, 'catalog.xml')\n",
      "\n",
      "        yeardoc = minidom.parse(urllib.urlopen(year_xml))\n",
      "        datasets_item_list = yeardoc.getElementsByTagName('thredds:dataset')\n",
      "#     for day in days_item_list:\n",
      "#         day_name = day.attributes['name'].value\n",
      "#         if not day_name.isdigit():\n",
      "#             continue\n",
      "#         print '\\nStart day: %s' %day_name\n",
      "\n",
      "#         day_cat = os.path.join(year_cat, day_name)\n",
      "#         day_xml = os.path.join(day_cat, 'catalog.xml')\n",
      "#         daydoc = minidom.parse(urllib.urlopen(day_xml))\n",
      "#         datasets_item_list = daydoc.getElementsByTagName('thredds:dataset')\n",
      "\n",
      "        for _dataset in datasets_item_list:\n",
      "\n",
      "            dataset_name = _dataset.attributes['name'].value\n",
      "            if dataset_name.startswith(dataset_starts_with) and \\\n",
      "                                (dataset_name.endswith('.nc') or dataset_name.endswith('.nc')):\n",
      "                # print dataset_name\n",
      "\n",
      "                if count < args.offset:\n",
      "                    count += 1\n",
      "                    continue\n",
      "\n",
      "                print \"\\nstart %d granule\\n\" %count\n",
      "                dataset_date = dataset_name[9:][:8]\n",
      "                _year = dataset_date[:4]\n",
      "                _month = dataset_date[4:6]\n",
      "                _day = dataset_date[-2:]\n",
      "\n",
      "                granule_date = '%s-%s-%s' %(_year, _month, _day)\n",
      "                granule_path = os.path.join(year_cat, dataset_name)\n",
      "                print 'Start granule: %s' %granule_path\n",
      "\n",
      "                output_dir = '%s/%s/%s/%s/%s/%s/%s/%s' % (unicode(args.export_path), product_name, pp_name, proj,\n",
      "                                                          _year, _month, _day, dataset_name)\n",
      "\n",
      "                if os.path.isdir(output_dir):\n",
      "                    savepath = '%s/%s_ascending.png'%(output_dir, os.path.basename(dataset_name))\n",
      "                    if os.path.isfile(savepath):\n",
      "                        count += 1\n",
      "                        continue\n",
      "\n",
      "                try:\n",
      "                    handler_obj.createClippedImage((granule_path,), pp_names, unicode(args.export_path),\n",
      "                                               callback_url, product_name, granule_date, glob_job_dict)\n",
      "                except:\n",
      "                    print '==> 1 ERROR'\n",
      "                    time.sleep(15)\n",
      "                    try:\n",
      "                        handler_obj.createClippedImage((granule_path,), pp_names, unicode(args.export_path),\n",
      "                                               callback_url, product_name, granule_date, glob_job_dict)\n",
      "                    except:\n",
      "                        print '==> 2 ERROR'\n",
      "                        time.sleep(15)\n",
      "                        try:\n",
      "                            handler_obj.createClippedImage((granule_path,), pp_names, unicode(args.export_path),\n",
      "                                               callback_url, product_name, granule_date, glob_job_dict)\n",
      "                        except:\n",
      "                            print '==> 3 ERROR'\n",
      "                            count += 1\n",
      "                            continue\n",
      "\n",
      "                print \"finish %d granules\\n\" %count\n",
      "\n",
      "                count += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}