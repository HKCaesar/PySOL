{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'readS1' from 'readS1.pyc'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from readS1 import *\n",
    "import readS1\n",
    "reload(readS1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'gshhs_rasterize' from '/home/mag/Documents/repos/solab/PySOL/gshhs_rasterize.pyc'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gshhs_rasterize\n",
    "reload(gshhs_rasterize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from numpy import asarray, zeros, reshape, double, arange, \\\n",
    "                  ma, log10, diff, mean, flipud, floor, pi, sqrt, size, fliplr, meshgrid, exp, cos, radians, \\\n",
    "                  empty, NaN, nanmin, nanstd, round\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import argrelmin, argrelmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from IPython.html import widgets\n",
    "from ipywidgets import widgets\n",
    "# [widget for widget in dir(widgets) if widget.endswith('Widget')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.mlab import find\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import pyresample as pr\n",
    "\n",
    "import os\n",
    "\n",
    "__author__   = 'Alexander Myasoedov'\n",
    "__email__    = 'mag@rshu.ru'\n",
    "__created__  = datetime.datetime(2014, 10, 28)\n",
    "__modified__ = datetime.datetime(2015, 11, 23)\n",
    "__version__  = \"1.0\"\n",
    "__status__   = \"Development\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First we average all the data without masking"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# СДЕЛАТЬ маску земли scale=10,  а потом проинтерполировать на scale=1, чтобы было быстрее\n",
    "# Пока не используем маску земли, поскольку Термальный Шум заведомо ниже над Океаном"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Uncomment to initiate the dict for further averaged values\n",
    "if not 'rawCountsAvg' in locals():\n",
    "    rawCountsAvg = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for _dir, sub_dir, fileNameList in os.walk('./dumps'):\n",
    "    for fn in fileNameList:\n",
    "        if fn.startswith('rawCountsAvg_bckp') and fn.endswith('.pickle'):\n",
    "            with open(_dir+'/'+fn, 'rb') as f:\n",
    "                _rawCountsAvg = pickle.load(f)\n",
    "                rawCountsAvg.update(_rawCountsAvg)\n",
    "                f.close()\n",
    "                del _rawCountsAvg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling the raw_counts array from all of the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    There are 4 modes of operation for Sentinel-1\n",
    "\n",
    "• Strip Map (SM): 80 km swath, 5 x 5 m spatial resolution\n",
    "\n",
    "• Interferometric Wide Swath (IW): 250 km swath, 5 x 20 m spatial resolution\n",
    "\n",
    "• Extra-Wide Swath (EW): 400 km swath, 20 x 40 m spatial resolution\n",
    "\n",
    "• Wave (WV): 20 x 20 km, 5 x 5 m spatial resolution\n",
    "\n",
    "    We fill the raw_counts array from all of the files of every mode separately, depending on the 'incidenceAngle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', Image.DecompressionBombWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inpath = '/media/SOLabNFS2/store/satellite/sentinel-1/'\n",
    "# inpath = '/media/SOLabNFS2/tmp/different_SAR/sentinel-1/Svalbard-Barents/'\n",
    "\n",
    "for _dir, sub_dir, fileNameList in os.walk(inpath):\n",
    "    for fn in fileNameList:\n",
    "#         if fn.startswith('S1A_SM_GRD') and fn.endswith('.zip'):\n",
    "#         if fn.startswith('S1A_IW_GRD') and fn.endswith('.zip'):\n",
    "        if fn.startswith('S1A_EW_GRD') and fn.endswith('.zip'):\n",
    "#         if fn.startswith('S1A_WV_GRD') and fn.endswith('.zip'):\n",
    "            if fn[-57:-55] == 'DV':\n",
    "                polarization = ['vv', 'vh']\n",
    "            elif fn[-57:-55] == 'DH':\n",
    "                polarization = ['hh', 'hv']\n",
    "            elif fn[-57:-55] == 'SV':\n",
    "                polarization = 'vv'\n",
    "            elif fn[-57:-55] == 'SH':\n",
    "                polarization = 'hh'\n",
    "\n",
    "            # Average raw_counts for both polarizations and append to mat file\n",
    "            # from numpy import empty\n",
    "            # sz = raw_counts[polarization[0]].shape\n",
    "            # rawCountsAvg[p] = empty((1, sz[0]), dtype='object')\n",
    "\n",
    "            # For example, if five students sit a test, and the scores are 24, 85, 89, 91 and 95, the mean score is 60.6.\n",
    "            # This, however, is untypical — the average has been dragged down by one outlying score of 24,\n",
    "            # possibly because one student had not been studying.\n",
    "            # In this case, the median of 89 is much more typical.\n",
    "            \n",
    "            # First check if some of the files were not fully processed already\n",
    "            if fn[:-39] in rawCountsAvg and 'skip' in rawCountsAvg[fn[:-39]] and rawCountsAvg[fn[:-39]]['skip'] == 'skip':\n",
    "                continue\n",
    "            if len(polarization[0])==1: # if only 1 polarization:\n",
    "                if ((fn[:-39] not in rawCountsAvg) or \\\n",
    "                ('incidenceAngle' not in rawCountsAvg[fn[:-39]]) or \\\n",
    "                ('mean' not in rawCountsAvg[fn[:-39]][polarization]) or \\\n",
    "                ('median' not in rawCountsAvg[fn[:-39]][polarization])):\n",
    "                    print \"Processing %s\" % fn\n",
    "                    try:\n",
    "                        raw_counts, _, _, GEOgrid= readS1.readS1_raw_counts(inpath=inpath, fn=fn)\n",
    "                    except Exception:\n",
    "                        print Exception.message\n",
    "                        continue\n",
    "                    rawCountsAvg[fn[:-39]] = {}\n",
    "                    rawCountsAvg[fn[:-39]][polarization] = {}\n",
    "                    rawCountsAvg[fn[:-39]]['incidenceAngle'] = interp1d(GEOgrid['pixel'][0,:], GEOgrid['incidenceAngle'][0,:])(arange(raw_counts[polarization].shape[1]))\n",
    "                    rawCountsAvg[fn[:-39]][polarization]['mean']   = ma.mean  (double(raw_counts[polarization]), axis=0)\n",
    "                    rawCountsAvg[fn[:-39]][polarization]['median'] = ma.median(double(raw_counts[polarization]), axis=0)\n",
    "                else:\n",
    "                    # print \"Alredy processed %s\" % fn\n",
    "                    continue\n",
    "            else:\n",
    "                if ((fn[:-39] not in rawCountsAvg) or \\\n",
    "                not rawCountsAvg[fn[:-39]].has_key(polarization[0]) or \\\n",
    "                not rawCountsAvg[fn[:-39]].has_key(polarization[1]) or \\\n",
    "                ('mean' not in rawCountsAvg[fn[:-39]][polarization[0]]) or \\\n",
    "                ('median' not in rawCountsAvg[fn[:-39]][polarization[0]]) or \\\n",
    "                ('mean' not in rawCountsAvg[fn[:-39]][polarization[1]]) or \\\n",
    "                ('median' not in rawCountsAvg[fn[:-39]][polarization[1]])):\n",
    "                    print \"Processing %s\" % fn\n",
    "                    try:\n",
    "                        raw_counts, pol, _, GEOgrid= readS1.readS1_raw_counts(inpath=inpath, fn=fn)\n",
    "                    except Exception:\n",
    "                        print Exception.message\n",
    "                        continue\n",
    "                    if not raw_counts:\n",
    "                        rawCountsAvg[fn[:-39]] = {}\n",
    "                        rawCountsAvg[fn[:-39]]['skip'] = 'skip'\n",
    "                        print \"Skipping file with no raw_counts: %s\" % fn[:-39]\n",
    "                        continue\n",
    "                    if pol != polarization and raw_counts:\n",
    "                        polarization = pol[0]\n",
    "                        rawCountsAvg[fn[:-39]] = {}\n",
    "                        rawCountsAvg[fn[:-39]][polarization] = {}\n",
    "                        rawCountsAvg[fn[:-39]]['incidenceAngle'] = interp1d(GEOgrid['pixel'][0,:], GEOgrid['incidenceAngle'][0,:])(arange(raw_counts[polarization].shape[1]))\n",
    "                        rawCountsAvg[fn[:-39]][polarization]['mean']   = ma.mean  (double(raw_counts[polarization]), axis=0)\n",
    "                        rawCountsAvg[fn[:-39]][polarization]['median'] = ma.median(double(raw_counts[polarization]), axis=0)\n",
    "                    else:\n",
    "                        rawCountsAvg[fn[:-39]] = {}\n",
    "                        rawCountsAvg[fn[:-39]][polarization[0]] = {}\n",
    "                        rawCountsAvg[fn[:-39]][polarization[1]] = {}\n",
    "                        rawCountsAvg[fn[:-39]]['incidenceAngle'] = interp1d(GEOgrid['pixel'][0,:], GEOgrid['incidenceAngle'][0,:])(arange(raw_counts[polarization[0]].shape[1]))\n",
    "                        for p in polarization:\n",
    "                            rawCountsAvg[fn[:-39]][p]['mean']   = ma.mean  (double(raw_counts[p]), axis=0)\n",
    "                            rawCountsAvg[fn[:-39]][p]['median'] = ma.median(double(raw_counts[p]), axis=0)\n",
    "                    # when processing finished put the skip flag, so we won't process it next time\n",
    "                    # this is done, cause there are many files which are corrupted and\n",
    "                    # don't have all of the polarizations, so we process only one polarization and skip it next time\n",
    "                    rawCountsAvg[fn[:-39]]['skip'] = 'skip'\n",
    "                else:\n",
    "                    # print \"Alredy processed %s\" % fn\n",
    "                    continue\n",
    "\n",
    "            # Get size of a dict for further dunmping\n",
    "            sizeOfDict = 0\n",
    "            for v, _ in rawCountsAvg.iteritems():\n",
    "                for z, _ in rawCountsAvg[v].iteritems():\n",
    "                    if 'incidenceAngle' not in z and 'skip' not in z:\n",
    "                        for _, yy in rawCountsAvg[v][z].iteritems():\n",
    "                            sizeOfDict = sizeOfDict + yy.nbytes\n",
    "            sizeOfDict = double(sizeOfDict)/1024/1024\n",
    "            print \"rawCountsAvg size, %.2f Mb\"  % sizeOfDict\n",
    "            if sizeOfDict > 1e2:\n",
    "                print \"Size of rawCountsAvg exceeds 100Mb, saving...\"\n",
    "                with open('dumps/rawCountsAvg_bckp' + fn[-54:-41] + '.pickle', 'wb') as f:\n",
    "                    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "                    pickle.dump(rawCountsAvg, f, pickle.HIGHEST_PROTOCOL)\n",
    "                f.close()\n",
    "                del rawCountsAvg\n",
    "                rawCountsAvg = {}\n",
    "\n",
    "# if processing last file\n",
    "if fn == fileNameList[-1]:\n",
    "    print \"Saving after last file...\"\n",
    "    with open('dumps/rawCountsAvg_bckp' + fn[-54:-41] + '.pickle', 'wb') as f:\n",
    "        # Pickle the 'data' dictionary using the highest protocol available.\n",
    "        pickle.dump(rawCountsAvg, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "del raw_counts, polarization, GEOgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196 2218\n"
     ]
    }
   ],
   "source": [
    "inpath = '/media/SOLabNFS2/store/satellite/sentinel-1/'\n",
    "l=0\n",
    "for _dir, sub_dir, fileNameList in os.walk(inpath):\n",
    "    for fn in fileNameList:\n",
    "#         if fn.startswith('S1A_SM_GRD') and fn.endswith('.zip'):\n",
    "#         if fn.startswith('S1A_IW_GRD') and fn.endswith('.zip'):\n",
    "        if fn.startswith('S1A_EW_GRD') and fn.endswith('.zip'):\n",
    "            l = l + 1\n",
    "print len(rawCountsAvg), l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print \"Saving after last file...\"\n",
    "# with open('dumps/rawCountsAvg_bckp' + fn[-54:-41] + '.pickle', 'wb') as f:\n",
    "#     # Pickle the 'data' dictionary using the highest protocol available.\n",
    "#     pickle.dump(rawCountsAvg, f, pickle.HIGHEST_PROTOCOL)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "source": [
    "## PANDAS Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(pd.read_pickle('dumps/rawCountsAvg_bckp' + fn[-54:-41] + '.pickle'))\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = {}\n",
    "for v, _ in rawCountsAvg.iteritems():\n",
    "    for p, _ in rawCountsAvg[v].iteritems():\n",
    "        if 'incidenceAngle' not in p and 'skip' not in p:\n",
    "#             print p\n",
    "            _df = pd.DataFrame(data = zip(rawCountsAvg[v]['incidenceAngle'], rawCountsAvg[v][p]['median']), columns=['incidenceAngle', 'median'])\n",
    "            _df['median'][_df['median']==0]=NaN # making all Zero values NaNs\n",
    "            if not df.has_key(p):\n",
    "                df[p] = _df\n",
    "            else:\n",
    "                df[p] = df[p].append(_df, ignore_index=True)\n",
    "\n",
    "df_sorted = {}\n",
    "for p in df.iterkeys():\n",
    "    df_sorted[p] = df[p].sort_values(['incidenceAngle'], ascending=True)\n",
    "    df_sorted[p].incidenceAngle = df_sorted[p].incidenceAngle.round(2)\n",
    "\n",
    "df_grouped = {}\n",
    "for p in df.iterkeys():\n",
    "    df_grouped[p] = df_sorted[p].groupby('incidenceAngle').aggregate(min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.hold(True)\n",
    "\n",
    "p = 'hh'\n",
    "\n",
    "for k in rawCountsAvg.itervalues():\n",
    "#     print k['incidenceAngle']\n",
    "#     print k['vv']['median']\n",
    "    if k.has_key(p):\n",
    "        plt.plot(k['incidenceAngle'], k[p]['median'], '--', linewidth=.3)\n",
    "        plt.plot(df_grouped[p]['median'], 'm-', linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRAFTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pythonhosted.org/PyModelFit/over.html#fitting-a-model-to-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = {}\n",
    "for v, _ in rawCountsAvg.iteritems():\n",
    "    for p, _ in rawCountsAvg[v].iteritems():\n",
    "        if 'incidenceAngle' not in p:\n",
    "#             print p\n",
    "            _df = pd.DataFrame(data = zip(rawCountsAvg[v]['incidenceAngle'], rawCountsAvg[v][p]['median']), columns=['incidenceAngle', 'median'])\n",
    "            if not df.has_key(p):\n",
    "                df[p] = _df\n",
    "            else:\n",
    "                df[p] = df[p].append(_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_sorted = {}\n",
    "for p in df.iterkeys():\n",
    "    df_sorted[p] = df[p].sort_values(['incidenceAngle'], ascending=True)\n",
    "    df_sorted[p].incidenceAngle = df_sorted[p].incidenceAngle.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_s = df_sorted[p]\n",
    "# df_s_i = pd.DataFrame(linspace(df_s.incidenceAngle.min(), df_s.incidenceAngle.max(), df_s.shape[0]), columns=['interpIncidenceAngle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = 'hh'\n",
    "# p = 'vv'\n",
    "df_sorted[p].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_grouped = {}\n",
    "for p in df.iterkeys():\n",
    "    df_grouped[p] = df_sorted[p].groupby('incidenceAngle').aggregate(min)\n",
    "\n",
    "p = 'hh'\n",
    "# p = 'vv'\n",
    "df_grouped[p].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_grouped[p].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_grouped[p].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### ОБРЕЗАТЬ, если резкий градиент, на краях углов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_sorted[p].groupby('incidenceAngle').min().plot(), df_sorted[p].groupby('incidenceAngle').max().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p = 'hh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.hold(True)\n",
    "\n",
    "for k in rawCountsAvg.itervalues():\n",
    "#     print k['incidenceAngle']\n",
    "#     print k['vv']['median']\n",
    "    plt.plot(k['incidenceAngle'], k[p]['median'], '.')\n",
    "\n",
    "    plt.plot(df_grouped[p]['median'], 'k-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def noiseLutAvg(rawCountsAvg, pol='vv', scale=1):\n",
    "    # http://stackoverflow.com/questions/5083340/find-minimum-element-in-a-dictionary-of-dictionaries\n",
    "    # http://www.u.arizona.edu/~erdmann/mse350/topics/list_comprehensions.html\n",
    "\n",
    "    # print ( (rawCountsAvg[k]['vv']['mean']) for k in rawCountsAvg.iterkeys() )\n",
    "    # min(rawCountsAvg.keys(), key=lambda k:rawCountsAvg[k]['vv']['mean'])\n",
    "\n",
    "    # get the mean values of VV polarization from the Dictionary\n",
    "    _vv = map( lambda k: rawCountsAvg[k], rawCountsAvg.iterkeys() )\n",
    "    __vv = []\n",
    "    for k in range(len(_vv)):\n",
    "        if pol in _vv[k]:\n",
    "            __vv.append(_vv[k][pol]['mean'][::scale])\n",
    "    # find maximum length of the Data\n",
    "    _vv_max_len = max(map(len, __vv))\n",
    "    # create an empty array for further values filling\n",
    "    vv = empty((_vv_max_len, len(__vv)))\n",
    "    vv.fill(NaN)\n",
    "    for j in arange(len(__vv)):\n",
    "        vv[0:len(__vv[j]),j] = __vv[j]\n",
    "#     del _vv, __vv, _vv_max_len\n",
    "    print \"Number of %s images = %i\" % (pol, len(vv[0,:]))\n",
    "    return vv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def noiseLutMskEdgs(nLt):\n",
    "    nLtMasked = ma.array(nLt, fill_value=NaN)\n",
    "    # Mask drops of values on the edges before averaging\n",
    "    # for vv coeff=4, for vh coeff=1, for hv coeff=1/4\n",
    "    coeff=1\n",
    "    for j in arange(nLt.shape[1]):\n",
    "        # find peak values\n",
    "        minpeak = argrelmin(nLt[:,j], order=int(round(nanstd(nLt[:,j])))*coeff, mode='wrap')[0]\n",
    "        maxpeak = argrelmax(nLt[:,j], order=2, mode='wrap')[0]\n",
    "        # take minimum of the max peak\n",
    "        # take maximum of the min peak\n",
    "        # to crop as much as possible, avoiding fake dropping values\n",
    "        try:\n",
    "            peakInd = [\n",
    "                        max(minpeak[0],maxpeak[0]),\n",
    "                        min(minpeak[-1],maxpeak[-1])\n",
    "                      ]\n",
    "        except:\n",
    "            print 'Some of the min/max peaks are empty'\n",
    "            continue\n",
    "        nLtMasked[0:peakInd[0],j] = ma.masked\n",
    "        nLtMasked[peakInd[1]:-1,j] = ma.masked\n",
    "    return nLtMasked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nLt = noiseLutAvg(rawCountsAvg, pol='vh', scale=10)\n",
    "nLtMasked = noiseLutMskEdgs(nLt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(nLt[:,1], 'k.')\n",
    "plt.plot(nLtMasked[:,1], 'b.')\n",
    "# plt.plot(nLtMasked[:,3][~nLtMasked[:,3].mask], 'b.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(nanmin(nLt, axis=1), 'b.')\n",
    "plt.plot(nLtMasked.min(axis=1), 'k.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(nLt)\n",
    "# plt.plot(nanmin(nLt, axis=1), 'yo')\n",
    "plt.plot(nLtMasked.min(axis=1), 'rx')\n",
    "plt.xlim(0,len(nLtMasked))\n",
    "# oPath = '/home/mag/Documents/repos/solab/PySOL/notebooks/Sentinel-1/for_presentation/'\n",
    "# oFileName = os.path.join(oPath, 'VH_noise')\n",
    "# plt.savefig(oFileName, dpi=300,\n",
    "#             facecolor='None',\n",
    "#             bbox_inches='tight',\n",
    "#             transparent=False,\n",
    "#             pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open test file to compare noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Ania_Ladoga_29_May_2015/\n",
    "# for VV - VH\n",
    "inpath = '/media/SOLabNFS2/tmp/different_SAR/sentinel-1/Ania_Ladoga_29_May_2015/'\n",
    "fn = 'S1A_IW_GRDH_1SDV_20150603T154002_20150603T154027_006211_0081A9_5F10.zip'\n",
    "\n",
    "# for HH - HV\n",
    "# Liza Polar Low 2015 June 01\n",
    "# inpath = '/media/SOLabNFS2/tmp/different_SAR/sentinel-1/Liza_PL_01_June_2015/'\n",
    "# fn = 'S1A_EW_GRDM_1SDH_20150601T173700_20150601T173804_006183_0080D9_94AD.zip'\n",
    "\n",
    "s1 = readS1.readS1(inpath=inpath, fn=fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get vars from s1 class\n",
    "for k, v in s1.__dict__.iteritems():\n",
    "    locals()[k]=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scale = 10\n",
    "p='vh'\n",
    "nLt = noiseLutAvg(rawCountsAvg, p, scale=10)\n",
    "nLtMasked = noiseLutMskEdgs(nLt)\n",
    "nLtMin = nLtMasked.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Preserving shapes of noiseLUTs and raw_counts\n",
    "print nLtMin.shape\n",
    "print raw_counts[p][::scale,::scale].shape\n",
    "print (nLtMin)[0:raw_counts[p][::scale,::scale].shape[1]].shape\n",
    "# I have to take into account difference in incident angles for different images\n",
    "# what makes the averaged NoiseLUT shift from its true values\n",
    "# You can see that in the plot\n",
    "print (nLtMin)[\\\n",
    "                nLtMin.shape[0]-raw_counts[p][::scale,::scale].shape[1]::].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.semilogy( ( double(raw_counts[p][::scale,::scale].mean(axis=0))**2)/sigmaNought_2[p][::scale,::scale].mean(axis=0)**2)\n",
    "plt.hold(True)\n",
    "plt.semilogy( noiseLut_2[p][::scale,::scale].mean(axis=0)**2/sigmaNought_2[p][::scale,::scale].mean(axis=0)**2 , 'r')\n",
    "plt.semilogy( (nLtMin)[0:raw_counts[p][::scale,::scale].shape[1]]**2/sigmaNought_2[p][::scale,::scale].mean(axis=0)**2, 'k')\n",
    "plt.semilogy( (nLtMin)[\\\n",
    "                        nLtMin.shape[0]-raw_counts[p][::scale,::scale].shape[1]::]**2/sigmaNought_2[p][::scale,::scale].mean(axis=0)**2, 'c')\n",
    "plt.ylim(1e-21,1)\n",
    "plt.xlim(0,raw_counts[p][::scale,::scale].shape[1])\n",
    "plt.axes()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Не забыть про учёт угловой зависимости при вычислении осреднённого шума"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for VH, HV - multiply S1 noiseLUTs by nLtCoeff=1e10\n",
    "# for VV, HH - multiply S1 noiseLUTs by nLtCoeff=sqrt(2)*1e10\n",
    "nLtCoeff = sqrt(1)*1e10\n",
    "\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.hold(True)\n",
    "plt.semilogy( double(raw_counts[p][::scale,::scale].mean(axis=0))**2, 'b.' )\n",
    "# I have to take into account difference in incident angles for different images\n",
    "# what makes the averaged NoiseLUT shift from its true values\n",
    "# You can see that in the plot\n",
    "plt.semilogy( (nLtMin)[\\\n",
    "                        nLtMin.shape[0]-raw_counts[p][::scale,::scale].shape[1]::]**2, 'c')\n",
    "plt.semilogy( (nLtMin)[0:raw_counts[p][::scale,::scale].shape[1]]**2, 'k' )\n",
    "plt.semilogy( (noiseLut_2[p][::scale,::scale].mean(axis=0)*5e8)**2, 'g:' )\n",
    "plt.semilogy( (noiseLut_2[p][::scale,::scale].mean(axis=0)*nLtCoeff), 'g--' )\n",
    "plt.ylim(1e2,1e5)\n",
    "plt.xlim(0,raw_counts[p][::scale,::scale].shape[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.hold(True)\n",
    "plt.semilogy( ( double(raw_counts[p][::scale,::scale].mean(axis=0))**2)/sigmaNought_2[p][::scale,::scale].mean(axis=0)**2)\n",
    "plt.semilogy( noiseLut_2[p][::scale,::scale].mean(axis=0)*nLtCoeff/sigmaNought_2[p][::scale,::scale].mean(axis=0)**2 , 'r')\n",
    "plt.semilogy( (nLtMin)[0:raw_counts[p][::scale,::scale].shape[1]]**2/sigmaNought_2[p][::scale,::scale].mean(axis=0)**2, 'k')\n",
    "plt.ylim(5e-4,.2)\n",
    "plt.xlim(0,raw_counts[p][::scale,::scale].shape[1])\n",
    "plt.axes()\n",
    "oPath = '/home/mag/Documents/repos/solab/PySOL/notebooks/Sentinel-1/for_presentation/'\n",
    "oFileName = os.path.join(oPath, 'VH_noise_corrected')\n",
    "plt.savefig(oFileName, dpi=300,\n",
    "            facecolor='None',\n",
    "            bbox_inches='tight',\n",
    "            transparent=False,\n",
    "            pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrected raw_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For VV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scale = 10\n",
    "p='vv'\n",
    "nLt = noiseLutAvg(rawCountsAvg, p, scale)\n",
    "nLtMasked = noiseLutMskEdgs(nLt)\n",
    "nLtMin = nLtMasked.min(axis=1)\n",
    "nLtCoeff=sqrt(2)*1e10\n",
    "\n",
    "rc = double(raw_counts[p][::scale,::scale])**2\n",
    "nl_s1 = (noiseLut_2[p][::scale,::scale].mean(axis=0))*nLtCoeff\n",
    "nl = (nLtMin)[0:raw_counts[p][::scale,::scale].shape[1]]**2\n",
    "# Apply Calibration, remove the thermal noise estimation and Convert to Intensity\n",
    "s0_s1 = (rc - nl_s1 )/sigmaNought_2[p][::scale,::scale]**2\n",
    "s0    = (rc - nl )/sigmaNought_2[p][::scale,::scale]**2\n",
    "\n",
    "s0crrctd = {}\n",
    "s0crrctd[p] = s0_s1\n",
    "\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.hold(True)\n",
    "plt.subplot(211)\n",
    "plt.semilogy( s0.mean(axis=1) )\n",
    "plt.semilogy( s0_s1.mean(axis=1), 'r--' )\n",
    "plt.semilogy( (rc/sigmaNought_2[p][::scale,::scale]**2).mean(axis=1), 'g' )\n",
    "plt.ylim(5e-2,1.5e-1)\n",
    "plt.subplot(212)\n",
    "plt.semilogy( nl )\n",
    "plt.semilogy( nl_s1, 'r--' )\n",
    "plt.ylim(2e2,1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "plt.hold(True)\n",
    "plt.subplot(121)\n",
    "plt.imshow( 10*log10(s0_s1), vmin=-20, vmax=0 )\n",
    "plt.colorbar()\n",
    "plt.gray()\n",
    "plt.subplot(122)\n",
    "plt.imshow( 10*log10(s0), vmin=-20, vmax=0 )\n",
    "plt.colorbar()\n",
    "plt.gray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For VH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scale = 10\n",
    "p='vh'\n",
    "nLt = noiseLutAvg(rawCountsAvg, p, scale)\n",
    "nLtMasked = noiseLutMskEdgs(nLt)\n",
    "nLtMin = nLtMasked.min(axis=1)\n",
    "nLtCoeff=1e10\n",
    "\n",
    "rc = double(raw_counts[p][::scale,::scale])**2\n",
    "nl_s1 = (noiseLut_2[p][::scale,::scale].mean(axis=0))*nLtCoeff\n",
    "nl = (nLtMin)[0:raw_counts[p][::scale,::scale].shape[1]]**2\n",
    "# Apply Calibration, remove the thermal noise estimation and Convert to Intensity\n",
    "s0_s1 = (rc - nl_s1 )/sigmaNought_2[p][::scale,::scale]**2\n",
    "s0    = (rc - nl )/sigmaNought_2[p][::scale,::scale]**2\n",
    "\n",
    "# s0crrctd = {}\n",
    "s0crrctd[p] = s0_s1\n",
    "\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.hold(True)\n",
    "plt.subplot(211)\n",
    "plt.semilogy( s0.mean(axis=1) )\n",
    "plt.semilogy( s0_s1.mean(axis=1), 'r--' )\n",
    "plt.semilogy( (rc/sigmaNought_2[p][::scale,::scale]**2).mean(axis=1), 'g' )\n",
    "plt.ylim(8e-3,5e-2)\n",
    "plt.subplot(212)\n",
    "plt.semilogy( nl )\n",
    "plt.semilogy( nl_s1, 'r--' )\n",
    "plt.ylim(2e2,2e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "plt.hold(True)\n",
    "plt.subplot(121)\n",
    "plt.imshow( 10*log10(s0_s1), vmin=-30, vmax=-10 )\n",
    "plt.colorbar()\n",
    "plt.gray()\n",
    "plt.subplot(122)\n",
    "plt.imshow( 10*log10(s0), vmin=-30, vmax=-10 )\n",
    "plt.colorbar()\n",
    "plt.gray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for p in polarization:\n",
    "    s0crrctd[p] = wiener(s0crrctd[p], mysize=(7,7), noise=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# S1 Pixel resolution\n",
    "# we use pxlResSAR for further GSHHS rasterizing and reprojecting data with pyresample\n",
    "\n",
    "lonlim = (lons_2[::scale,::scale].min(),lons_2[::scale,::scale].max())\n",
    "latlim = (lats_2[::scale,::scale].min(),lats_2[::scale,::scale].max())\n",
    "\n",
    "# enlarge lonlims for cropping a bit larger area for masking\n",
    "lonlimGSHHS = (lonlim[0]-1.0, lonlim[1]+1.0)\n",
    "latlimGSHHS = (latlim[0]-1.0, latlim[1]+1.0)\n",
    "\n",
    "\n",
    "# Get first guess pixel resolution\n",
    "import distancelib\n",
    "pxlResSARm  = asarray(distancelib.getPixelResolution(lats_2[::scale,::scale], \\\n",
    "                                                     lons_2[::scale,::scale], \\\n",
    "                                                     lons_2[::scale,::scale].shape, 'km'))*1e3\n",
    "pxlResSARdeg  = asarray(distancelib.getPixelResolution(lats_2[::scale,::scale], \\\n",
    "                                                       lons_2[::scale,::scale], \\\n",
    "                                                       lons_2[::scale,::scale].shape, 'deg'))\n",
    "\n",
    "print \"S1 cell resolution, %s deg\"  % str(pxlResSARdeg)\n",
    "print \"S1 cell resolution, %s m\"  % str(pxlResSARm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pyresample as pr\n",
    "from pyproj import Proj\n",
    "\n",
    "# Define areas with pyresample\n",
    "swath_def = pr.geometry.SwathDefinition(lons=lons_2[::scale,::scale], lats=lats_2[::scale,::scale])\n",
    "\n",
    "area_def_4326 = swath_area_def(name='Temporal SWATH EPSG Projection 4326', proj='eqc',\n",
    "                          lonlim=lonlimGSHHS, latlim=latlimGSHHS, ellps=\"WGS84\", res=pxlResSARm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the SAR pixel resolution from the area_def for further identical shapes\n",
    "up    = min(latlimGSHHS)\n",
    "down  = max(latlimGSHHS)\n",
    "left  = min(lonlimGSHHS)\n",
    "right = max(lonlimGSHHS)\n",
    "area_extent_deg = (left, down, right, up)\n",
    "\n",
    "area_extent_deg_shape = area_def_4326.shape\n",
    "\n",
    "pxlResSARdeg = asarray( (abs(area_extent_deg[2] - area_extent_deg[0]) / float(area_extent_deg_shape[1]), \\\n",
    "                abs(area_extent_deg[3] - area_extent_deg[1]) / float(area_extent_deg_shape[0])) )\n",
    "\n",
    "pxlResSARm = asarray( (area_def_4326.pixel_size_x, area_def_4326.pixel_size_y) )\n",
    "print \"S1 cell resolution, %s deg\"  % str(pxlResSARdeg)\n",
    "print \"S1 cell resolution, %s m\"  % str(pxlResSARm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Apply Mask from GSHHS\n",
    "\n",
    "import gshhs_rasterize\n",
    "reload(gshhs_rasterize)\n",
    "\n",
    "# ESRI shapefile containing land polygons\n",
    "shapefile = '/media/SOLabNFS/store/auxdata/coastline/GSHHS_shp/f/GSHHS_f_L1.shp'\n",
    "\n",
    "# reproject GSHHS onto S1 grid before calculations\n",
    "print \"Rasterizing Land Mask\"\n",
    "mask_arr_4326 = gshhs_rasterize.gshhs_rasterize_4326(lonlimGSHHS, latlimGSHHS, \\\n",
    "                                     pxlResSARdeg, area_def_4326.shape, True, \\\n",
    "                                     shapefile)\n",
    "mask_arr_swath = pr.kd_tree.resample_nearest(area_def_4326, mask_arr_4326, swath_def, \\\n",
    "                                             radius_of_influence=4*pxlResSARm.max(), epsilon=0.5, fill_value=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB! Don't forget to flipud when bearing.mean() >200\n",
    "### NB! Don't forget to fliplr when bearing.mean() < 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "roughness = {}\n",
    "sigma0wAvg = {}\n",
    "roughnessNrmlzd = {}\n",
    "for p in polarization:\n",
    "        roughness[p] = ma.masked_where(mask_arr_swath, s0crrctd[p])\n",
    "        sigma0wAvg[p] = ma.median(roughness[p], axis=0)\n",
    "        roughnessNrmlzd[p] = (roughness[p]-sigma0wAvg[p])/sigma0wAvg[p]\n",
    "del roughness, sigma0wAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculate bearing from initial lats/lons for further wind calculation\n",
    "# Taking initial values as bearing is more accurate after interpolation than vice versa\n",
    "bearing = zeros((GEOgrid['lons'].shape[0]-1,GEOgrid['lons'].shape[1]))\n",
    "\n",
    "for n in range(0,GEOgrid['lons'].shape[1]):\n",
    "    col = ([GEOgrid['lats'][:-1,n], GEOgrid['lons'][:-1,n]], [GEOgrid['lats'][1:,n], GEOgrid['lons'][1:,n]])\n",
    "    for m in range(0,GEOgrid['lons'].shape[0]-1):\n",
    "        bearing[m][n] = distancelib.bearing(asarray(col[0])[:,m], asarray(col[1])[:,m])\n",
    "\n",
    "# interpolate to raw_counts.shape\n",
    "# bearing_2 = imresize(bearing, ncepGFSmodelWindSwath['wind_dir'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print 'bearing = %.2f' % bearing.mean()\n",
    "\n",
    "if bearing.mean() < 200:\n",
    "#     mask_arr_swath = fliplr(mask_arr_swath)\n",
    "#     wind_speed_asar = fliplr(wind_speed_asar)\n",
    "    lats_2 = fliplr(lats_2)\n",
    "    lons_2 = fliplr(lons_2)\n",
    "#     for k, v in ncepGFSmodelWindSwath.iteritems():\n",
    "#         ncepGFSmodelWindSwath[k] = fliplr(v)\n",
    "    for k, v in roughnessNrmlzd.iteritems():\n",
    "        roughnessNrmlzd[k] = fliplr(v)\n",
    "#     for k, v in sigma0w.iteritems():\n",
    "#         sigma0w[k] = fliplr(v)\n",
    "#     for k, v in s0crrctd.iteritems():\n",
    "#         s0crrctd[k] = fliplr(v)\n",
    "\n",
    "elif bearing.mean() > 200:\n",
    "#     mask_arr_swath = flipud(mask_arr_swath)\n",
    "#     wind_speed_asar = flipud(wind_speed_asar)\n",
    "    lats_2 = flipud(lats_2)\n",
    "    lons_2 = flipud(lons_2)\n",
    "#     for k, v in ncepGFSmodelWindSwath.iteritems():\n",
    "#         ncepGFSmodelWindSwath[k] = flipud(v)\n",
    "    for k, v in roughnessNrmlzd.iteritems():\n",
    "        roughnessNrmlzd[k] = flipud(v)\n",
    "#     for k, v in sigma0w.iteritems():\n",
    "#         sigma0w[k] = flipud(v)\n",
    "#     for k, v in s0crrctd.iteritems():\n",
    "#         s0crrctd[k] = flipud(v)\n",
    "\n",
    "del k,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def ntrctv_imshow(p = 'hh', vmi=-1., vma=1., cmap='RdBu_r'):\n",
    "    plt.figure(figsize=(16,16*double(data[p].shape[0])/double(data[p].shape[1])))\n",
    "    plt.imshow((data[p]), vmin=vmi, vmax=vma)\n",
    "    plt.colorbar()\n",
    "    plt.set_cmap(cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data=roughnessNrmlzd.copy()\n",
    "# data[p] = np.where(data[p]<-1,-1,data[p])\n",
    "# data[p] = np.where(data[p]>1,1,data[p])\n",
    "\n",
    "ntrctv = widgets.interact(ntrctv_imshow, p = widgets.RadioButtons(description='polarization', options=data.keys()), \\\n",
    "                 vmi=widgets.FloatSliderWidget(min=-2, max=2, value=-1, step=0.1), \\\n",
    "                 vma=widgets.FloatSliderWidget(min=-2, max=2, value=1., step=0.1), \\\n",
    "                cmap = ['RdBu_r', 'Greys', 'bone']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p = 'hv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.imshow(noiseLut_2[p][::scale,::scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure()\n",
    "plt.plot(noiseLut_2[p][::scale,::scale].mean(axis=0), 'b')\n",
    "# plt.plot(noiseLut_2[p][::scale,::scale].mean(axis=0)*1e6, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.semilogy( ( double(raw_counts[p][::scale,::scale].mean(axis=0))**2)/sigmaNought_2[p][::scale,::scale].mean(axis=0)**2)\n",
    "plt.hold(True)\n",
    "plt.semilogy( noiseLut_2[p][::scale,::scale].mean(axis=0)**2/sigmaNought_2[p][::scale,::scale].mean(axis=0) , 'r')\n",
    "plt.axes()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.semilogy( double(raw_counts[p][::scale,::scale].mean(axis=0))**2 )\n",
    "plt.semilogy( noiseLut_2['hh'][::scale,::scale].mean(axis=0)*1e13, '--' )\n",
    "plt.semilogy( noiseLut_2['hv'][::scale,::scale].mean(axis=0)*1e13, ':k')\n",
    "# plt.ylim(1e3,2e4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.imshow( 10*log10( (double(raw_counts[p][::scale,::scale])**2 - noiseLut_2[p][::scale,::scale]*1e13 )/sigmaNought_2[p][::scale,::scale]**2),\n",
    "           vmin=-20, vmax=0)\n",
    "plt.gray()\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sigma0[p] = ( double(raw_counts[p])**2 - noiseLut_2[p] )/sigmaNought_2[p]**2\n",
    "\n",
    "plt.close('all')\n",
    "plt.plot( ( double(raw_counts[p][::scale,::scale].mean(axis=0))**2 - noiseLut_2[p][::scale,::scale].mean(axis=0)     )/sigmaNought_2[p][::scale,::scale].mean(axis=0)**2)\n",
    "plt.plot( ( double(raw_counts[p][::scale,::scale].mean(axis=0))**2 - noiseLut_2[p][::scale,::scale].mean(axis=0)*1e13 )/sigmaNought_2[p][::scale,::scale].mean(axis=0)**2, 'k:')\n",
    "# plt.plot(noiseLut_2[p][::scale,::scale].mean(axis=0)*1e13, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Try to split the image into several parts and normalize each part to remove the scalloping effect\n",
    "# which is still visible even after ESA applies the TOPSAR technique\n",
    "# https://sentinel.esa.int/web/sentinel/user-guides/sentinel-1-sar/acquisition-modes/interferometric-wide-swath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# From https://gist.github.com/endolith/250860\n",
    "\n",
    "from numpy import NaN, Inf, arange, isscalar, asarray, array\n",
    " \n",
    "def peakdet(v, delta, x = None):\n",
    "    \"\"\"\n",
    "    Converted from MATLAB script at http://billauer.co.il/peakdet.html\n",
    "    \n",
    "    Returns two arrays\n",
    "    \n",
    "    function [maxtab, mintab]=peakdet(v, delta, x)\n",
    "    %PEAKDET Detect peaks in a vector\n",
    "    %        [MAXTAB, MINTAB] = PEAKDET(V, DELTA) finds the local\n",
    "    %        maxima and minima (\"peaks\") in the vector V.\n",
    "    %        MAXTAB and MINTAB consists of two columns. Column 1\n",
    "    %        contains indices in V, and column 2 the found values.\n",
    "    %      \n",
    "    %        With [MAXTAB, MINTAB] = PEAKDET(V, DELTA, X) the indices\n",
    "    %        in MAXTAB and MINTAB are replaced with the corresponding\n",
    "    %        X-values.\n",
    "    %\n",
    "    %        A point is considered a maximum peak if it has the maximal\n",
    "    %        value, and was preceded (to the left) by a value lower by\n",
    "    %        DELTA.\n",
    "    \n",
    "    % Eli Billauer, 3.4.05 (Explicitly not copyrighted).\n",
    "    % This function is released to the public domain; Any use is allowed.\n",
    "    \n",
    "    \"\"\"\n",
    "    maxtab = []\n",
    "    mintab = []\n",
    "       \n",
    "    if x is None:\n",
    "        x = arange(len(v))\n",
    "    \n",
    "    v = asarray(v)\n",
    "    \n",
    "    if len(v) != len(x):\n",
    "        sys.exit('Input vectors v and x must have same length')\n",
    "    \n",
    "    if not isscalar(delta):\n",
    "        sys.exit('Input argument delta must be a scalar')\n",
    "    \n",
    "    if delta <= 0:\n",
    "        sys.exit('Input argument delta must be positive')\n",
    "    \n",
    "    mn, mx = Inf, -Inf\n",
    "    mnpos, mxpos = NaN, NaN\n",
    "    \n",
    "    lookformax = True\n",
    "    \n",
    "    for i in arange(len(v)):\n",
    "        this = v[i]\n",
    "        if this > mx:\n",
    "            mx = this\n",
    "            mxpos = x[i]\n",
    "        if this < mn:\n",
    "            mn = this\n",
    "            mnpos = x[i]\n",
    "        \n",
    "        if lookformax:\n",
    "            if this < mx-delta:\n",
    "                maxtab.append((mxpos, mx))\n",
    "                mn = this\n",
    "                mnpos = x[i]\n",
    "                lookformax = False\n",
    "        else:\n",
    "            if this > mn+delta:\n",
    "                mintab.append((mnpos, mn))\n",
    "                mx = this\n",
    "                mxpos = x[i]\n",
    "                lookformax = True\n",
    " \n",
    "    return array(maxtab), array(mintab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find the peaks value index to split the image in several parts\n",
    "# NB! We use sigma0 data for peaks not the LUTs, as the peaks in data and LUTs are a bit different, We believe the data itself, not the LUTs\n",
    "# First guess number of peaks calculated from LUTs\n",
    "from scipy.signal import argrelmax\n",
    "nsLutIndLen = len(argrelmax(noiseLut_2[p][::scale,::scale].mean(axis=0), order=10, mode='wrap')[0])\n",
    "#  Loop is done comparing vector to some threshold value until peaks are found\n",
    "for ii in xrange(1,500,2):\n",
    "    maxtab = peakdet(sigma0wAvg[p], 1e-6*ii)[0]\n",
    "#     print len(maxtab[:,0])\n",
    "    if len(maxtab[:,0]) == nsLutIndLen:\n",
    "        break\n",
    "sgmPeakInd = maxtab[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.plot(sigma0wAvg[p])\n",
    "plt.hold(True)\n",
    "plt.plot(arange(sgmPeakInd[1],sgmPeakInd[2]),sigma0wAvg[p][sgmPeakInd[1]:sgmPeakInd[2]], 'r')\n",
    "plt.plot(maxtab[:,0], maxtab[:,1], 'go')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rghns1 = roughness[p][0:900,sgmPeakInd[1]:sgmPeakInd[2]]\n",
    "sgmAvg1 = rghns1.mean(axis=1)\n",
    "rghnsNrmlzd = (rghns1.T-sgmAvg1)/sgmAvg1\n",
    "rghnsNrmlzd = rghnsNrmlzd.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print argrelmax(sgmAvg1, order=30, mode='wrap')\n",
    "print diff(argrelmax(sgmAvg1, order=30, mode='wrap'))\n",
    "print '\\n'\n",
    "\n",
    "from scipy.signal import argrelmin\n",
    "print argrelmin(sgmAvg1, order=30, mode='wrap')\n",
    "print diff(argrelmin(sgmAvg1, order=30, mode='wrap'))\n",
    "\n",
    "print mean(diff(argrelmax(sgmAvg1, order=30, mode='wrap')))\n",
    "print mean(diff(argrelmin(sgmAvg1, order=30, mode='wrap'))[0,1:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print peakdet(sgmAvg1, 2e-4)[0][:,0]\n",
    "print diff(peakdet(sgmAvg1, 2e-4)[0][:,0])\n",
    "mean(diff(peakdet(sgmAvg1, 2e-4)[0][:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "# plt.plot(3*log10(noiseLut_2[p][::scale,::scale][600,:]))\n",
    "plt.plot(10*log10(noiseLut_2[p][::scale,::scale][600,:]*1e6), 'k')\n",
    "# plt.plot(10*log10(noiseLut_2[p][::scale,::scale][600,:]))\n",
    "plt.hold(True)\n",
    "plt.plot(10*log10(sigma0w[p][600,:]), 'r')\n",
    "plt.plot(10*log10(sigma0w[p][600,:]-noiseLut_2[p][::scale,::scale][600,:]*1e6), 'g')\n",
    "# plt.ylim((-110,-23))\n",
    "plt.ylim((-37,-23))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.plot(sgmAvg1)\n",
    "# plt.hold(True)\n",
    "# plt.plot(maxtab[:,0], maxtab[:,1], 'go')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy import fftpack\n",
    "\n",
    "# Take the fourier transform of the image.\n",
    "f1 = fftpack.fft(sgmAvg1)\n",
    "\n",
    "# Now shift the quadrants around so that low spatial frequencies are in\n",
    "# the center of the 1D fourier transformed image.\n",
    "fshift = fftpack.fftshift( f1 )\n",
    "\n",
    "# Calculate a 1D power spectrum\n",
    "psd1D = abs( fshift )**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.plot(log10( psd1D ))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fftShiftInd = peakdet(psd1D, 2e-4)[0][:,0]\n",
    "print fftShiftInd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# remove the low frequencies by masking with a rectangular window\n",
    "# High Pass Filter (HPF)\n",
    "# rows, cols = noiseLut_2[p][::scale,::scale].shape\n",
    "# crow,ccol = rows/2 , cols/2\n",
    "# fshift[crow-10:crow-9, ccol-10:ccol+10] = 0\n",
    "\n",
    "fshiftFiltered = fshift\n",
    "\n",
    "fftShiftInd = peakdet(psd1D, 2e-4)[0][:,0]\n",
    "for i in range(0,len(fftShiftInd)/2):\n",
    "    fshiftFiltered[fftShiftInd[i]]=0\n",
    "for i in range(len(fftShiftInd)/2+1,len(fftShiftInd)):\n",
    "    fshiftFiltered[fftShiftInd[i]]=0\n",
    "    \n",
    "# fshiftFiltered[fftShiftInd[0]]=0\n",
    "# fshiftFiltered[fftShiftInd[1]]=0\n",
    "# fshiftFiltered[fftShiftInd[3]]=0\n",
    "# fshiftFiltered[fftShiftInd[4]]=0\n",
    "\n",
    "# Calculate a 1D power spectrum\n",
    "psd1DFiltered = abs( fshiftFiltered )**2\n",
    "\n",
    "# shift back (we shifted the center before)\n",
    "f_ishift = fftpack.ifftshift(fshiftFiltered)\n",
    "\n",
    "# inverse fft to get the image back\n",
    "img_back = fftpack.ifft(f_ishift)\n",
    "img_back = abs(img_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.plot(log10( psd1D ))\n",
    "plt.hold(True)\n",
    "plt.plot(log10( psd1DFiltered ))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.plot(sgmAvg1)\n",
    "plt.hold(True)\n",
    "plt.plot(img_back, 'g--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize=(8,32/3*double(sigma0w[p].shape[0])/double(sigma0w[p].shape[1])))\n",
    "plt.subplot(121)\n",
    "plt.imshow(rghns1, vmin=0, vmax=0.01)\n",
    "plt.gray()\n",
    "plt.subplot(122)\n",
    "plt.imshow(rghnsNrmlzd, vmin=-1, vmax=1)\n",
    "# plt.imshow(roughness[p], vmin=0, vmax=0.1)\n",
    "plt.gray()\n",
    "\n",
    "plt.set_cmap('bone_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "py.sign_in('magican', '5di4fue0lf')\n",
    "\n",
    "noiseLutInd = Scatter(\n",
    "        y=sigma0wAvg[p],\n",
    "        mode='markers',\n",
    "        marker=Marker(\n",
    "            color='red',\n",
    "            symbol='square'\n",
    "        )\n",
    "    )\n",
    "data = Data([noiseLutInd])\n",
    "\n",
    "# (3) Make Figure object\n",
    "fig = Figure(data=data)\n",
    "\n",
    "# (@) Send to Plotly and show in notebook\n",
    "py.iplot(fig, filename='noiseLutInd')\n",
    "# plot_url = py.plot(fig, filename='noiseLutInd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "py.sign_in('magican', '5di4fue0lf')\n",
    "\n",
    "data = Data([\n",
    "    Heatmap(\n",
    "        z=flipud(roughnessNrmlzd['hh'][::10,::10]),\n",
    "        colorscale='Greys',\n",
    "        zauto=False,\n",
    "        zmin=-1,\n",
    "        zmax=1\n",
    "    )\n",
    "])\n",
    "\n",
    "layout = Layout(\n",
    "    autosize=True,\n",
    "#     autosize=False,\n",
    "#     width=roughnessNrmlzd['hh'].shape[1]/3,\n",
    "#     height=roughnessNrmlzd['hh'].shape[0]/3,\n",
    ")\n",
    "\n",
    "# (3) Make Figure object\n",
    "fig = Figure(data=data, layout=layout)\n",
    "\n",
    "# (@) Send to Plotly and show in notebook\n",
    "py.iplot(fig, filename='roughnessNrmlzd')\n",
    "# plot_url = py.plot(fig, filename='noiseLutInd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "def ntrctv_imshow(p = 'hh', vmi=-1., vma=1., cmap='Greys'):\n",
    "    plt.figure(figsize=(8,8*double(data[p].shape[0])/double(data[p].shape[1])))\n",
    "    plt.imshow(data[p], vmin=vmi, vmax=vma)\n",
    "    plt.colorbar()\n",
    "    plt.set_cmap(cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data=roughnessNrmlzd.copy()\n",
    "\n",
    "ntrctv = widgets.interact(ntrctv_imshow, p = widgets.RadioButtons(description='polarization', options=data.keys()), \\\n",
    "                 vmi=widgets.FloatSliderWidget(min=-2, max=2, value=-1, step=0.1), \\\n",
    "                 vma=widgets.FloatSliderWidget(min=-2, max=2, value=1., step=0.1), \\\n",
    "                cmap = ['binary', 'Blues', 'BuGn', 'BuPu', 'gist_yarg',\n",
    "                             'GnBu', 'Greens', 'Greys', 'Oranges', 'OrRd',\n",
    "                             'PuBu', 'PuBuGn', 'PuRd', 'Purples', 'RdPu',\n",
    "                             'Reds', 'YlGn', 'YlGnBu', 'YlOrBr', 'YlOrRd']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.getsizeof(sigma0[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s1 = sigma0[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.int8(s1).nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.imshow(incidenceAngle_2[p][::scale,::scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.plot(sigma0wAvg[p])\n",
    "plt.plot(sigma0w[p].mean(axis=0), 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.imshow(roughness[p], vmin=0, vmax=0.01)\n",
    "# plt.imshow(roughness[p], vmin=0, vmax=0.1)\n",
    "plt.gray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(roughnessNrmlzd[p], vmin=-1, vmax=1)\n",
    "plt.set_cmap('bone_r')\n",
    "# plt.set_cmap('seismic')\n",
    "# plt.set_cmap('RdBu_r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize=(16,16/3*double(sigma0w[p].shape[0])/double(sigma0w[p].shape[1])))\n",
    "plt.subplot(121)\n",
    "plt.imshow(roughness[p], vmin=0, vmax=0.1)\n",
    "plt.gray()\n",
    "plt.colorbar()\n",
    "plt.title(p)\n",
    "plt.subplot(122)\n",
    "plt.imshow(roughnessNrmlzd[p], vmin=-1, vmax=1)\n",
    "plt.gray()\n",
    "plt.colorbar()\n",
    "plt.title(p + ' Nrmlzd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(roughnessNrmlzd[p], vmin=-1, vmax=1)\n",
    "# plt.set_cmap('bone_r')\n",
    "# plt.set_cmap('seismic')\n",
    "plt.set_cmap('RdBu_r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize=(16,16/3*double(sigma0w[polarization[0]].shape[0])/double(sigma0w[polarization[0]].shape[1])))\n",
    "plt.subplot(131)\n",
    "plt.imshow(10*log10(sigma0w[polarization[0]]), vmin=-20, vmax=5)\n",
    "plt.gray()\n",
    "plt.colorbar()\n",
    "plt.title(polarization[0])\n",
    "plt.subplot(132)\n",
    "plt.imshow(10*log10(sigma0w[polarization[1]]), vmin=-30, vmax=5)\n",
    "plt.gray()\n",
    "plt.colorbar()\n",
    "plt.title(polarization[1])\n",
    "# plt.subplot(133)\n",
    "# plt.imshow(roughness[polarization[0]][::scale,::scale], vmin=0, vmax=roughness[polarization[0]].mean())\n",
    "# plt.gray()\n",
    "# plt.colorbar()\n",
    "# plt.title(polarization[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Adding Model wind\n",
    "\n",
    "startTime = datetime.datetime.strptime(\\\n",
    "                              manifest['xfdu:XFDU']['metadataSection']['metadataObject'][12]\\\n",
    "                              ['metadataWrap']['xmlData']['safe:acquisitionPeriod']['safe:startTime'],\\\n",
    "                              \"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "\n",
    "ncepGFSmodelWind = ncepGFSmodel(startTime, lats_2, lons_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize=(16,16/3*double(sigma0w[p].shape[0])/double(sigma0w[p].shape[1])))\n",
    "plt.subplot(121)\n",
    "plt.imshow(ncepGFSmodelWind['wind_speed'])\n",
    "plt.jet()\n",
    "plt.colorbar()\n",
    "plt.subplot(122)\n",
    "plt.imshow(ncepGFSmodelWind['wind_dir'])\n",
    "plt.jet()\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt.close('all')\n",
    "# plt.figure(figsize=(16,4))\n",
    "# plt.subplot(121)\n",
    "# plt.imshow(ncepGFSmodelWind['lats_wind'])\n",
    "# plt.jet()\n",
    "# plt.colorbar()\n",
    "# plt.subplot(122)\n",
    "# plt.imshow(ncepGFSmodelWind['lons_wind'])\n",
    "# plt.jet()\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt.close('all')\n",
    "# plt.plot(ncepGFSmodelWind['lons_wind'][0,:])\n",
    "# plt.plot(ncepGFSmodelWind['lats_wind'][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reprojecting data\n",
    "\n",
    "import distancelib\n",
    "\n",
    "# Pixel resolution\n",
    "# we use pxlResWind/pxlResSAR for further pyresample radius_of_influence and sigmas\n",
    "pxlResWind = asarray(distancelib.getPixelResolution(ncepGFSmodelWind['lats_wind'], \\\n",
    "                                                    ncepGFSmodelWind['lons_wind'], \\\n",
    "                                                    ncepGFSmodelWind['lons_wind'].shape, 'km'))\n",
    "# pxlResSAR  = asarray(distancelib.getPixelResolution(lats_2, lons_2, lons_2.shape, 'km'))*1e3\n",
    "\n",
    "# Note pxlResWind is in KM, multiply by 1e3 for meters\n",
    "print \"S1 cell resolution, %s m\"  % pxlResSARm\n",
    "print \"Wind cell resolution, %s km\" % pxlResWind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import RectSphereBivariateSpline\n",
    "\n",
    "def ncepGFSmodel2swath(lats, lons, data, lats_2, lons_2):\n",
    "\n",
    "    func = RectSphereBivariateSpline(lats, lons, data)\n",
    "    data_2 = func.ev(lats_2.ravel()*pi/180, \\\n",
    "                     lons_2.ravel()*pi/180)\\\n",
    "                     .reshape(lats_2.shape)\n",
    "    return data_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# reproject NCEP onto S1 grid before calculations\n",
    "# Using RectSphereBivariateSpline - Bivariate spline approximation over a rectangular mesh on a sphere\n",
    "# as it is much more efficiant for full resolution\n",
    "# as well as smoothes nicely the image\n",
    "\n",
    "# We don't want to work with full res wind so scaling the image for about 100m resolution\n",
    "# Adjust scale to get appropriate value\n",
    "scale = 10\n",
    "\n",
    "lts = flipud(ncepGFSmodelWind['lats_wind'])[:,0]*pi/180\n",
    "lns = ncepGFSmodelWind['lons_wind'][0,:]*pi/180\n",
    "data = flipud(ncepGFSmodelWind['wind_speed'])\n",
    "data2 = flipud(ncepGFSmodelWind['wind_dir'])\n",
    "\n",
    "lts_2 = lats_2[::scale,::scale]\n",
    "lns_2 = lons_2[::scale,::scale]\n",
    "\n",
    "wind_speed_model_swath = ncepGFSmodel2swath(lts, lns, data, lts_2, lns_2)\n",
    "wind_dir_model_swath   = ncepGFSmodel2swath(lts, lns, data2, lts_2, lns_2)\n",
    "\n",
    "del data, data2\n",
    "    \n",
    "pxlResWindSwath = asarray(distancelib.getPixelResolution(lts_2, \\\n",
    "                                                    lns_2, \\\n",
    "                                                    lns_2.shape, 'km'))\n",
    "print \"Interpolated Wind cell resolution, %s km\" % pxlResWindSwath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize=(16,16/3*double(sigma0w[polarization[0]].shape[0])/double(sigma0w[polarization[0]].shape[1])))\n",
    "plt.subplot(121)\n",
    "plt.imshow(wind_speed_model_swath[::33,::33])\n",
    "plt.jet()\n",
    "plt.colorbar()\n",
    "plt.subplot(122)\n",
    "plt.imshow(wind_dir_model_swath[::33,::33])\n",
    "plt.jet()\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculate bearing from initial lats/lons for further wind calculation\n",
    "# Taking initial values as bearing is more accurate after interpolation than vice versa\n",
    "bearing = zeros((GEOgrid['lons'].shape[0]-1,GEOgrid['lons'].shape[1]))\n",
    "\n",
    "for n in range(0,GEOgrid['lons'].shape[1]):\n",
    "    col = ([GEOgrid['lats'][:-1,n], GEOgrid['lons'][:-1,n]], [GEOgrid['lats'][1:,n], GEOgrid['lons'][1:,n]])\n",
    "    for m in range(0,GEOgrid['lons'].shape[0]-1):\n",
    "        bearing[m][n] = distancelib.bearing(asarray(col[0])[:,m], asarray(col[1])[:,m])\n",
    "\n",
    "# interpolate to raw_counts.shape\n",
    "bearing_2 = imresize(bearing, wind_dir_model_swath.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize=(16,16/3*double(sigma0w[p].shape[0])/double(sigma0w[p].shape[1])))\n",
    "plt.imshow(bearing_2)\n",
    "plt.jet()\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Wind Speed\n",
    "\n",
    "# windSpeed = {}\n",
    "\n",
    "# # the C-2PO (C-band Cross-Polarized Ocean) model\n",
    "# # B. Zhang and W. Perrie, Cross-polarized synthetic aperture radar:\n",
    "# # A new potential measurement technique for hurricanes, Bull. Amer. Meteor. Soc., pp. 531-541, 2012.\n",
    "# if 'vh' in polarization:\n",
    "#     windSpeed['vh'] = (10*log10(sigma0w['vh']) + 35.652)/0.58\n",
    "# if 'hv' in polarization:\n",
    "#     windSpeed['hv'] = (10*log10(sigma0w['hv']) + 35.652)/0.58\n",
    "\n",
    "# # convert the signal measured in HH into VV polarization before using a GMF from the CMOD family\n",
    "# if 'hh' in polarization:\n",
    "#     alpha=1 # for Sentinel Toolbox alpha=1, initially it was 0.6\n",
    "#     sigma0w['hh2vv'] = sigma0w['hh']*(1+2*tan(incidenceAngle_2['hh'])**2)**2/(1+alpha*tan(incidenceAngle_2['hh'])**2)**2\n",
    "\n",
    "# plt.close('all')\n",
    "# plt.figure(figsize=(16,16/3*double(sigma0w[polarization[0]].shape[0])/double(sigma0w[polarization[0]].shape[1])))\n",
    "# plt.imshow(windSpeed['vh'][::33,::33], vmin=0, vmax=20)\n",
    "# plt.jet()\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#NB! WINDDIR = 0 WHEN WIND BLOWS TOWARDS RADAR!\n",
    "wind_dir_model_swath_rel = 90 + bearing_2 - wind_dir_model_swath\n",
    "try:\n",
    "    from cmod_gpu import rcs2windOpenCl\n",
    "    wind_speed_asar = rcs2windOpenCl(sar=sigma0w[p], \\\n",
    "                                     windir=wind_dir_model_swath_rel, \\\n",
    "                                     theta=incidenceAngle_2[p][::scale,::scale])\n",
    "except Exception:\n",
    "    from cmod_vect import rcs2windPar\n",
    "    wind_speed_asar = rcs2windPar(sigma0w[p], cmdv=5, \\\n",
    "                                  windir=wind_dir_model_swath_rel, \\\n",
    "                                  theta=incidenceAngle_2[p][::scale,::scale], nprocs=numProcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize=(16,16/3*double(sigma0w[p].shape[0])/double(sigma0w[p].shape[1])))\n",
    "plt.plot(wind_speed_model_swath[500,:])\n",
    "plt.plot(wind_speed_asar[500,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize=(16,16/3*double(sigma0w[p].shape[0])/double(sigma0w[p].shape[1])))\n",
    "plt.plot(wind_speed_model_swath[:,1500])\n",
    "plt.plot(wind_speed_asar[:,1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize=(16,16/3*double(sigma0w[p].shape[0])/double(sigma0w[p].shape[1])))\n",
    "plt.subplot(121)\n",
    "plt.imshow(wind_speed_model_swath, vmin=3, vmax=10)\n",
    "plt.jet()\n",
    "plt.colorbar()\n",
    "plt.subplot(122)\n",
    "plt.imshow(wind_speed_asar, vmin=3, vmax=10)\n",
    "plt.jet()\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reprojecting data\n",
    "\n",
    "\n",
    "roughness_4326 = pr.kd_tree.resample_nearest(swath_def, roughnessNrmlzd, area_def_4326, \\\n",
    "                                             radius_of_influence=4*pxlResSARm.max(), epsilon=0.5, fill_value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save png image and kml file\n",
    "\n",
    "import simplekml\n",
    "\n",
    "def create_KML_asar(area_extent, savepath):\n",
    "    kml = simplekml.Kml()\n",
    "\n",
    "    pol = kml.newpolygon(name='area_extent', visibility=1)\n",
    "    pol.tessellate = 1\n",
    "\n",
    "    pol.altitudemode = 'clampToGround'\n",
    "    pol.outerboundaryis.coords = [(area_extent[0], area_extent[1]), (area_extent[2], area_extent[3])]\n",
    "    if type(savepath) == list:\n",
    "        for _savepath in savepath:\n",
    "            kml.save(_savepath)\n",
    "    else:\n",
    "        kml.save(savepath)\n",
    "\n",
    "oPath = '/home/mag/tmp/'\n",
    "oFileName = os.path.join(oPath, fn+'.png')\n",
    "\n",
    "close('all')\n",
    "gray()\n",
    "imsave(oFileName, roughness_masked, vmin=0, vmax=2)\n",
    "create_KML_asar(area_def.area_extent, os.path.join(oPath, fn+'.kml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate tiles\n",
    "\n",
    "import gdal\n",
    "import sys\n",
    "sys.path.append('/usr/bin')\n",
    "from gdal2tiles import GDAL2Tiles\n",
    "\n",
    "def create_asar_tiles(png_filename, tiles_output_dir, proj):\n",
    "    local_argv = ['/usr/bin/gdal2tiles.py', '-p', 'raster', '-r', 'cubic',\n",
    "                  '-s', proj, png_filename, tiles_output_dir]\n",
    "    argv = gdal.GeneralCmdLineProcessor(local_argv)\n",
    "    if argv:\n",
    "        gdal2tiles = GDAL2Tiles(argv[1:])\n",
    "        gdal2tiles.process()\n",
    "\n",
    "create_KML_asar(area_def.area_extent, os.path.join(_path, fileName+'.kml'))\n",
    "\n",
    "tiles_3413_output_dir = os.path.join(oPath, 'tiles')\n",
    "png_3413_filename = oFileName\n",
    "create_asar_tiles(png_3413_filename, tiles_3413_output_dir, 'EPSG:3413')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check the lat/lon/incidenceAngle after interpolation\n",
    "\n",
    "print diff([GEOgrid['incidenceAngle'].min(),incidenceAngle_2.min()])\n",
    "print diff([GEOgrid['incidenceAngle'].max(),incidenceAngle_2.max()])\n",
    "\n",
    "print diff([GEOgrid['lats'].min(),lats_2.min()])\n",
    "print diff([GEOgrid['lats'].max(), lats_2.max()])\n",
    "\n",
    "print diff([GEOgrid['lons'].min(), lons_2.min()])\n",
    "print diff([GEOgrid['lons'].max(), lons_2.max()])\n",
    "\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(16,14))\n",
    "\n",
    "plt.subplot(321)\n",
    "plt.plot(range(0,raw_counts.shape[1]), lats_2[0,:])\n",
    "plt.plot(GEOgrid['pixel'][0,:], GEOgrid['lats'][0,:], 'r.')\n",
    "plt.hold\n",
    "plt.plot(range(0,raw_counts.shape[1]), lats_2[-1,:])\n",
    "plt.plot(GEOgrid['pixel'][-1,:], GEOgrid['lats'][-1,:], 'k.')\n",
    "\n",
    "plt.subplot(322)\n",
    "plt.plot(range(0,raw_counts.shape[0]), lats_2[:,0])\n",
    "plt.plot(GEOgrid['line'][:,0], GEOgrid['lats'][:,0], 'r.')\n",
    "plt.hold\n",
    "plt.plot(range(0,raw_counts.shape[0]), lats_2[:,-1])\n",
    "plt.plot(GEOgrid['line'][:,-1], GEOgrid['lats'][:,-1], 'k.')\n",
    "\n",
    "plt.subplot(323)\n",
    "plt.plot(range(0,raw_counts.shape[1]), incidenceAngle_2[0,:])\n",
    "plt.plot(GEOgrid['pixel'][0,:], GEOgrid['incidenceAngle'][0,:], 'r.')\n",
    "plt.hold\n",
    "plt.plot(range(0,raw_counts.shape[1]), incidenceAngle_2[-1,:])\n",
    "plt.plot(GEOgrid['pixel'][-1,:], GEOgrid['incidenceAngle'][-1,:], 'k.')\n",
    "\n",
    "plt.subplot(324)\n",
    "plt.plot(range(0,raw_counts.shape[0]), incidenceAngle_2[:,0])\n",
    "plt.plot(GEOgrid['line'][:,0], GEOgrid['incidenceAngle'][:,0], 'r.')\n",
    "plt.hold\n",
    "plt.plot(range(0,raw_counts.shape[0]), incidenceAngle_2[:,-1])\n",
    "plt.plot(GEOgrid['line'][:,-1], GEOgrid['incidenceAngle'][:,-1], 'k.')\n",
    "\n",
    "plt.subplot(325)\n",
    "plt.plot(range(0,raw_counts.shape[1]), sigmaNought_2[0,:])\n",
    "plt.plot(cLUTs['pixel'][0,::10], cLUTs['sigmaNought'][0,::10], 'r.')\n",
    "\n",
    "plt.subplot(326)\n",
    "plt.plot(range(0,raw_counts.shape[0]), sigmaNought_2[:,0])\n",
    "plt.plot(cLUTs['line'][:,0], cLUTs['sigmaNought'][:,0], 'r.')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Product:\tS1A_IW_GRDH_1SDV_20141004T155619_20141004T155644_002682_002FE5_BE58_Calib\n",
    "\n",
    "Image-X:\t10215\tpixel\n",
    "Image-Y:\t5465\tpixel\n",
    "Longitude:\t25°19'38\" E\tdegree\n",
    "Latitude:\t59°52'25\" N\tdegree\n",
    "\n",
    "BandName\tValue\tUnit\n",
    "Sigma0_VV:\t19.47602\tintensity\t\n",
    "\n",
    "latitude:\t59.87349\tdeg\n",
    "longitude:\t25.327261\tdeg\n",
    "incident_angle:\t37.08765\tdeg\n",
    "elevation_angle:\t32.87843\tdeg\n",
    "slant_range_time:\t5738716.0\tns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lats_2[5465,10215], lons_2[5465,10215], incidenceAngle_2[p][5465,10215], sigma0[p][5465,10215]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print arcsin((4.740000e+02/6.666194e+02)**2)*180/pi, incidenceAngle_2[p][0,0]\n",
    "print arcsin((4.740000e+02/5.596816e+02)**2)*180/pi, incidenceAngle_2[p][-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "\n",
    "ds = gdal.Open(inpath + fileLocation['s1aiwgrd' + polarization])\n",
    "ds.RasterCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ds = gdal.Open('/media/data/data/OTHER/RS2 Agulhas and Lion/RS2_SQA_1xQGSS20091224_164846_00000004/imagery_HV.tif')\n",
    "ds.RasterCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ds = gdal.Open('/media/SOLabNFS2/tmp/sentinel-1/S1A_IW_SLC__1SDV_20141003T151001_20141003T151028_002667_002F87_7088.SAFE/measurement/s1a-iw1-slc-vv-20141003t151002-20141003t151027-002667-002f87-004.tiff')\n",
    "slc = ds.ReadAsArray()\n",
    "slc.dtype\n",
    "del slc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# from scipy.interpolate import griddata\n",
    "# from numpy import meshgrid, linspace\n",
    "\n",
    "# Slant Range, Single-Look Complex (SLC) products\n",
    "# are images in the slant range by azimuth imaging plane,\n",
    "# in the image plane of satellite data acquisition.\n",
    "# THAT MEANS that they must be interpolated more carefully using slantRangeTime and azimuthTime as in ASAR\n",
    "\n",
    "# create new grid of raw_counts shape\n",
    "# line_2, pixel_2 = meshgrid(linspace(0, lats.shape[0], raw_counts.shape[0]), linspace(0, lats.shape[1], raw_counts.shape[1]))\n",
    "# Interpolate onto a new grid\n",
    "# lats_2 = griddata((line.ravel(), pixel.ravel()), lats.ravel(), (line_2, pixel_2), method='cubic')\n",
    "# lons_2 = griddata((line.ravel(), pixel.ravel()), lons.ravel(), (line_2, pixel_2), method='cubic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#     # Nice Image (Roughness)\n",
    "#     if p == 'hh':\n",
    "#         ph = (2.20495, -14.3561e-2, 11.28e-4)\n",
    "#         sigma0_hh_ref = exp( ( ph[0]+incidenceAngle_2[p]*ph[1]+incidenceAngle_2[p]**2*ph[2])*log(10) )\n",
    "#         roughness[p] = sigma0w[p]/sigma0_hh_ref\n",
    "#     elif p == 'vv':\n",
    "#         pv = (2.29373, -15.393e-2, 15.1762e-4)\n",
    "#         sigma0_vv_ref = exp( ( pv[0]+incidenceAngle_2[p]*pv[1]+incidenceAngle_2[p]**2*pv[2])*log(10) )\n",
    "#         roughness[p] = sigma0w[p]/sigma0_vv_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "USING minidom\n",
    "\n",
    "from xml.dom import minidom\n",
    "manifest = minidom.parse(inpath + \"manifest.safe\")\n",
    "for node in manifest.getElementsByTagName('dataObject'):\n",
    "    if node.getAttribute('repID') == 's1Level1ProductSchema':\n",
    "        fileLocation.append(node.childNodes[1].childNodes[1].getAttribute('href'))\n",
    "\n",
    "# open first file with minidom\n",
    "xmldoc = minidom.parse(inpath + fileLocation[0])\n",
    "\n",
    "document_file = open(inpath + fileLocation[0], \"r\") # Open a file in read-only mode\n",
    "original_doc = document_file.read() # read the file object\n",
    "document = xmltodict.parse(original_doc) # Parse the read document string\n",
    "\n",
    "# get productType/polarisation from the Annotation Data Set Records (ADSR)\n",
    "document['product']['adsHeader'].keys()\n",
    "productType = xmldoc.getElementsByTagName('productType')[0].childNodes[0].data\n",
    "polarisation = xmldoc.getElementsByTagName('polarisation')[0].childNodes[0].data\n",
    "\n",
    "# get geolocation grid elements by tag name\n",
    "latGrid = xmldoc.getElementsByTagName('latitude')\n",
    "lonGrid = xmldoc.getElementsByTagName('longitude')\n",
    "lineGrid = xmldoc.getElementsByTagName('line')\n",
    "pixelGrid = xmldoc.getElementsByTagName('pixel')\n",
    "\n",
    "# preallocate variables\n",
    "lat = zeros( ( (latGrid).length, 1) )\n",
    "lon = zeros( lat.shape )\n",
    "line = zeros( lat.shape )\n",
    "pixel = zeros( lat.shape )\n",
    "\n",
    "# read Geolocation grid points into line\n",
    "for n in range((latGrid).length):\n",
    "    lat[n] = latGrid[n].childNodes[0].data\n",
    "    lon[n] = lonGrid[n].childNodes[0].data\n",
    "    line[n] = lineGrid[n].childNodes[0].data\n",
    "    pixel[n] = pixelGrid[n].childNodes[0].data\n",
    "\n",
    "# find zero pixel to rehape to array\n",
    "ind = find(pixel == 0)\n",
    "pixel = reshape(pixel, (ind.size, latGrid.length/ind.size))\n",
    "line = reshape(line, (ind.size, latGrid.length/ind.size))\n",
    "lat = reshape(lat, (ind.size, latGrid.length/ind.size))\n",
    "lon = reshape(lon, (ind.size, latGrid.length/ind.size))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# USING PIL IMAGE if no AGG with large file support\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "# get normalization func\n",
    "norm = Normalize(vmin=-1, vmax=+1)\n",
    "# normalize array\n",
    "imRoughness4326 = (norm(roughness_4326[p])*255).astype(np.uint8)\n",
    "# apply colormap to an array\n",
    "imRoughness4326 = (plt.cm.bone_r(imRoughness4326)*255).astype(np.uint8)\n",
    "# convert array to image\n",
    "imRoughness4326 = Image.fromarray(imRoughness4326)\n",
    "# convert to RGBA for transparency\n",
    "imRoughness4326 = imRoughness4326.convert('RGBA')\n",
    "oFileName = os.path.join(oPath+'proj/', fn[:-3]+p+'_4326_bone_r.png')\n",
    "imRoughness4326.save(oFileName)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Using PIL to make all white pixels transparent\n",
    "datas = img.getdata()\n",
    "\n",
    "newData = []\n",
    "for item in datas:\n",
    "    if item[0] == 255 and item[1] == 255 and item[2] == 255:\n",
    "        newData.append((255, 255, 255, 0))\n",
    "    else:\n",
    "        newData.append(item)\n",
    "\n",
    "img.putdata(newData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Areas for download\n",
    "# (minlon, minlat, maxlon, maxlat)\n",
    "Northern Arctic   (-180, 80, 180, 90)\n",
    "Norwegian Sea     (-45, 65, 20, 80)\n",
    "Barents Sea       (20, 60, 60, 80)\n",
    "Kara Sea          (60, 65, 100, 80)\n",
    "Laptev Sea        (100, 70, 140, 80)\n",
    "East Siberian Sea (140, 70, 180, 80)\n",
    "Beaufaurt Sea     (-180, 65, -120, 80)\n",
    "West Greenland    (-120, 60, -45, 80)\n",
    "Baltic            (10, 53, 34, 65)\n",
    "Black Sea         (27, 40, 42, 48)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# find the peaks value index to split the image in several parts\n",
    "from scipy.signal import argrelextrema, argrelmax\n",
    "from scipy.signal import find_peaks_cwt\n",
    "\n",
    "noiseLutInd = noiseLut_2[p][::scale,::scale].mean(axis=0)\n",
    "noiseLutInd = argrelextrema(noiseLutInd, greater)\n",
    "print noiseLutInd\n",
    "\n",
    "noiseLutInd = argrelmax(noiseLut_2[p][::scale,::scale].mean(axis=0))\n",
    "print noiseLutInd\n",
    "\n",
    "noiseLutInd = find_peaks_cwt(noiseLut_2[p][::scale,::scale].mean(axis=0), arange(1,100))\n",
    "print noiseLutInd\n",
    "\n",
    "noiseLutInd = find_peaks_cwt(sigma0wAvg[p], arange(1,800), noise_perc=0.1)\n",
    "print noiseLutInd\n",
    "\n",
    "noiseLutInd = find_peaks_cwt(sigma0wAvg[p], arange(10,22), noise_perc=0.1)\n",
    "print noiseLutInd\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
