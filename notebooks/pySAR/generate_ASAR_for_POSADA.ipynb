{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from ipywidgets import widgets\n",
    "# [widget for widget in dir(widgets) if widget.endswith('Widget')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import os, sys\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "import re\n",
    "import epr\n",
    "\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "from numpy import linspace, arange, row_stack, delete, round, double, asarray\n",
    "from math import atan\n",
    "\n",
    "from createMapsEtopo1 import findSubsetIndices\n",
    "import pygrib\n",
    "\n",
    "import pyresample as pr\n",
    "from pyproj import Proj\n",
    "import distancelib\n",
    "import gshhs_rasterize\n",
    "\n",
    "from pylab import *\n",
    "\n",
    "import simplekml\n",
    "\n",
    "\n",
    "import ConfigParser\n",
    "import redis\n",
    "\n",
    "# sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))), 'cmod'))\n",
    "# sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "\n",
    "__author__ = 'Alexander Myasoedov and Denis Spiridonov'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pxlRes = None\n",
    "proj = 'EPSG:3413'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set the number of CPUs\n",
    "# numProcs = cpu_count()-2\n",
    "numProcs = 6\n",
    "resolution = 150\n",
    "\n",
    "# redis_conf = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))), 'redis.conf')\n",
    "redis_conf = os.path.join('/home/mag/Documents/repos/solab/posada/', 'redis.conf')\n",
    "\n",
    "config = ConfigParser.RawConfigParser()\n",
    "config.read(redis_conf)\n",
    "redis_host = config.get('AUTH', 'HOSTNAME')\n",
    "redis_passwd = config.get('AUTH', 'PASSWORD')\n",
    "\n",
    "r = redis.Redis(host=redis_host, password=redis_passwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON parameter definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "asarJSON = {}\n",
    "asarJSON['variables_list'] = ['sigma0', 'wind_speed', 'roughness']\n",
    "asarJSON['min_max_values']  = {'sigma0':     [-35, 5],\n",
    "                               'wind_speed': [0, 35],\n",
    "                               'roughness':  [-1, 1]\n",
    "                            }\n",
    "asarJSON['is_land_masked']  = {'sigma0':     False,\n",
    "                               'wind_speed': True,\n",
    "                               'roughness':  False\n",
    "                        }\n",
    "asarJSON['polarizations_list'] = ['hh', 'vv', 'hv', 'vh']\n",
    "# 'u1' (NC_UBYTE)  2**8 =0-255\n",
    "# 'u2' (NC_USHORT  2**16=0-65535\n",
    "# 'u4' (NC_UINT)   2**32=0-4294967295\n",
    "# 'u8' (NC_UINT64) 2**64=0-18446744073709551615\n",
    "# if 2\\two data types are specified-we generate nc tyle pyramid and export data at max resolution to nc file\n",
    "asarJSON['nc_data_type'] = ['u1', 'u8']\n",
    "# asarJSON['nc_data_type'] = ['u1']\n",
    "\n",
    "with open('ASAR.json', 'w') as outfile:\n",
    "    json.dump(asarJSON, outfile, indent=4, separators=(',', ': '), sort_keys=True)\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Func Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def windDirection(u, v):\n",
    "    U = u.ravel()\n",
    "    V = v.ravel()\n",
    "    direction = zeros(size(U))\n",
    "    for i in range(0, len(U)):\n",
    "        if U[i] >= 0 and V[i] > 0: direction[i] = ((180 / pi) * atan(abs(U[i] / V[i])) + 180)\n",
    "        if U[i] < 0 and V[i] > 0: direction[i] = (-(180 / pi) * atan(abs(U[i] / V[i])) + 180)\n",
    "        if U[i] >= 0 and V[i] < 0: direction[i] = (-(180 / pi) * atan(abs(U[i] / V[i])) + 360)\n",
    "        if U[i] < 0 and V[i] < 0: direction[i] = ((180 / pi) * atan(abs(U[i] / V[i])))\n",
    "        if V[i] == 0 and U[i] > 0: direction[i] = 270\n",
    "        if V[i] == 0 and U[i] < 0: direction[i] = 90\n",
    "        if V[i] == 0 and U[i] == 0: direction[i] = 0\n",
    "    return reshape(direction, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PR_Mouche(theta, phi):\n",
    "    A_0 = 0.00650704\n",
    "    B_0 = 0.128983\n",
    "    C_0 = 0.992839\n",
    "    A_HALF_PI = 0.00782194\n",
    "    B_HALF_PI = 0.121405\n",
    "    C_HALF_PI = 0.992839\n",
    "    A_PI = 0.00598416\n",
    "    B_PI = 0.140952\n",
    "    C_PI = 0.992885\n",
    "\n",
    "    P_0 = A_0 * exp(B_0 * theta) + C_0\n",
    "    P_HALF_PI = A_HALF_PI * exp(B_HALF_PI * theta) + C_HALF_PI\n",
    "    P_PI = A_PI * exp(B_PI * theta) + C_PI\n",
    "\n",
    "    C0 = (P_0 + P_PI + 2 * P_HALF_PI) / 4\n",
    "    C1 = (P_0 - P_PI) / 2\n",
    "    C2 = (P_0 + P_PI - 2 * P_HALF_PI) / 4\n",
    "\n",
    "    P = C0 + C1 * cos(radians(phi)) + C2 * cos(radians(2 * phi))\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ncepGFSmodel(startTime, lats_2, lons_2):\n",
    "    \"\"\"\n",
    "    NCEP GFS model wind for givven time, lat/lon crop\n",
    "    \"\"\"\n",
    "    ncepGFSmodel = {} # empty dict for ncepGFSmodel\n",
    "\n",
    "#     iPath_wind = '/nfs1/store/model/ncep/gfs/'\n",
    "    iPath_wind = '/media/SOLabNFS2/store/model/ncep/gfs/'\n",
    "\n",
    "    # find the ncep gfs filename to open from ASAR filename\n",
    "    baseHour = floor((startTime.hour+3/2)/6)*6\n",
    "    baseHour = min(18, baseHour)\n",
    "    if startTime.hour-baseHour>1.5:\n",
    "        forecastHour = 3\n",
    "    else:\n",
    "        forecastHour = 0\n",
    "\n",
    "    if startTime <= datetime.datetime(2014, 8, 19):\n",
    "        ncepFileName = 'gfs' + startTime.strftime(\"%Y%m%d\") + '/gfs.t' + '%.2d' %(baseHour) + 'z.master.grbf' + '%.2d' %(forecastHour)\n",
    "\n",
    "        grbs = pygrib.open(iPath_wind + ncepFileName)\n",
    "\n",
    "        u_wind = None\n",
    "        v_wind = None\n",
    "\n",
    "        # wind contains u=u_wind.values[:], Lats=u_wind.latlons()[0], Lons=u_wind.latlons()[1]\n",
    "        for idx, msg_info in enumerate(grbs.select()):\n",
    "            if msg_info['short_name'] == '10u':\n",
    "                u_wind = grbs.message(idx + 1)\n",
    "            elif msg_info['short_name'] == '10v':\n",
    "                v_wind = grbs.message(idx + 1)\n",
    "\n",
    "        u = u_wind.values[:]\n",
    "        v = v_wind.values[:]\n",
    "        lats_wind = u_wind.latlons()[0]\n",
    "        lons_wind = u_wind.latlons()[1]\n",
    "    else:\n",
    "        try:\n",
    "            ncepFileName = 'gfs.' + startTime.strftime(\"%Y%m%d\") + '%.2d' %(baseHour) + '/gfs.t' + '%.2d' %(baseHour) + 'z.master.grbf' + '%.2d' %(forecastHour) + '.10m.uv.grib2'\n",
    "\n",
    "            grbs = pygrib.open(iPath_wind + ncepFileName)\n",
    "\n",
    "            u_wind = grbs.message(1)\n",
    "            v_wind = grbs.message(2)\n",
    "            u = u_wind['values']\n",
    "            v = v_wind['values']\n",
    "            lats_wind = u_wind['latitudes']\n",
    "            lons_wind = u_wind['longitudes']\n",
    "            lons_wind = reshape(lons_wind, (lons_wind.shape[0]/720, 720))\n",
    "            lats_wind = reshape(lats_wind, (lats_wind.shape[0]/720, 720))\n",
    "        except Exception:\n",
    "            ncepFileName = 'gfs.' + startTime.strftime(\"%Y%m%d\") + '%.2d' %(baseHour) + '/gfs.t' + '%.2d' %(baseHour) + 'z.pgrb2.0p25.f' + '%.3d' %(forecastHour)\n",
    "\n",
    "            grbs = pygrib.open(iPath_wind + ncepFileName)\n",
    "\n",
    "            u_wind = grbs.message(1)\n",
    "            v_wind = grbs.message(2)\n",
    "            u = u_wind['values']\n",
    "            v = v_wind['values']\n",
    "            lats_wind = u_wind['latitudes']\n",
    "            lons_wind = u_wind['longitudes']\n",
    "            lons_wind = reshape(lons_wind, (lons_wind.shape[0]/1440, 1440))\n",
    "            lats_wind = reshape(lats_wind, (lats_wind.shape[0]/1440, 1440))\n",
    "\n",
    "    #Make sure the longitude is between -180.00 .. 179.9\n",
    "    lons_wind = map(lambda x : (lons_wind.ravel()[x]+180)-int((lons_wind.ravel()[x]+180)/360)*360-180, range(0,lons_wind.size))\n",
    "    lons_wind = reshape(lons_wind, lats_wind.shape)\n",
    "    # plt.close('all')\n",
    "    # plt.imshow(lons_wind)\n",
    "    # plt.colorbar()\n",
    "\n",
    "#     #Make sure the latitudes is between -90.00 .. 89.9, starting from North - positive\n",
    "#     lats_wind = map(lambda x : (lats_wind.ravel()[x]+90)-int((lats_wind.ravel()[x]+90)/180)*180-90, xrange(0,lats_wind.size))\n",
    "#     lats_wind = reshape(lats_wind, lons_wind.shape)\n",
    "#     if lats_wind[0,0] < lats_wind[-1,-1]:\n",
    "#         lats_wind = flipud(lats_wind)\n",
    "#         u = flipud(u)\n",
    "#         v = flipud(v)\n",
    "#     plt.close('all')\n",
    "#     plt.imshow(lats_wind)\n",
    "#     plt.colorbar()\n",
    "\n",
    "\n",
    "    # find subset\n",
    "    res = findSubsetIndices(lats_2.min(),lats_2.max(),lons_2.min(),lons_2.max(),lats_wind[:,0],lons_wind[0,:])\n",
    "    # expand subset by 1 pixel for better further pyresample\n",
    "    res[0]=res[0]-2\n",
    "    res[1]=res[1]+2\n",
    "    res[2]=res[2]-2\n",
    "    res[3]=res[3]+2\n",
    "\n",
    "    # crop the data\n",
    "    u = u[int(res[2]):int(res[3]),int(res[0]):int(res[1])]\n",
    "    v = v[int(res[2]):int(res[3]),int(res[0]):int(res[1])]\n",
    "    ncepGFSmodel['lats_wind'] = lats_wind[int(res[2]):int(res[3]),int(res[0]):int(res[1])]\n",
    "    ncepGFSmodel['lons_wind'] = lons_wind[int(res[2]):int(res[3]),int(res[0]):int(res[1])]\n",
    "\n",
    "    ncepGFSmodel['wind_dir'] = windDirection(u,v)\n",
    "    ncepGFSmodel['wind_speed'] = sqrt(u**2 + v**2)\n",
    "    ncepGFSmodel['u'] = u\n",
    "    ncepGFSmodel['v'] = v\n",
    "    ncepGFSmodel['baseHour'] = baseHour\n",
    "    ncepGFSmodel['forecastHour'] = forecastHour\n",
    "#     del u_wind, v_wind\n",
    "    return ncepGFSmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_KML(area_extent, savepath):\n",
    "    kml = simplekml.Kml()\n",
    "\n",
    "    pol = kml.newpolygon(name='area_extent', visibility=1)\n",
    "    pol.tessellate = 1\n",
    "\n",
    "    pol.altitudemode = 'clampToGround'\n",
    "    # minx, miny, maxx, maxy\n",
    "    pol.outerboundaryis.coords = [(min(area_extent[0], area_extent[2]),\n",
    "                                   min(area_extent[1], area_extent[3])),\n",
    "                                  (max(area_extent[0], area_extent[2]),\n",
    "                                   max(area_extent[1], area_extent[3]))]\n",
    "    if type(savepath) == list:\n",
    "        for _savepath in savepath:\n",
    "            kml.save(_savepath)\n",
    "    else:\n",
    "        kml.save(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def swath_area_def(name='Temporal SWATH EPSG Projection 4326', proj='eqc', lonlim=(-180,180), latlim=(-90,90),\n",
    "                   ellps=\"WGS84\", res=111.2e3, lat_ts=None, lat_0=None, lon_0=None):\n",
    "    \"\"\"\n",
    "    Convert given swath coordinates to pyresample area definition.\n",
    "    The arguments are standard for Proj:\n",
    "    name\n",
    "    proj\n",
    "    lonlim\n",
    "    latlim\n",
    "    ellipsoid\n",
    "    resolution(meters)\n",
    "    lat_ts (latitude of true scale)\n",
    "    lat_0,lon_0 is central point\n",
    "    EXAMPLE:\n",
    "\n",
    "    epsg3426 is the default one\n",
    "    for epsg3413:\n",
    "    swath_area_def(name='Temporal SWATH EPSG Projection 3413', proj='stere', lonlim=(-180,180), latlim=(30,90), ellps=\"WGS84\", res=111.2e3, lat_ts=70, lat_0=90, lon_0=-45)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    up    = max(latlim)\n",
    "    down  = min(latlim)\n",
    "    left  = min(lonlim)\n",
    "    right = max(lonlim)\n",
    "\n",
    "    print 'up, down, left, right: ', round(up), round(down), round(left), round(right)\n",
    "\n",
    "    area_id = name.replace(\" \", \"_\").lower()\n",
    "    proj_id = area_id\n",
    "\n",
    "    if proj == 'eqc':\n",
    "        p = Proj(proj=proj, llcrnrlat=up, urcrnrlat=down, llcrnrlon=left, urcrnrlon=right, ellps=ellps)\n",
    "        proj4_args = '+proj=' + str(proj) + ' ' + \\\n",
    "             '+llcrnrlat=' + str(up) + ' ' + \\\n",
    "             '+urcrnrlat=' + str(down) + ' ' + \\\n",
    "             '+llcrnrlon=' + str(left) + ' ' + \\\n",
    "             '+urcrnrlon=' + str(right) + ' ' + \\\n",
    "             '+ellps=' + str(ellps)\n",
    "    elif lat_ts!=None and lat_0!=None:\n",
    "        # lat_ts is latitude of true scale.\n",
    "        # lon_0,lat_0 is central point.\n",
    "        p = Proj(proj=proj, lat_0=lat_0, lon_0=lon_0, lat_ts=lat_ts, ellps=ellps)\n",
    "        proj4_args = '+proj=' + str(proj) + ' ' + \\\n",
    "             '+lat_0=' + str(lat_0) + ' ' + \\\n",
    "             '+lon_0=' + str(lon_0) + ' ' + \\\n",
    "             '+lat_ts=' + str(lat_ts) + ' ' + \\\n",
    "             '+ellps=' + str(ellps)\n",
    "    elif lon_0!=None and lat_0!=None and lat_ts==None:\n",
    "        # lon_0,lat_0 is central point.\n",
    "        p = Proj(proj=proj, lat_0=lat_0, lon_0=lon_0, ellps=ellps)\n",
    "        proj4_args = '+proj=' + str(proj) + ' ' + \\\n",
    "             '+lat_0=' + str(lat_0) + ' ' + \\\n",
    "             '+lon_0=' + str(lon_0) + ' ' + \\\n",
    "             '+ellps=' + str(ellps)\n",
    "    elif lon_0==None and lat_0==None and lat_ts==None:\n",
    "        # lon_0,lat_0 is central point.\n",
    "        lat_0 = (up + down) / 2\n",
    "        lon_0 = (right + left) / 2\n",
    "        p = Proj(proj=proj, lat_0=lat_0, lon_0=lon_0, ellps=ellps)\n",
    "        proj4_args = '+proj=' + str(proj) + ' ' + \\\n",
    "             '+lat_0=' + str(lat_0) + ' ' + \\\n",
    "             '+lon_0=' + str(lon_0) + ' ' + \\\n",
    "             '+ellps=' + str(ellps)\n",
    "\n",
    "    left_ex1, up_ex1 = p(left, up)\n",
    "    right_ex1, up_ex2 = p(right, up)\n",
    "    left_ex2, down_ex1 = p(left, down)\n",
    "    right_ex2, down_ex2 = p(right, down)\n",
    "\n",
    "    if proj == 'stere':\n",
    "        lon = (left+right)/2.0\n",
    "        if (lon >=0 and lon <90) or (lon >=-360 and lon < -270):\n",
    "            print 11111111111\n",
    "            area_extent = (\n",
    "                           min(left_ex1, left_ex2, right_ex1, right_ex2),\n",
    "                           min(down_ex1, down_ex2, up_ex1, up_ex2),\n",
    "                           max(left_ex1, left_ex2, right_ex1, right_ex2),\n",
    "                           max(down_ex1, down_ex2, up_ex1, up_ex2)\n",
    "                        )\n",
    "        elif (lon >=90 and lon <180) or (lon >=-270 and lon < -180):\n",
    "            print 2222222222222\n",
    "            area_extent = (\n",
    "                           max(left_ex1, left_ex2, right_ex1, right_ex2),\n",
    "                           max(down_ex1, down_ex2, up_ex1, up_ex2),\n",
    "                           min(left_ex1, left_ex2, right_ex1, right_ex2),\n",
    "                           min(down_ex1, down_ex2, up_ex1, up_ex2)\n",
    "                        )\n",
    "        elif (lon >= 180 and lon < 270) or (lon >= -180 and lon < -90):\n",
    "            print 333333333333\n",
    "            area_extent = (\n",
    "                           min(left_ex1, left_ex2, right_ex1, right_ex2),\n",
    "                           min(down_ex1, down_ex2, up_ex1, up_ex2),\n",
    "                           max(left_ex1, left_ex2, right_ex1, right_ex2),\n",
    "                           max(down_ex1, down_ex2, up_ex1, up_ex2)\n",
    "                        )\n",
    "        else:\n",
    "            print 44444444444444444\n",
    "            area_extent = (\n",
    "                           min(left_ex1, left_ex2, right_ex1, right_ex2),\n",
    "                           min(down_ex1, down_ex2, up_ex1, up_ex2),\n",
    "                           max(left_ex1, left_ex2, right_ex1, right_ex2),\n",
    "                           max(down_ex1, down_ex2, up_ex1, up_ex2)\n",
    "                        )\n",
    "    else:\n",
    "        # минимум из всех координат X, Y, максимум из всех координат X, Y\n",
    "        # Такой результат даёт правильный area_extent для 3413\n",
    "        # При этом для 4326 area_extent остаётся неизменным\n",
    "        # area_def_3413 = swath_area_def(name='Temporal SWATH EPSG Projection 3413', proj='stere', \\\n",
    "        #                                lonlim=(-180,180), latlim=(30,90), ellps=\"WGS84\", res=1500, \\\n",
    "        #                                lat_ts=70, lat_0=90, lon_0=-45)\n",
    "        # Area extent: (-5050747.263141337, 0.0, 0.0, 5050747.263141336)\n",
    "        area_extent = (\n",
    "                        min(left_ex1, left_ex2, right_ex1, right_ex2),\n",
    "                        min(up_ex1, up_ex2, down_ex1, down_ex2),\n",
    "                        max(left_ex1, left_ex2, right_ex1, right_ex2),\n",
    "                        max(up_ex1, up_ex2, down_ex1, down_ex2)\n",
    "                    )\n",
    "\n",
    "    #~ print 'left: ', left_ex1, left_ex2\n",
    "    #~ print 'right: ', right_ex1, right_ex2\n",
    "    #~ print 'up: ', up_ex1, up_ex2\n",
    "    #~ print 'down: ', down_ex1, down_ex2\n",
    "\n",
    "#     Using abs() to avoid negative numbers of coloumns/rows as for epsg3413 for example\n",
    "    xsize = abs(int((area_extent[2] - area_extent[0]) / res[0]))\n",
    "    ysize = abs(int((area_extent[3] - area_extent[1]) / res[1]))\n",
    "\n",
    "    swath_area_def = pr.utils.get_area_def(area_id, name, proj_id, proj4_args, xsize, ysize, area_extent)\n",
    "\n",
    "#     print swath_area_def\n",
    "\n",
    "    return swath_area_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def imresize(image, size):\n",
    "    \"\"\"\n",
    "    Resizes coefficient arrays using bivariate spline approximation.\n",
    "    \"\"\"\n",
    "    m, n = image.shape\n",
    "    X = linspace(0, m - 1, size[0])\n",
    "    Y = linspace(0, n - 1, size[1])\n",
    "    kx, ky = min([m - 1, 3]), min([n - 1, 3])\n",
    "    interp = RectBivariateSpline(\n",
    "        arange(m), arange(n), image, kx=kx, ky=ky)\n",
    "    resized = interp(X, Y)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _format_extent_spacing(extent, spacing, extmax, midazimuth=False,\n",
    "                           midrange=False):\n",
    "    \"\"\"Format (and check) extent and spacing.\"\"\"\n",
    "    # Check extent\n",
    "    ext = round(extent).flatten()\n",
    "    if ext.size != 4:\n",
    "        raise Exception('extent must contain 4 elements')\n",
    "    if (ext[0:2] < extmax[0:2]).any() or (ext[2:4] > extmax[2:4]).any():\n",
    "        exttmp = array(ext)\n",
    "        ext[0:2] = maximum(ext[0:2], extmax[0:2])\n",
    "        ext[2:4] = minimum(ext[2:4], extmax[2:4])\n",
    "        print 'Warning : extent is outside SAR image, '+str(exttmp)+\\\n",
    "            ' becomes '+str(ext)\n",
    "    if (ext[0:2] > ext[2:4]).any():\n",
    "        raise Exception('extent[0:2] must be less or equal than '+\\\n",
    "                        'extent[2:4]')\n",
    "    # Check spacing\n",
    "    spa = round(spacing).flatten()\n",
    "    if spa.size == 1:\n",
    "        spa = repeat(spa[0], 2)\n",
    "    elif spa.size == 2:\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception('spacing must contain 1 or 2 elements')\n",
    "    if (spa < [1, 1]).any():\n",
    "        spatmp = array(spa)\n",
    "        spa = maximum(spa, [1, 1])\n",
    "        print 'Warning : spacing too small, '+str(spatmp)+' becomes '+\\\n",
    "            str(spa)\n",
    "    if (spa > ext[2:4]-ext[0:2]).any():\n",
    "        spatmp = array(spa)\n",
    "        spa = minimum(spa, ext[2:4]-ext[0:2])\n",
    "        print 'Warning : spacing too large, '+str(spatmp)+' becomes '+\\\n",
    "            str(spa)\n",
    "    # Make extent to be spacing modulo\n",
    "    ext[2:4] -= (ext[2:4]-ext[0:2]) % spa\n",
    "#     # 1D extent\n",
    "#     if midazimuth == True:\n",
    "#         dim = (ext[2]-ext[0]+1)/spa[0]\n",
    "#         ext[0:3:2] = ext[0] + (dim-1)//2*spa[0] + [0, spa[0]-1]\n",
    "#     if midrange == True:\n",
    "#         dim = (ext[3]-ext[1]+1)/spa[1]\n",
    "#         ext[1:4:2] = ext[1] + (dim-1)//2*spa[1] + [0, spa[1]-1]\n",
    "    return (ext, spa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_sigma0(iPath, fileName, pxlRes=800.0):\n",
    "\n",
    "    print os.path.join(iPath, fileName)\n",
    "    try:\n",
    "        product = epr.Product(os.path.join(iPath, fileName))\n",
    "    except:\n",
    "        print 'unable to read file'\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        band = product.get_band('proc_data')\n",
    "    except epr.EPRValueError:\n",
    "        print 'unable to get band \"proc_data\": epr_get_band_id: band not found'\n",
    "        return False\n",
    "\n",
    "    sc_w = double(product.get_scene_width())\n",
    "    sc_h = double(product.get_scene_height())\n",
    "    \n",
    "\n",
    "    print 'sc_w*sc_h = ', sc_w * sc_h\n",
    "    if sc_w*sc_h > 60000000:\n",
    "#     if sc_w*sc_h > 30000000:\n",
    "        print \"ASAR Image too large, skipping\"\n",
    "        return False\n",
    "\n",
    "    # Get lat/lon from geolocation grid\n",
    "    dataset = product.get_dataset('GEOLOCATION_GRID_ADS')\n",
    "    fltp_lats = map(\n",
    "        lambda x:\n",
    "        dataset.read_record(x).get_field('first_line_tie_points.lats').get_elems(),\n",
    "        range(dataset.get_num_records())\n",
    "    )\n",
    "    lltp_lats = map(\n",
    "        lambda x:\n",
    "        dataset.read_record(x).get_field('last_line_tie_points.lats').get_elems(),\n",
    "        range(dataset.get_num_records())\n",
    "    )\n",
    "    fltp_lons = map(\n",
    "        lambda x:\n",
    "        dataset.read_record(x).get_field('first_line_tie_points.longs').get_elems(),\n",
    "        range(dataset.get_num_records())\n",
    "    )\n",
    "    lltp_lons = map(\n",
    "        lambda x:\n",
    "        dataset.read_record(x).get_field('last_line_tie_points.longs').get_elems(),\n",
    "        range(dataset.get_num_records())\n",
    "    )\n",
    "\n",
    "    fltp_lats = asarray(double(fltp_lats))/1e6\n",
    "    lltp_lats = asarray(double(lltp_lats))/1e6\n",
    "    fltp_lons = asarray(double(fltp_lons))/1e6\n",
    "    lltp_lons = asarray(double(lltp_lons))/1e6\n",
    "\n",
    "    lats = row_stack((fltp_lats, lltp_lats[-1, :]))\n",
    "    lons = row_stack((fltp_lons, lltp_lons[-1, :]))\n",
    "\n",
    "    lats = fliplr(lats)\n",
    "    lons = fliplr(lons)\n",
    "\n",
    "    # Find scale to reduce image to the specified resolution\n",
    "    arrShape =  asarray([sc_w, sc_h])\n",
    "    _lats = asarray([lats[0,0], lats[-1,-1], lats[0,-1], lats[-1,0]])\n",
    "    _lons = asarray([lons[0,0], lons[-1,-1], lons[0,-1], lons[-1,0]])\n",
    "    imageRes = round(mean(asarray(distancelib.getPixelResolution(_lats, \\\n",
    "                                                                 _lons, \\\n",
    "                                                                 arrShape, 'km'))*1e3))\n",
    "    scale = pxlRes/imageRes\n",
    "\n",
    "    extMax = (0.,0.,arrShape[0]-1,arrShape[1]-1)\n",
    "    ext    = (0.,0.,arrShape[0]-1,arrShape[1]-1)\n",
    "\n",
    "    # Format extent/spacing\n",
    "    ext, spa = _format_extent_spacing(extent=ext, spacing = scale, extmax=extMax)\n",
    "    \n",
    "    # Read data with stepping=spacing\n",
    "    try:\n",
    "        raw_counts = band.read_as_array(sc_w, sc_h, xstep=spa[0], ystep=spa[1])\n",
    "        incident_angle = product.get_band('incident_angle').read_as_array(sc_w, sc_h, xstep=spa[0], ystep=spa[1])\n",
    "    except epr.EPRValueError:\n",
    "        print \"EPRValueError\"\n",
    "        return False\n",
    "\n",
    "    \n",
    "    lats_2 = imresize(lats, raw_counts.shape)\n",
    "    lons_2 = imresize(lons, raw_counts.shape)\n",
    "\n",
    "#     if lats.max() <= 35:\n",
    "#         print \"skipping no area overlap\"\n",
    "#         return False\n",
    "\n",
    "    # Trimming the array by removing zero values from rows and cols\n",
    "    msk = []\n",
    "    for m in range(raw_counts.shape[0]):\n",
    "        if raw_counts[m, :].sum() == 0:\n",
    "            msk.append(m)\n",
    "    raw_counts = delete(raw_counts, msk, axis=0)\n",
    "    lats_2 = delete(lats_2, msk, axis=0)\n",
    "    lons_2 = delete(lons_2, msk, axis=0)\n",
    "    incident_angle = delete(incident_angle, msk, axis=0)\n",
    "    polarization = product.get_sph().get_field('MDS1_TX_RX_POLAR').get_elem()\n",
    "\n",
    "    msk = []\n",
    "    for n in range(raw_counts.shape[1]):\n",
    "        if raw_counts[:, n].sum() == 0:\n",
    "            msk.append(n)\n",
    "    raw_counts = delete(raw_counts, msk, axis=1)\n",
    "    lats_2 = delete(lats_2, msk, axis=1)\n",
    "    lons_2 = delete(lons_2, msk, axis=1)\n",
    "    incident_angle = delete(incident_angle, msk, axis=1)\n",
    "\n",
    "    # Adding Sigma_0\n",
    "    calibration_constant = \\\n",
    "    product.get_dataset('MAIN_PROCESSING_PARAMS_ADS').read_record(0).get_field('calibration_factors.1.ext_cal_fact').get_elems()\n",
    "    # sigma0 = 10*log10( raw_counts**2*sin(incident_angle*pi/180)/calibration_constant )\n",
    "    sigma0 = raw_counts**2*sin(incident_angle*pi/180)/calibration_constant\n",
    "    \n",
    "    return sigma0, lats_2, lons_2, incident_angle, polarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_roughness(sigma0, incident_angle, polarisation):\n",
    "    \"\"\"Compute sea surface roughness.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma0 : ndarray\n",
    "        NRCS backscatter.\n",
    "    incident_angle : ndarray\n",
    "        Incidence angle in degrees.\n",
    "    polarisation : str\n",
    "        'VV' or 'HH' or 'VH' or 'HV'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    roughness: ndarray\n",
    "    \"\"\"\n",
    "    \n",
    "    # Lee-Wiener filtering blures to much the ASAR scenes, so we use it only for wind field\n",
    "    # from scipy.signal import wiener\n",
    "    # sigma0w = wiener(sigma0, mysize=(3,3), noise=None)\n",
    "    # sigma0w = sigma0\n",
    "\n",
    "    # # Earlier form Fabrice - simplyfied\n",
    "    # polarization = product.get_sph().get_field('MDS1_TX_RX_POLAR').get_elem()\n",
    "    # if polarization == 'H/H':\n",
    "    #     ph = (2.20495, -14.3561e-2, 11.28e-4)\n",
    "    #     sigma0_hh_ref = exp( ( ph[0]+incident_angle*ph[1]+incident_angle**2*ph[2])*log(10) )\n",
    "    #     roughness = sigma0w/sigma0_hh_ref\n",
    "    # elif polarization == 'V/V':\n",
    "    #     pv = (2.29373, -15.393e-2, 15.1762e-4)\n",
    "    #     sigma0_vv_ref = exp( ( pv[0]+incident_angle*pv[1]+incident_angle**2*pv[2])*log(10) )\n",
    "    #     roughness = sigma0w/sigma0_vv_ref\n",
    "\n",
    "    # From sar/cerbere\n",
    "    from cmod_vect import cmod5n_forward as cmod5\n",
    "    if polarisation == 'VV' or polarisation == 'V/V': # Use cmod5\n",
    "        sigma0_vv = cmod5(10, 45, incident_angle)\n",
    "        return sigma0/sigma0_vv\n",
    "    elif polarisation == 'HH' or polarisation == 'H/H': # Use cmod5 and Thompson polarisation ratio\n",
    "        sigma0_vv = cmod5(10, 45, incident_angle)\n",
    "        alpha = 0.7\n",
    "        polrat = (1 + 2*tan(incident_angle*pi/180)**2)**2 / \\\n",
    "                 (1 + alpha*tan(incident_angle*pi/180)**2)**2\n",
    "        return sigma0/sigma0_vv*polrat\n",
    "    elif polarisation == 'VH' or polarisation == 'HV' \\\n",
    "      or polarisation == 'V/H' or polarisation == 'H/V': # Use simple model\n",
    "        # nrcs_vh_db = 0.580*wsp - 35.652\n",
    "        # nrcs_vh_lin = 10^(nrcs_vh_db/10.)\n",
    "        sigma0_cross = 10**((0.58*10-35.652)/10)\n",
    "        return sigma0/sigma0_cross\n",
    "    else:\n",
    "        raise Exception('Unknown polarisation : '+polarisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_wind(fileName, sigma0w, lats_2, lons_2, incident_angle, polarization):\n",
    "    # Adding Model wind\n",
    "    startTime = datetime.datetime.strptime(fileName[14:29], \"%Y%m%d_%H%M%S\")\n",
    "    ncepGFSmodelWind = ncepGFSmodel(startTime, lats_2, lons_2)\n",
    "    if not ncepGFSmodelWind:\n",
    "        return False\n",
    "\n",
    "    # Reprojecting data\n",
    "\n",
    "    # Pixel resolution\n",
    "    # we use pxlResWind/pxlResSAR for further pyresample radius_of_influence and sigmas\n",
    "    try:\n",
    "        pxlResWind = asarray(\n",
    "            distancelib.getPixelResolution(ncepGFSmodelWind['lats_wind'],\n",
    "                                           ncepGFSmodelWind['lons_wind'],\n",
    "                                           ncepGFSmodelWind['lats_wind'].shape,\n",
    "                                           'km')\n",
    "        )\n",
    "    except IndexError:\n",
    "        return False\n",
    "    pxlResSAR = asarray(\n",
    "        distancelib.getPixelResolution(lats_2, lons_2, lons_2.shape, 'km')\n",
    "    )*1e3\n",
    "    # Note pxlResWind is in KM, multiply by 1e3 for meters\n",
    "#    print \"ASAR cell resolution, %s m\"  % pxlResSAR\n",
    "#    print \"Wind cell resolution, %s km\" % pxlResWind\n",
    "\n",
    "    # reproject NCEP onto ASAR grid before calculations\n",
    "    # Try both BivariateSpline, griddata and pyresample\n",
    "\n",
    "    ncep_def = pr.geometry.GridDefinition(lons=ncepGFSmodelWind['lons_wind'],\n",
    "                                          lats=ncepGFSmodelWind['lats_wind'])\n",
    "    swath_def = pr.geometry.SwathDefinition(lons=lons_2, lats=lats_2)\n",
    "\n",
    "    # wind_speed_model_swath = pr.kd_tree.resample_gauss(\n",
    "    #     ncep_def, ncepGFSmodelWind['wind_speed'].ravel(), swath_def,\n",
    "    #     radius_of_influence=2*pxlResWind.max()*1e3, neighbours=12,\n",
    "    #     sigmas=pxlResWind.max()*1e3, fill_value=None, nprocs=numProcs\n",
    "    # )\n",
    "    wind_dir_model_swath = pr.kd_tree.resample_gauss(\n",
    "        ncep_def, ncepGFSmodelWind['wind_dir'].ravel(), swath_def,\n",
    "        radius_of_influence=2*pxlResWind.max()*1e3, neighbours=12,\n",
    "        sigmas=pxlResWind.max()*1e3, fill_value=None, nprocs=numProcs\n",
    "    )\n",
    "\n",
    "    # calculate bearing from initial lats/lons for further wind calculation\n",
    "    bearing = zeros((lons.shape[0]-1, lons.shape[1]))\n",
    "\n",
    "    for n in range(0, lons.shape[1]):\n",
    "        col = ([lats[:-1, n], lons[:-1, n]], [lats[1:, n], lons[1:, n]])\n",
    "        for m in range(0, lons.shape[0]-1):\n",
    "            bearing[m][n] = distancelib.bearing(asarray(col[0])[:, m],\n",
    "                                                asarray(col[1])[:, m])\n",
    "\n",
    "    # interpolate to raw_counts.shape\n",
    "    bearing_2 = imresize(bearing, raw_counts.shape)\n",
    "\n",
    "    # NB! WINDDIR = 0 WHEN WIND BLOWS TOWARDS RADAR!\n",
    "    wind_dir_model_swath_rel = 90 + bearing_2 - wind_dir_model_swath\n",
    "\n",
    "    if polarization == 'H/H':\n",
    "        PR = PR_Mouche(incident_angle, wind_dir_model_swath_rel)\n",
    "        try:\n",
    "            from cmod_gpu import rcs2windOpenCl\n",
    "            wind_speed_asar = rcs2windOpenCl(sar=sigma0w*PR,\n",
    "                                             windir=wind_dir_model_swath_rel,\n",
    "                                             theta=incident_angle)\n",
    "        except Exception:\n",
    "            from cmod_vect import rcs2windPar\n",
    "            wind_speed_asar = rcs2windPar(sigma0w*PR, cmdv=5,\n",
    "                                          windir=wind_dir_model_swath_rel,\n",
    "                                          theta=incident_angle,\n",
    "                                          nprocs=numProcs)\n",
    "    elif polarization == 'V/V':\n",
    "        try:\n",
    "            from cmod_gpu import rcs2windOpenCl\n",
    "            wind_speed_asar = rcs2windOpenCl(sar=sigma0w,\n",
    "                                             windir=wind_dir_model_swath_rel,\n",
    "                                             theta=incident_angle)\n",
    "        except Exception:\n",
    "            from cmod_vect import rcs2windPar\n",
    "            wind_speed_asar = rcs2windPar(sigma0w, cmdv=5,\n",
    "                                          windir=wind_dir_model_swath_rel,\n",
    "                                          theta=incident_angle,\n",
    "                                          nprocs=numProcs)\n",
    "    \n",
    "    return wind_speed_asar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 2012 / Day: 061\n"
     ]
    }
   ],
   "source": [
    "asar_path = '/nfs1/store/satellite/asar/'\n",
    "# _dir = '/nfs1/store/satellite/asar/2010/270'\n",
    "# fileName = 'ASA_WSM_1PNPDK20100927_195408_000000862093_00200_44843_3688.N1'\n",
    "_dir = '/nfs1/store/satellite/asar/2012/078'\n",
    "fileName = 'ASA_WSM_1PNPDE20120318_034254_000001043112_00392_52561_3510.N1'\n",
    "\n",
    "_dir = '/nfs1/store/satellite/asar/2012/061/'\n",
    "fileName = 'ASA_WSM_1PNPDE20120301_091843_000001163112_00151_52320_7147.N1'\n",
    "\n",
    "input_filename = os.path.join(_dir, fileName)\n",
    "dt = datetime.datetime.strptime(\n",
    "    fileName[14:22], '%Y%m%d'\n",
    ").timetuple()\n",
    "year = str(dt.tm_year)\n",
    "day = str(dt.tm_yday)\n",
    "\n",
    "if len(day) == 1:\n",
    "    day = '00' + day\n",
    "elif len(day) == 2:\n",
    "    day = '0' + day\n",
    "print \"Year: %s / Day: %s\"  % (year, day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigma0 calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating sigma0.......\n",
      "/nfs1/store/satellite/asar/2012/061/ASA_WSM_1PNPDE20120301_091843_000001163112_00151_52320_7147.N1\n",
      "sc_w*sc_h =  56614680.0\n"
     ]
    }
   ],
   "source": [
    "print \"Calculating sigma0.......\"\n",
    "\n",
    "sigma0, lats_2, lons_2, incident_angle, polarization = compute_sigma0(_dir, fileName, pxlRes=resolution)\n",
    "# roughness = compute_roughness(sigma0, incident_angle, polarization)\n",
    "# wind_speed_asar = compute_wind(fileName, sigma0, lats_2, lons_2, incident_angle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Reprojecting and Masking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Get first guess pixel resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting first guess pixel resolution from Image.......\n",
      "       S1 cell resolution, [ 0.0013616   0.00134683] deg\n",
      "       S1 cell resolution, [ 151.30106287  149.65924579] m\n"
     ]
    }
   ],
   "source": [
    "print \"Getting first guess pixel resolution from Image.......\"\n",
    "\n",
    "# Pixel resolution \n",
    "# we use pxlRes for further GSHHS rasterizing and\n",
    "# reprojecting data with pyresample\n",
    "\n",
    "lonlim = (lons_2.min(), lons_2.max())\n",
    "latlim = (lats_2.min(), lats_2.max())\n",
    "\n",
    "# enlarge lonlims for cropping a bit larger area for masking\n",
    "lonlimGSHHS = (lonlim[0]-1.0, lonlim[1]+1.0)\n",
    "latlimGSHHS = (latlim[0]-1.0, latlim[1]+1.0)\n",
    "\n",
    "pxlRes = asarray(\n",
    "    distancelib.getPixelResolution(lats_2, lons_2, lons_2.shape, 'km')\n",
    ")*1e3\n",
    "pxlResDeg = asarray(\n",
    "    distancelib.getPixelResolution(lats_2, lons_2,lons_2.shape, 'deg')\n",
    ")\n",
    "\n",
    "print \"       S1 cell resolution, %s deg\" % str(pxlResDeg)\n",
    "print \"       S1 cell resolution, %s m\" % str(pxlRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining areas.......\n",
      "    reprojecting to EPSG:3413\n",
      "up, down, left, right:  40.0 31.0 10.0 17.0\n",
      "11111111111\n"
     ]
    }
   ],
   "source": [
    "print \"Defining areas.......\"\n",
    "\n",
    "# Define areas with pyresample\n",
    "swath_def = pr.geometry.SwathDefinition(\n",
    "    lons=lons_2, lats=lats_2\n",
    ")\n",
    "\n",
    "# for proj in ['EPSG:4326', 'EPSG:3413']:\n",
    "# for proj in ['EPSG:4326']:\n",
    "print \"    reprojecting to %s\" % proj\n",
    "if proj == 'EPSG:4326':\n",
    "    area_def = swath_area_def(name='Temporal SWATH EPSG Projection 4326',\n",
    "                              proj='eqc',\n",
    "                              lonlim=lonlimGSHHS,\n",
    "                              latlim=latlimGSHHS, ellps=\"WGS84\",\n",
    "                              res=pxlRes)\n",
    "    # Set the parameters for GSHHS masking\n",
    "    area_def.proj_ = '4326'\n",
    "    area_def.proj_name = None\n",
    "    area_def.units = 'deg'\n",
    "elif proj == 'EPSG:3413':\n",
    "    area_def = swath_area_def(name='Temporal SWATH EPSG Projection 3413',\n",
    "                              proj='stere',\n",
    "                              lonlim=lonlimGSHHS,\n",
    "                              latlim=latlimGSHHS, ellps=\"WGS84\",\n",
    "                              res=pxlRes,\n",
    "                              lat_ts=70, lat_0=90, lon_0=-45)\n",
    "    # Set the parameters for GSHHS masking\n",
    "    area_def.proj_ = '+units=m +ellps=WGS84 +lon_0=-45 +proj=stere +lat_ts=70 +lat_0=90'\n",
    "    area_def.proj_name = '3413'\n",
    "    area_def.units = 'm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recalculating Pixel resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recalculating Pixel resolution.......\n",
      "       S1 cell resolution, [ 0.00080228  0.00096307] deg\n",
      "       S1 cell resolution, [ 151.30431734  149.66438356] m\n"
     ]
    }
   ],
   "source": [
    "print \"Recalculating Pixel resolution.......\"\n",
    "\n",
    "# Get the SAR pixel resolution from the area_def\n",
    "# for further identical shapes\n",
    "up = min(latlimGSHHS)\n",
    "down = max(latlimGSHHS)\n",
    "left = min(lonlimGSHHS)\n",
    "right = max(lonlimGSHHS)\n",
    "area_extent_deg = (left, down, right, up)\n",
    "\n",
    "area_extent_deg_shape = area_def.shape\n",
    "\n",
    "pxlResDeg = asarray(\n",
    "    (abs(area_extent_deg[2] - area_extent_deg[0]) /\n",
    "        float(area_extent_deg_shape[1]),\n",
    "     abs(area_extent_deg[3] - area_extent_deg[1]) /\n",
    "        float(area_extent_deg_shape[0]))\n",
    ")\n",
    "\n",
    "pxlRes = asarray(\n",
    "    (abs(area_def.pixel_size_x), abs(area_def.pixel_size_y))\n",
    ")\n",
    "print \"       S1 cell resolution, %s deg\" % str(pxlResDeg)\n",
    "print \"       S1 cell resolution, %s m\" % str(pxlRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reprojecting non masked arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reprojecting data to new projection(s).......\n"
     ]
    }
   ],
   "source": [
    "print \"Reprojecting data to new projection(s).......\"\n",
    "\n",
    "sigma0_res = pr.kd_tree.resample_nearest(\n",
    "    swath_def, sigma0.ravel(), area_def,\n",
    "    radius_of_influence=4*pxlRes.max(),\n",
    "    epsilon=0.5, fill_value=None, nprocs=numProcs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if mask = True:\n",
    "    \n",
    "# else:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masking (only for Wind and Roughness)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print \"Masking.......\"\n",
    "\n",
    "# Apply Mask from GSHHS\n",
    "reload(gshhs_rasterize)\n",
    "\n",
    "# ESRI shapefile containing land polygons\n",
    "shapefile = '/media/SOLabNFS/store/auxdata/coastline/GSHHS_shp/f/GSHHS_f_L1.shp'\n",
    "lakes = True\n",
    "\n",
    "# Doesn't work for now using 4326 and then pyresampling\n",
    "# mask_arr_3413 = gshhs_rasterize.gshhs_rasterize(\n",
    "#     lonlimGSHHS, latlimGSHHS,\n",
    "#     area_def, lakes, shapefile)\n",
    "\n",
    "print \"       Rasterizing Land Mask\"\n",
    "\n",
    "# Rasterizing Land Mask\n",
    "mask_arr_4326 = gshhs_rasterize.gshhs_rasterize_4326(\n",
    "    lonlimGSHHS, latlimGSHHS, pxlResDeg, True, shapefile)\n",
    "\n",
    "print \"       Reprojecting GSHHS raster onto swath grid\"\n",
    "\n",
    "# reproject GSHHS onto swath grid before calculations\n",
    "mask_arr_swath = pr.kd_tree.resample_nearest(\n",
    "    area_def_4326, mask_arr_4326.ravel(), swath_def,\n",
    "    radius_of_influence=4*pxlRes.max(), epsilon=0.5,\n",
    "    fill_value=None, nprocs=numProcs\n",
    ")\n",
    "\n",
    "# Make masked array\n",
    "sigma0w = ma.masked_where(mask_arr_swath, sigma0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reprojecting masked arrays"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print \"Reprojecting masked data to new projection(s).......\"\n",
    "\n",
    "sigma0w_res = pr.kd_tree.resample_nearest(\n",
    "    swath_def, sigma0w[p], area_def_3413,\n",
    "    radius_of_influence=4*pxlRes.max(), epsilon=0.5, fill_value=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create NC tiles"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "num_of_files = len(\\\n",
    "[fns for dn,_,fns in os.walk(asar_path) for fn in fns if fn.startswith('ASA_WSM') and fn.endswith('.N1')])\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "for _dir, sub_dir, _files in tqdm(os.walk(asar_path), total=num_of_files):\n",
    "    for fileName in _files:\n",
    "        if fileName.startswith('ASA_WSM') and fileName.endswith('.N1'):\n",
    "            sleep(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/mag/Documents/repos/solab/posada/handlers/')\n",
    "\n",
    "import Tiles.nctiles\n",
    "reload(Tiles.nctiles)\n",
    "# Переделать чтобы сначала маскировался массив и перегонялся в uint8, а потом уде разбивать на тайлы\n",
    "# Может будет быстрее\n",
    "# исрользовать numba или numexpr, cpython\n",
    "# оказалось, что не быстрее\n",
    "from Tiles.nctiles import create_nc_tiles, write_attrib_to_nc, array_size_normalize\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "def get_date_parameters(granule_name):\n",
    "    ''' return year, month and day for current granule\n",
    "\n",
    "    '''\n",
    "    prog = re.compile(r'(\\d{8})')\n",
    "    file_date = prog.findall(granule_name)[0]\n",
    "    prog = re.compile(r'(\\d{6})')\n",
    "    file_time = prog.findall(granule_name)[1]\n",
    "    year = file_date[:4]\n",
    "    month = file_date[4:6]\n",
    "    day = file_date[6:]\n",
    "    \n",
    "    startTime = datetime.datetime.strptime(year+month+day+file_time,\n",
    "    \"%Y%m%d%H%M%S\")\n",
    "\n",
    "    return year, month, day, startTime\n",
    "\n",
    "def mkdirs(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%timeit -r 1 -n 1\n",
    "\n",
    "# def create_nc_tiles(inpath, fn, out_dir, scale=1):\n",
    "print \"Creating NC tiles.......\"\n",
    "\n",
    "pn = '/home/mag/Documents/repos/solab/PySOL/notebooks/pySAR/'\n",
    "pn = '/nfs1/store/nctiles/SOLAB_ASAR/epsg_3413/'\n",
    "pn = '/media/SOLabNFS/hyrax-cluster/data/public/allData/tzh/'\n",
    "\n",
    "granule_name = fileName[:-3]\n",
    "\n",
    "year, month, day, startTime = get_date_parameters(granule_name)\n",
    "\n",
    "out_pn = os.path.join(pn, year, month, day)\n",
    "nc_path = os.path.join(out_pn, granule_name+'.nc')\n",
    "\n",
    "mkdirs(os.path.join(pn, year))\n",
    "mkdirs(os.path.join(pn, year, month))\n",
    "mkdirs(os.path.join(pn, year, month, day))\n",
    "\n",
    "if os.path.isfile(nc_path):\n",
    "    os.remove(nc_path)\n",
    "# if not os.path.isdir(os.path.dirname(nc_path)):\n",
    "#     os.makedirs(os.path.dirname(nc_path))\n",
    "\n",
    "# set Polarization formatting to comply with standards\n",
    "p = polarization.lower().replace('/', '')\n",
    "max_zoom_level = create_nc_tiles(10*log10(sigma0_res), 'sigma0', nc_path, 'u1', p, configpath='ASAR.json')\n",
    "# max_zoom_level = create_nc_tiles(10*log10(sigma0_res), 'sigma0', nc_path, polarization=p, configpath=pn+'ASAR.json')\n",
    "write_attrib_to_nc(nc_path, area_def, startTime,\n",
    "                   max_zoom_level, resolution, {p})\n",
    "\n",
    "print \"%skb\" % str(np.round(os.path.getsize(nc_path)/1024))\n",
    "print \"%sMb\" % str(np.round(os.path.getsize(nc_path)/1024/1024))\n",
    "\n",
    "r.flushall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset as ncDataset\n",
    "dataset = ncDataset(nc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nc_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset.polarizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_array = array_size_normalize(10*log10(sigma0_res), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_array[6000,3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imshow(source_array, vmin=-35, vmax=5)\n",
    "colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imshow(10*log10(sigma0_res.data), vmin=-35, vmax=5)\n",
    "colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache the data from a specified set of netCDF files\n",
    "#### Request only one tile from every zoom level to cache all zoom levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset as ncDataset\n",
    "\n",
    "# granule = fileName[:-3]+'.nc'\n",
    "\n",
    "# granule_list = listdir(pn)\n",
    "out_pn = '/nfs1/store/nctiles/SOLAB_ASAR/epsg_3413/2012/03/18/'\n",
    "granule_list = [s for s in os.listdir(out_pn) if s.endswith('.nc')]\n",
    "\n",
    "# var_list = ['sigma0w', 'wind_speed']\n",
    "var_list = ['sigma0']\n",
    "urls = []\n",
    "coords = []\n",
    "\n",
    "for granule in granule_list:\n",
    "    dataset = ncDataset(os.path.join(out_pn,granule))\n",
    "    coords = dataset.variables['Data'].shape\n",
    "    for var in var_list:\n",
    "        for zoom in range(coords[2]):\n",
    "            urls.append(\n",
    "                'http://10.170.0.153/wms?product=SOLAB_ASAR&' +\n",
    "                'variable=' + var +\n",
    "                '&granule=' + granule[:-3] +\n",
    "                '&projection=EPSG%3A3413&polarization=vv&vmin=63&vmax=191&' +\n",
    "                'zoom=' + str(zoom) +\n",
    "                '&x=' + str(0) +\n",
    "                '&y=' + str(0))\n",
    "urls.sort()\n",
    "\n",
    "print len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "for u in urls:\n",
    "    request = urlopen(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "request = urlopen(urls[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import redis\n",
    "r = redis.Redis(host='10.170.0.153', password='jM8vBgR4', db=0)\n",
    "r.dbsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r.flushall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "def ntrctv_imshow(p = 'hh', vmi=-1., vma=1., cmap='bone', crop='[:,:]', dimNum=0):\n",
    "    # check if data type is dictionary and there is no wind\n",
    "    if type(data) == dict and all(k!='wind_speed' for k in data.iterkeys()):\n",
    "        plt.figure(figsize=(8,8*double(data[p].shape[0])/double(data[p].shape[1])))\n",
    "        plt.imshow(eval(\"data[p]\"+str(crop)), vmin=vmi, vmax=vma)\n",
    "    elif type(data) == dict and any(k=='wind_speed' for k in data.iterkeys()):\n",
    "        plt.figure(figsize=(8,8*double(data[p].shape[0])/double(data[p].shape[1])))\n",
    "        X,Y = meshgrid( arange(0,eval(\"data[p]\"+str(crop)).shape[1]),arange(0,eval(\"data[p]\"+str(crop)).shape[0]) )\n",
    "        U = (data['u'])\n",
    "        V = (data['v'])\n",
    "        U = eval(\"U\"+str(crop))\n",
    "        V = eval(\"V\"+str(crop))\n",
    "        scl = 100\n",
    "        plt.quiver(X[::scl,::scl], Y[::scl,::scl], U[::scl,::scl], V[::scl,::scl])\n",
    "        plt.imshow(eval(\"data[p]\"+str(crop)), vmin=vmi, vmax=vma)\n",
    "        plt.axis('tight')\n",
    "    elif type(data) != dict and len(data.shape)>2:\n",
    "        plt.figure(figsize=(8,8*double(data[:,:,0].shape[0])/double(data[:,:,0].shape[1])))\n",
    "        plt.imshow(eval(\"data[:,:,\"+str(dimNum)+\"]\"+str(crop)), vmin=vmi, vmax=vma)\n",
    "    elif type(data) != dict:\n",
    "        plt.figure(figsize=(8,8*double(data.shape[0])/double(data.shape[1])))\n",
    "        plt.imshow(eval(\"data\"+str(crop)), vmin=vmi, vmax=vma)\n",
    "    plt.colorbar()\n",
    "    plt.set_cmap(cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scale = 1\n",
    "# data = 10*log10(sigma0)[::scale,::scale]\n",
    "data = 10*log10(sigma0_res)[::scale,::scale]\n",
    "\n",
    "ntrctv = widgets.interact(ntrctv_imshow, \\\n",
    "                          vmi=widgets.FloatSlider(min=-30, max=10, value=-20, step=1), \\\n",
    "                          vma=widgets.FloatSlider(min=-30, max=10, value=5, step=1), \\\n",
    "                          cmap = ['Greys_r', 'bone', 'RdBu_r'], crop = '[:,:]', \\\n",
    "                          dimNum=['0']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Попытка ускорить генерацию тайлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_array = array_size_normalize(10*log10(sigma0_res), False)\n",
    "number_tiles = source_array.shape[0]/256\n",
    "zoom_level = int(math.log(number_tiles, 2))\n",
    "\n",
    "out_array = np.ones([zoom_level+1, number_tiles, number_tiles, 256, 256])\n",
    "\n",
    "_min = -20\n",
    "_max = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16384, 16384)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decrease_zoom(source_array):\n",
    "    rows, cols = source_array.shape\n",
    "    rows_2 = rows/2\n",
    "    cols_2 = cols/2\n",
    "    sh = rows_2, rows//rows_2, cols_2, cols//cols_2\n",
    "    return source_array.reshape(sh).mean(-1).mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def array_slice(m):\n",
    "    m = ma.where(m <= _min, _min + 0.0001, m)\n",
    "    m = ma.where(m >= _max, _max, m)\n",
    "    number_tiles = m.shape[0]/256\n",
    "    result = np.zeros([number_tiles, number_tiles, 256, 256])\n",
    "    # No need to use itertools etc. as this gives no speed up in this looping\n",
    "    for i in xrange(number_tiles):\n",
    "        for j in xrange(number_tiles):\n",
    "            result[i,j,:] = m[j*256:(j+1)*256, i*256:(i+1)*256]\n",
    "\n",
    "    return result\n",
    "\n",
    "def saveResult(result):\n",
    "    results.append(result)\n",
    "\n",
    "def test_par(source_array):\n",
    "    number_tiles = source_array.shape[0]/256\n",
    "    zoom_level = int(math.log(number_tiles, 2))\n",
    "    for zoom in range(zoom_level, -1, -1):\n",
    "        pool.apply_async(array_slice, args=(source_array, ), callback=saveResult)\n",
    "        if zoom:\n",
    "            source_array = decrease_zoom(source_array)\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "pool = Pool(processes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 1: 26.4 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "out_array = test_par(source_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(source_array):\n",
    "    number_tiles = source_array.shape[0]/256\n",
    "    zoom_level = int(math.log(number_tiles, 2))\n",
    "    out_array = np.ones([zoom_level+1, number_tiles, number_tiles, 256, 256])\n",
    "    for zoom in range(zoom_level, -1, -1):\n",
    "        m = source_array\n",
    "        m = ma.where(m <= _min, _min + 0.0001, m)\n",
    "        m = ma.where(m >= _max, _max, m)\n",
    "        number_tiles = m.shape[0]/256\n",
    "\n",
    "        for i in xrange(number_tiles):\n",
    "            for j in xrange(number_tiles):\n",
    "                out_array[zoom, i, j, :] = m[j*256:(j+1)*256, i*256:(i+1)*256]\n",
    "\n",
    "        if zoom:\n",
    "            source_array = decrease_zoom(source_array)\n",
    "  \n",
    "    return out_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 34.9 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "out_array = test(source_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print source_array.shape, out_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Tiles.nctiles\n",
    "reload(Tiles.nctiles)\n",
    "# Переделать чтобы сначала маскировался массив и перегонялся в uint8, а потом уде разбивать на тайлы\n",
    "# Может будет быстрее\n",
    "# исрользовать numba или numexpr, cpython\n",
    "# оказалось, что не быстрее\n",
    "from Tiles.nctiles import create_nc_tiles, write_attrib_to_nc, array_size_normalize"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Переделать чтобы сначала маскировался массив и перегонялся в uint8, а потом уде разбивать на тайлы\n",
    "# Может будет быстрее\n",
    "# исрользовать numba или numexpr, cpython\n",
    "# оказалось, что не быстрее\n",
    "\n",
    "def create_base_tiles(input_array):\n",
    "    number_tiles = input_array.shape[0]/256\n",
    "\n",
    "    lines = []\n",
    "    all_list = []\n",
    "\n",
    "    for i in xrange(number_tiles):\n",
    "        for j in xrange(number_tiles):\n",
    "            # print '[%d:%d' % (j*256, (j+1)*256), ',%d:%d]'%(i*256, (i+1)*256)\n",
    "            lines.append(input_array[j*256:(j+1)*256, i*256:(i+1)*256])\n",
    "\n",
    "        all_list.append(lines)\n",
    "        lines = []\n",
    "    return all_list\n",
    "\n",
    "def write_tile_to_nc(nc_variable, tiles_array, variable, zoom, _min=0, _max=4,\n",
    "                     nc_data_type='u1', polarization=None):\n",
    "    for row_number in range(len(tiles_array)):\n",
    "        tile_row = tiles_array[row_number]\n",
    "        for col_number in range(len(tile_row)):\n",
    "            m = tile_row[col_number]\n",
    "            m = ma.where(m <= _min, _min + 0.0001, m)\n",
    "            m = ma.where(m >= _max, _max, m)\n",
    "\n",
    "            # 0-254 , 255 for mask\n",
    "            if nc_data_type == 'u1':\n",
    "                m = (m-_min)/float(_max-_min) * (2**8-2)\n",
    "                m = np.where(m.mask, (2**8-1), m)\n",
    "                m = np.uint8(m)\n",
    "            elif nc_data_type == 'u2':\n",
    "                m = (m-_min)/float(_max-_min) * (2**16-2)\n",
    "                m = np.where(m.mask, (2**16-1), m)\n",
    "                m = np.uint16(m)\n",
    "            elif nc_data_type == 'u4':\n",
    "                m = (m-_min)/float(_max-_min) * (2**32-2)\n",
    "                m = np.where(m.mask, (2**32-1), m)\n",
    "                m = np.uint32(m)\n",
    "            elif nc_data_type == 'u8':\n",
    "                m = (m-_min)/float(_max-_min) * (2**64-2)\n",
    "                m = np.where(m.mask, (2**64-1), m)\n",
    "                m = np.uint64(m)\n",
    "\n",
    "def write_tile_to_nc_v2(nc_variable, m, variable, zoom, _min=0, _max=4,\n",
    "                     nc_data_type='u1', polarization=None):\n",
    "\n",
    "    m = ma.where(m <= _min, _min + 0.0001, m)\n",
    "    m = ma.where(m >= _max, _max, m)\n",
    "\n",
    "    # 0-254 , 255 for mask\n",
    "    if nc_data_type == 'u1':\n",
    "        m = (m-_min)/float(_max-_min) * (2**8-2)\n",
    "        m = np.where(m.mask, (2**8-1), m)\n",
    "        m = np.uint8(m)\n",
    "        print m.shape\n",
    "    elif nc_data_type == 'u2':\n",
    "        m = (m-_min)/float(_max-_min) * (2**16-2)\n",
    "        m = np.where(m.mask, (2**16-1), m)\n",
    "        m = np.uint16(m)\n",
    "    elif nc_data_type == 'u4':\n",
    "        m = (m-_min)/float(_max-_min) * (2**32-2)\n",
    "        m = np.where(m.mask, (2**32-1), m)\n",
    "        m = np.uint32(m)\n",
    "    elif nc_data_type == 'u8':\n",
    "        m = (m-_min)/float(_max-_min) * (2**64-2)\n",
    "        m = np.where(m.mask, (2**64-1), m)\n",
    "        m = np.uint64(m)\n",
    "\n",
    "    number_tiles = m.shape[0]/256\n",
    "    lines = []\n",
    "    all_list = []\n",
    "\n",
    "    for i in xrange(number_tiles):\n",
    "        for j in xrange(number_tiles):\n",
    "            # print '[%d:%d' % (j*256, (j+1)*256), ',%d:%d]'%(i*256, (i+1)*256)\n",
    "            lines.append(m[j*256:(j+1)*256, i*256:(i+1)*256])\n",
    "\n",
    "        all_list.append(lines)\n",
    "        lines = []\n",
    "    return all_list\n",
    "\n",
    "%%timeit -r 1 -n 1\n",
    "tiles = create_base_tiles(10*log10(sigma0_res))\n",
    "write_tile_to_nc(0, tiles, 0, 0,\n",
    "                 _min=25, _max=5,\n",
    "                 nc_data_type='u1',\n",
    "                 polarization=0)\n",
    "\n",
    "%%timeit -r 1 -n 1\n",
    "tiles = write_tile_to_nc_v2(0, 10*log10(sigma0_res), 0, 0,\n",
    "                 _min=25, _max=5,\n",
    "                 nc_data_type='u1',\n",
    "                 polarization=0)\n",
    "\n",
    "%%cython -a\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "import cython\n",
    "cimport cython\n",
    "\n",
    "# DTYPE = np.float32\n",
    "# ctypedef np.float32_t DTYPE_t\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "def create_base_tiles_2(np.ndarray[float, ndim=2] input_array):\n",
    "    cdef int number_tiles = input_array.shape[0]/256\n",
    "\n",
    "#     cdef int N = temp.size()\n",
    "#     cdef list OutputList = N*[0]\n",
    "    \n",
    "    lines = []\n",
    "    all_list = []\n",
    "\n",
    "    for i in xrange(number_tiles):\n",
    "        for j in xrange(number_tiles):\n",
    "#             print '[%d:%d' % (j*256, (j+1)*256), ',%d:%d]'%(i*256, (i+1)*256)\n",
    "            lines.append(input_array[j*256:(j+1)*256, i*256:(i+1)*256])\n",
    "\n",
    "        all_list.append(lines)\n",
    "        lines = []\n",
    "    return all_list\n",
    "\n",
    "%%timeit -r 1 -n 1\n",
    "tiles = create_base_tiles_2(sigma0_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD VERSION with big_image png tiles"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from big_image import save_big_image\n",
    "__author__ = 'Alexander Myasoedov and Denis Spiridonov'\n",
    "\n",
    "# set the number of CPUs\n",
    "numProcs = cpu_count()-2\n",
    "\n",
    "redis_conf = os.path.join(\n",
    "    os.path.dirname(\n",
    "        os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "    ),\n",
    "    'redis.conf'\n",
    ")\n",
    "\n",
    "config = ConfigParser.RawConfigParser()\n",
    "config.read(redis_conf)\n",
    "redis_host = config.get('AUTH', 'HOSTNAME')\n",
    "redis_passwd = config.get('AUTH', 'PASSWORD')\n",
    "\n",
    "r = redis.Redis(host=redis_host, password=redis_passwd)\n",
    "\n",
    "\n",
    "def windDirection(u, v):\n",
    "    U = u.ravel()\n",
    "    V = v.ravel()\n",
    "    direction = zeros(size(U))\n",
    "    for i in range(0, len(U)):\n",
    "        if U[i] >= 0 and V[i] > 0:\n",
    "            direction[i] = ((180 / pi) * atan(abs(U[i] / V[i])) + 180)\n",
    "        if U[i] < 0 and V[i] > 0:\n",
    "            direction[i] = (-(180 / pi) * atan(abs(U[i] / V[i])) + 180)\n",
    "        if U[i] >= 0 and V[i] < 0:\n",
    "            direction[i] = (-(180 / pi) * atan(abs(U[i] / V[i])) + 360)\n",
    "        if U[i] < 0 and V[i] < 0:\n",
    "            direction[i] = ((180 / pi) * atan(abs(U[i] / V[i])))\n",
    "        if V[i] == 0 and U[i] > 0:\n",
    "            direction[i] = 270\n",
    "        if V[i] == 0 and U[i] < 0:\n",
    "            direction[i] = 90\n",
    "        if V[i] == 0 and U[i] == 0:\n",
    "            direction[i] = 0\n",
    "    return reshape(direction, v.shape)\n",
    "\n",
    "\n",
    "def PR_Mouche(theta, phi):\n",
    "    A_0 = 0.00650704\n",
    "    B_0 = 0.128983\n",
    "    C_0 = 0.992839\n",
    "    A_HALF_PI = 0.00782194\n",
    "    B_HALF_PI = 0.121405\n",
    "    C_HALF_PI = 0.992839\n",
    "    A_PI = 0.00598416\n",
    "    B_PI = 0.140952\n",
    "    C_PI = 0.992885\n",
    "\n",
    "    P_0 = A_0 * exp(B_0 * theta) + C_0\n",
    "    P_HALF_PI = A_HALF_PI * exp(B_HALF_PI * theta) + C_HALF_PI\n",
    "    P_PI = A_PI * exp(B_PI * theta) + C_PI\n",
    "\n",
    "    C0 = (P_0 + P_PI + 2 * P_HALF_PI) / 4\n",
    "    C1 = (P_0 - P_PI) / 2\n",
    "    C2 = (P_0 + P_PI - 2 * P_HALF_PI) / 4\n",
    "\n",
    "    P = C0 + C1 * cos(radians(phi)) + C2 * cos(radians(2 * phi))\n",
    "    return P\n",
    "\n",
    "\n",
    "def ncepGFSmodel(startTime, lats_2, lons_2):\n",
    "    \"\"\"NCEP GFS model wind for givven time, lat/lon crop\n",
    "\n",
    "    \"\"\"\n",
    "    ncepGFSmodel = {}  # empty dict for ncepGFSmodel\n",
    "\n",
    "    iPath_wind = '/media/SOLabNFS2/store/model/ncep/gfs/'\n",
    "\n",
    "    # find the ncep gfs filename to open from ASAR filename\n",
    "    baseHour = floor((startTime.hour+3/2)/6)*6\n",
    "    baseHour = min(18, baseHour)\n",
    "    if startTime.hour-baseHour > 1.5:\n",
    "        forecastHour = 3\n",
    "    else:\n",
    "        forecastHour = 0\n",
    "\n",
    "    if startTime <= datetime.datetime(2014, 8, 19):\n",
    "        ncepFileName = 'gfs' + startTime.strftime(\"%Y%m%d\") + '/gfs.t' +\\\n",
    "                       '%.2d' % (baseHour) + 'z.master.grbf' +\\\n",
    "                       '%.2d' % (forecastHour)\n",
    "\n",
    "        try:\n",
    "            grbs = pygrib.open(iPath_wind + ncepFileName)\n",
    "        except:\n",
    "            print \"File not found %s\" % (iPath_wind + ncepFileName)\n",
    "            return False\n",
    "\n",
    "        u_wind = None\n",
    "        v_wind = None\n",
    "\n",
    "        # wind contains u=u_wind.values[:], Lats=u_wind.latlons()[0], Lons=u_wind.latlons()[1]\n",
    "        for idx, msg_info in enumerate(grbs.select()):\n",
    "            if msg_info['short_name'] == '10u':\n",
    "                u_wind = grbs.message(idx + 1)\n",
    "            elif msg_info['short_name'] == '10v':\n",
    "                v_wind = grbs.message(idx + 1)\n",
    "\n",
    "        u = u_wind.values[:]\n",
    "        v = v_wind.values[:]\n",
    "        lats_wind = u_wind.latlons()[0]\n",
    "        lons_wind = u_wind.latlons()[1]\n",
    "    else:\n",
    "        ncepFileName = 'gfs.' + startTime.strftime(\"%Y%m%d\") +\\\n",
    "                       '%.2d' % (baseHour) + '/gfs.t' + '%.2d' % (baseHour) +\\\n",
    "                       'z.master.grbf' + '%.2d' % (forecastHour) +\\\n",
    "                       '.10m.uv.grib2'\n",
    "\n",
    "        grbs = pygrib.open(iPath_wind + ncepFileName)\n",
    "\n",
    "        u_wind = grbs.message(1)\n",
    "        v_wind = grbs.message(2)\n",
    "        u = u_wind['values']\n",
    "        v = v_wind['values']\n",
    "        lats_wind = u_wind['latitudes']\n",
    "        lons_wind = u_wind['longitudes']\n",
    "        lons_wind = reshape(lons_wind, (lons_wind.shape[0]/720, 720))\n",
    "        lats_wind = reshape(lats_wind, (lats_wind.shape[0]/720, 720))\n",
    "\n",
    "    # Make sure the longitude is between -180.00 .. 179.9\n",
    "    lons_wind = map(\n",
    "        lambda x:\n",
    "        (lons_wind.ravel()[x]+180)-int((lons_wind.ravel()[x]+180)/360)*360-180,\n",
    "        range(0, lons_wind.size)\n",
    "    )\n",
    "    lons_wind = reshape(lons_wind, lats_wind.shape)\n",
    "    # plt.close('all')\n",
    "    # plt.imshow(lons_wind)\n",
    "    # plt.colorbar()\n",
    "\n",
    "#     #Make sure the latitudes is between -90.00 .. 89.9, starting from North - positive\n",
    "#     lats_wind = map(lambda x : (lats_wind.ravel()[x]+90)-int((lats_wind.ravel()[x]+90)/180)*180-90, xrange(0,lats_wind.size))\n",
    "#     lats_wind = reshape(lats_wind, lons_wind.shape)\n",
    "#     if lats_wind[0,0] < lats_wind[-1,-1]:\n",
    "#         lats_wind = flipud(lats_wind)\n",
    "#         u = flipud(u)\n",
    "#         v = flipud(v)\n",
    "#     plt.close('all')\n",
    "#     plt.imshow(lats_wind)\n",
    "#     plt.colorbar()\n",
    "\n",
    "    # find subset\n",
    "    res = findSubsetIndices(lats_2.min(), lats_2.max(), lons_2.min(),\n",
    "                            lons_2.max(), lats_wind[:, 0], lons_wind[0, :])\n",
    "    # expand subset by 1 pixel for better further pyresample\n",
    "    res[0] = res[0]-2\n",
    "    res[1] = res[1]+2\n",
    "    res[2] = res[2]-2\n",
    "    res[3] = res[3]+2\n",
    "\n",
    "    # crop the data\n",
    "    u = u[int(res[2]):int(res[3]), int(res[0]):int(res[1])]\n",
    "    v = v[int(res[2]):int(res[3]), int(res[0]):int(res[1])]\n",
    "    ncepGFSmodel['lats_wind'] = lats_wind[int(res[2]):int(res[3]),\n",
    "                                          int(res[0]):int(res[1])]\n",
    "    ncepGFSmodel['lons_wind'] = lons_wind[int(res[2]):int(res[3]),\n",
    "                                          int(res[0]):int(res[1])]\n",
    "\n",
    "    ncepGFSmodel['wind_dir'] = windDirection(u, v)\n",
    "    ncepGFSmodel['wind_speed'] = sqrt(u**2 + v**2)\n",
    "\n",
    "#     del u_wind, v_wind\n",
    "    return ncepGFSmodel\n",
    "\n",
    "\n",
    "def create_KML_asar(area_extent, savepath):\n",
    "    kml = simplekml.Kml()\n",
    "\n",
    "    pol = kml.newpolygon(name='area_extent', visibility=1)\n",
    "    pol.tessellate = 1\n",
    "\n",
    "    pol.altitudemode = 'clampToGround'\n",
    "    pol.outerboundaryis.coords = [(min(area_extent[0], area_extent[2]),\n",
    "                                   min(area_extent[1], area_extent[3])),\n",
    "                                  (max(area_extent[0], area_extent[2]),\n",
    "                                   max(area_extent[1], area_extent[3]))]\n",
    "    if type(savepath) == list:\n",
    "        for _savepath in savepath:\n",
    "            kml.save(_savepath)\n",
    "    else:\n",
    "        kml.save(savepath)\n",
    "\n",
    "\n",
    "def swath_area_def(name='Temporal SWATH EPSG Projection 4326', proj='eqc',\n",
    "                   lonlim=(-180, 180), latlim=(-90, 90), ellps=\"WGS84\",\n",
    "                   res=111.2e3, lat_ts=None, lat_0=None, lon_0=None):\n",
    "    \"\"\"\n",
    "    Convert given swath coordinates to pyresample area definition.\n",
    "    The arguments are standard for Proj:\n",
    "    name\n",
    "    proj\n",
    "    lonlim\n",
    "    latlim\n",
    "    ellipsoid\n",
    "    resolution(meters)\n",
    "    lat_ts (latitude of true scale)\n",
    "    lat_0,lon_0 is central point\n",
    "    EXAMPLE:\n",
    "\n",
    "    epsg3426 is the default one\n",
    "    for epsg3413:\n",
    "    swath_area_def(name='Temporal SWATH EPSG Projection 3413',\n",
    "                   proj='stere', lonlim=(-180,180), latlim=(30,90),\n",
    "                   ellps=\"WGS84\", res=111.2e3, lat_ts=70, lat_0=90, lon_0=-45)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    up = max(latlim)\n",
    "    down = min(latlim)\n",
    "    left = min(lonlim)\n",
    "    right = max(lonlim)\n",
    "\n",
    "    print 'up, down, left, right: ', up, down, left, right\n",
    "\n",
    "    area_id = name.replace(\" \", \"_\").lower()\n",
    "    proj_id = area_id\n",
    "\n",
    "    if proj == 'eqc':\n",
    "        p = Proj(proj=proj, llcrnrlat=up, urcrnrlat=down, llcrnrlon=left,\n",
    "                 urcrnrlon=right, ellps=ellps)\n",
    "        proj4_args = '+proj=' + str(proj) + ' ' + \\\n",
    "                     '+llcrnrlat=' + str(up) + ' ' + \\\n",
    "                     '+urcrnrlat=' + str(down) + ' ' + \\\n",
    "                     '+llcrnrlon=' + str(left) + ' ' + \\\n",
    "                     '+urcrnrlon=' + str(right) + ' ' + \\\n",
    "                     '+ellps=' + str(ellps)\n",
    "    elif lat_ts is not None and lat_0 is not None:\n",
    "        # lat_ts is latitude of true scale.\n",
    "        # lon_0,lat_0 is central point.\n",
    "        p = Proj(proj=proj, lat_0=lat_0, lon_0=lon_0,\n",
    "                 lat_ts=lat_ts, ellps=ellps)\n",
    "        proj4_args = '+proj=' + str(proj) + ' ' + \\\n",
    "                     '+lat_0=' + str(lat_0) + ' ' + \\\n",
    "                     '+lon_0=' + str(lon_0) + ' ' + \\\n",
    "                     '+lat_ts=' + str(lat_ts) + ' ' + \\\n",
    "                     '+ellps=' + str(ellps)\n",
    "    elif lon_0 is not None and lat_0 is not None and lat_ts is None:\n",
    "        # lon_0,lat_0 is central point.\n",
    "        p = Proj(proj=proj, lat_0=lat_0, lon_0=lon_0, ellps=ellps)\n",
    "        proj4_args = '+proj=' + str(proj) + ' ' + \\\n",
    "                     '+lat_0=' + str(lat_0) + ' ' + \\\n",
    "                     '+lon_0=' + str(lon_0) + ' ' + \\\n",
    "                     '+ellps=' + str(ellps)\n",
    "    elif lon_0 is None and lat_0 is None and lat_ts is None:\n",
    "        # lon_0,lat_0 is central point.\n",
    "        lat_0 = (up + down) / 2\n",
    "        lon_0 = (right + left) / 2\n",
    "        p = Proj(proj=proj, lat_0=lat_0, lon_0=lon_0, ellps=ellps)\n",
    "        proj4_args = '+proj=' + str(proj) + ' ' + \\\n",
    "                     '+lat_0=' + str(lat_0) + ' ' + \\\n",
    "                     '+lon_0=' + str(lon_0) + ' ' + \\\n",
    "                     '+ellps=' + str(ellps)\n",
    "\n",
    "    left_ex1, up_ex1 = p(left, up)\n",
    "    right_ex1, up_ex2 = p(right, up)\n",
    "    left_ex2, down_ex1 = p(left, down)\n",
    "    right_ex2, down_ex2 = p(right, down)\n",
    "\n",
    "    if proj == 'stere':\n",
    "        lon = (left+right)/2.0\n",
    "        if (lon >= 0 and lon < 90) or (lon >= -360 and lon < -270):\n",
    "            print 11111111111\n",
    "            area_extent = (\n",
    "                           min(left_ex1, left_ex2, right_ex1, right_ex2),\n",
    "                           min(down_ex1, down_ex2, up_ex1, up_ex2),\n",
    "                           max(left_ex1, left_ex2, right_ex1, right_ex2),\n",
    "                           max(down_ex1, down_ex2, up_ex1, up_ex2)\n",
    "                          )\n",
    "        elif (lon >= 90 and lon < 180) or (lon >= -270 and lon < -180):\n",
    "            print 2222222222222\n",
    "            area_extent = (\n",
    "                       max(left_ex1, left_ex2, right_ex1, right_ex2),\n",
    "                       max(down_ex1, down_ex2, up_ex1, up_ex2),\n",
    "                       min(left_ex1, left_ex2, right_ex1, right_ex2),\n",
    "                       min(down_ex1, down_ex2, up_ex1, up_ex2)\n",
    "                       )\n",
    "        elif (lon >= 180 and lon < 270) or (lon >= -180 and lon < -90):\n",
    "            print 333333333333\n",
    "            area_extent = (\n",
    "                       min(left_ex1, left_ex2, right_ex1, right_ex2),\n",
    "                       min(down_ex1, down_ex2, up_ex1, up_ex2),\n",
    "                       max(left_ex1, left_ex2, right_ex1, right_ex2),\n",
    "                       max(down_ex1, down_ex2, up_ex1, up_ex2)\n",
    "                      )\n",
    "        else:\n",
    "            print 44444444444444444\n",
    "            area_extent = (\n",
    "                       min(left_ex1, left_ex2, right_ex1, right_ex2),\n",
    "                       min(down_ex1, down_ex2, up_ex1, up_ex2),\n",
    "                       max(left_ex1, left_ex2, right_ex1, right_ex2),\n",
    "                       max(down_ex1, down_ex2, up_ex1, up_ex2)\n",
    "                      )\n",
    "    else:\n",
    "        # минимум из всех координат X, Y, максимум из всех координат X, Y\n",
    "        # Такой результат даёт правильный area_extent для 3413\n",
    "        # При этом для 4326 area_extent остаётся неизменным\n",
    "        # area_def_3413 = swath_area_def(name='Temporal SWATH EPSG\n",
    "        # Projection 3413', proj='stere', \\\n",
    "        #         lonlim=(-180,180), latlim=(30,90), ellps=\"WGS84\", res=1500, \\\n",
    "        #         lat_ts=70, lat_0=90, lon_0=-45)\n",
    "        # Area extent: (-5050747.263141337, 0.0, 0.0, 5050747.263141336)\n",
    "        area_extent = (\n",
    "                min(left_ex1, left_ex2, right_ex1, right_ex2),\n",
    "                min(up_ex1, up_ex2, down_ex1, down_ex2),\n",
    "                max(left_ex1, left_ex2, right_ex1, right_ex2),\n",
    "                max(up_ex1, up_ex2, down_ex1, down_ex2)\n",
    "        )\n",
    "\n",
    "    print 'left: ', left_ex1, left_ex2\n",
    "    print 'right: ', right_ex1, right_ex2\n",
    "    print 'up: ', up_ex1, up_ex2\n",
    "    print 'down: ', down_ex1, down_ex2\n",
    "\n",
    "    # Using abs() to avoid negative numbers of coloumns/rows\n",
    "    # as for epsg3413 for example\n",
    "    xsize = abs(int((area_extent[2] - area_extent[0]) / res))\n",
    "    ysize = abs(int((area_extent[3] - area_extent[1]) / res))\n",
    "\n",
    "    swath_area_def = pr.utils.get_area_def(area_id, name, proj_id, proj4_args,\n",
    "                                           xsize, ysize, area_extent)\n",
    "\n",
    "#     print swath_area_def\n",
    "\n",
    "    return swath_area_def\n",
    "\n",
    "\n",
    "def imresize(image, size):\n",
    "    \"\"\"\n",
    "    Resizes coefficient arrays using bivariate spline approximation.\n",
    "    \"\"\"\n",
    "    m, n = image.shape\n",
    "    X = linspace(0, m - 1, size[0])\n",
    "    Y = linspace(0, n - 1, size[1])\n",
    "    kx, ky = min([m - 1, 3]), min([n - 1, 3])\n",
    "    interp = RectBivariateSpline(\n",
    "        arange(m), arange(n), image, kx=kx, ky=ky)\n",
    "    resized = interp(X, Y)\n",
    "    return resized\n",
    "\n",
    "\n",
    "def create_asar_image(iPath, oPaths, fileName):\n",
    "    oPath_4326, oPath_3413 = oPaths\n",
    "\n",
    "    print os.path.join(iPath, fileName)\n",
    "    try:\n",
    "        product = epr.Product(os.path.join(iPath, fileName))\n",
    "    except:\n",
    "        print 'unable to read file'\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        band = product.get_band('proc_data')\n",
    "    except epr.EPRValueError:\n",
    "        print 'unable to get band \"proc_data\": epr_get_band_id: band not found'\n",
    "        return False\n",
    "\n",
    "    sc_w = product.get_scene_width()\n",
    "    sc_h = product.get_scene_height()\n",
    "\n",
    "    print 'sc_w*sc_h = ', sc_w * sc_h\n",
    "    if sc_w*sc_h > 60000000:\n",
    "#    if sc_w*sc_h > 24000000:\n",
    "        print \"ASAR Image too large, skipping\"\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        raw_counts = band.read_as_array(sc_w, sc_h)  # , xstep=4, ystep=4)\n",
    "\n",
    "        incident_angle = product.get_band('incident_angle').read_as_array(sc_w,\n",
    "                                                                          sc_h)\n",
    "        # , xstep=4, ystep=4)\n",
    "    except epr.EPRValueError:\n",
    "        print \"EPRValueError\"\n",
    "        return False\n",
    "\n",
    "    # Get lat/lon from geolocation grid\n",
    "    dataset = product.get_dataset('GEOLOCATION_GRID_ADS')\n",
    "    fltp_lats = map(\n",
    "        lambda x:\n",
    "        dataset.read_record(x).get_field('first_line_tie_points.lats').get_elems(),\n",
    "        range(dataset.get_num_records())\n",
    "    )\n",
    "    lltp_lats = map(\n",
    "        lambda x:\n",
    "        dataset.read_record(x).get_field('last_line_tie_points.lats').get_elems(),\n",
    "        range(dataset.get_num_records())\n",
    "    )\n",
    "    fltp_lons = map(\n",
    "        lambda x:\n",
    "        dataset.read_record(x).get_field('first_line_tie_points.longs').get_elems(),\n",
    "        range(dataset.get_num_records())\n",
    "    )\n",
    "    lltp_lons = map(\n",
    "        lambda x:\n",
    "        dataset.read_record(x).get_field('last_line_tie_points.longs').get_elems(),\n",
    "        range(dataset.get_num_records())\n",
    "    )\n",
    "\n",
    "    fltp_lats = asarray(double(fltp_lats))/1e6\n",
    "    lltp_lats = asarray(double(lltp_lats))/1e6\n",
    "    fltp_lons = asarray(double(fltp_lons))/1e6\n",
    "    lltp_lons = asarray(double(lltp_lons))/1e6\n",
    "\n",
    "    lats = row_stack((fltp_lats, lltp_lats[-1, :]))\n",
    "    lons = row_stack((fltp_lons, lltp_lons[-1, :]))\n",
    "\n",
    "    lats = fliplr(lats)\n",
    "    lons = fliplr(lons)\n",
    "\n",
    "    lats_2 = imresize(lats, raw_counts.shape)\n",
    "    lons_2 = imresize(lons, raw_counts.shape)\n",
    "\n",
    "    if lats.max() <= 35:\n",
    "        print \"skipping no area overlap\"\n",
    "        return False\n",
    "\n",
    "    for p in oPaths:\n",
    "        for sp in p:\n",
    "            mkdirs(sp)\n",
    "\n",
    "    # Trimming the array by removing zero values from rows and cols\n",
    "    msk = []\n",
    "    for m in range(raw_counts.shape[0]):\n",
    "        if raw_counts[m, :].sum() == 0:\n",
    "            msk.append(m)\n",
    "    raw_counts = delete(raw_counts, msk, axis=0)\n",
    "    lats_2 = delete(lats_2, msk, axis=0)\n",
    "    lons_2 = delete(lons_2, msk, axis=0)\n",
    "    incident_angle = delete(incident_angle, msk, axis=0)\n",
    "\n",
    "    msk = []\n",
    "    for n in range(raw_counts.shape[1]):\n",
    "        if raw_counts[:, n].sum() == 0:\n",
    "            msk.append(n)\n",
    "    raw_counts = delete(raw_counts, msk, axis=1)\n",
    "    lats_2 = delete(lats_2, msk, axis=1)\n",
    "    lons_2 = delete(lons_2, msk, axis=1)\n",
    "    incident_angle = delete(incident_angle, msk, axis=1)\n",
    "\n",
    "    # Adding Sigma_0\n",
    "    calibration_constant = \\\n",
    "    product.get_dataset('MAIN_PROCESSING_PARAMS_ADS').read_record(0).get_field('calibration_factors.1.ext_cal_fact').get_elems()\n",
    "    # sigma0 = 10*log10( raw_counts**2*sin(incident_angle*pi/180)/calibration_constant )\n",
    "    sigma0 = raw_counts**2*sin(incident_angle*pi/180)/calibration_constant\n",
    "\n",
    "\tfrom scipy.signal import wiener\n",
    "\tsigma0w = wiener(sigma0, mysize=(3,3), noise=None)\n",
    "\t# Lee-Wiener filtering blures to much the ASAR scenes, so we use it only for wind field\n",
    "\t# sigma0w = sigma0\n",
    "\n",
    "\t# # Earlier form Fabrice - simplyfied\n",
    "\t# pol = product.get_sph().get_field('MDS1_TX_RX_POLAR').get_elem()\n",
    "\t# if pol == 'H/H':\n",
    "\t#     ph = (2.20495, -14.3561e-2, 11.28e-4)\n",
    "\t#     sigma0_hh_ref = exp( ( ph[0]+incident_angle*ph[1]+incident_angle**2*ph[2])*log(10) )\n",
    "\t#     roughness = sigma0w/sigma0_hh_ref\n",
    "\t# elif pol == 'V/V':\n",
    "\t#     pv = (2.29373, -15.393e-2, 15.1762e-4)\n",
    "\t#     sigma0_vv_ref = exp( ( pv[0]+incident_angle*pv[1]+incident_angle**2*pv[2])*log(10) )\n",
    "\t#     roughness = sigma0w/sigma0_vv_ref\n",
    "\n",
    "\t# From sar/cerbere\n",
    "\tfrom cmod_vect import cmod5n_forward as cmod5\n",
    "\tpol = product.get_sph().get_field('MDS1_TX_RX_POLAR').get_elem()\n",
    "\tdef compute_roughness(sigma0, incidence, polarisation):\n",
    "\t    \"\"\"Compute sea surface roughness.\n",
    "\n",
    "\t    Parameters\n",
    "\t    ----------\n",
    "\t    sigma0 : ndarray\n",
    "\t        NRCS backscatter.\n",
    "\t    incidence : ndarray\n",
    "\t        Incidence angle in degrees.\n",
    "\t    polarisation : str\n",
    "\t        'VV' or 'HH' or 'VH' or 'HV'\n",
    "\n",
    "\t    Returns\n",
    "\t    -------\n",
    "\t    ndarray\n",
    "\t    \"\"\"\n",
    "\t    if polarisation == 'VV' or polarisation == 'V/V': # Use cmod5\n",
    "\t        sigma0_vv = cmod5(10, 45, incidence)\n",
    "\t        return sigma0/sigma0_vv\n",
    "\t    elif polarisation == 'HH' or polarisation == 'H/H': # Use cmod5 and Thompson polarisation ratio\n",
    "\t        sigma0_vv = cmod5(10, 45, incidence)\n",
    "\t        alpha = 0.7\n",
    "\t        polrat = (1 + 2*tan(incidence*pi/180)**2)**2 / \\\n",
    "\t                 (1 + alpha*tan(incidence*pi/180)**2)**2\n",
    "\t        return sigma0/sigma0_vv*polrat\n",
    "\t    elif polarisation == 'VH' or polarisation == 'HV' \\\n",
    "\t      or polarisation == 'V/H' or polarisation == 'H/V': # Use simple model\n",
    "\t        # nrcs_vh_db = 0.580*wsp - 35.652\n",
    "\t        # nrcs_vh_lin = 10^(nrcs_vh_db/10.)\n",
    "\t        sigma0_cross = 10**((0.58*10-35.652)/10)\n",
    "\t        return sigma0/sigma0_cross\n",
    "\t    else:\n",
    "\t        raise Exception('Unknown polarisation : '+polarisation)\n",
    "\n",
    "\troughness = compute_roughness(sigma0, incident_angle, pol)\n",
    "\n",
    "    # Adding Model wind\n",
    "    startTime = datetime.datetime.strptime(fileName[14:29], \"%Y%m%d_%H%M%S\")\n",
    "    ncepGFSmodelWind = ncepGFSmodel(startTime, lats_2, lons_2)\n",
    "    if not ncepGFSmodelWind:\n",
    "        return False\n",
    "\n",
    "    # Reprojecting data\n",
    "\n",
    "    # Pixel resolution\n",
    "    # we use pxlResWind/pxlResSAR for further pyresample radius_of_influence and sigmas\n",
    "    try:\n",
    "        pxlResWind = asarray(\n",
    "            distancelib.getPixelResolution(ncepGFSmodelWind['lats_wind'],\n",
    "                                           ncepGFSmodelWind['lons_wind'],\n",
    "                                           ncepGFSmodelWind['lats_wind'].shape,\n",
    "                                           'km')\n",
    "        )\n",
    "    except IndexError:\n",
    "        return False\n",
    "    pxlResSAR = asarray(\n",
    "        distancelib.getPixelResolution(lats_2, lons_2, lons_2.shape, 'km')\n",
    "    )*1e3\n",
    "    # Note pxlResWind is in KM, multiply by 1e3 for meters\n",
    "#    print \"ASAR cell resolution, %s m\"  % pxlResSAR\n",
    "#    print \"Wind cell resolution, %s km\" % pxlResWind\n",
    "\n",
    "    # reproject NCEP onto ASAR grid before calculations\n",
    "    # Try both BivariateSpline, griddata and pyresample\n",
    "\n",
    "    ncep_def = pr.geometry.GridDefinition(lons=ncepGFSmodelWind['lons_wind'],\n",
    "                                          lats=ncepGFSmodelWind['lats_wind'])\n",
    "    swath_def = pr.geometry.SwathDefinition(lons=lons_2, lats=lats_2)\n",
    "\n",
    "    # wind_speed_model_swath = pr.kd_tree.resample_gauss(\n",
    "    #     ncep_def, ncepGFSmodelWind['wind_speed'].ravel(), swath_def,\n",
    "    #     radius_of_influence=2*pxlResWind.max()*1e3, neighbours=12,\n",
    "    #     sigmas=pxlResWind.max()*1e3, fill_value=None, nprocs=numProcs\n",
    "    # )\n",
    "    wind_dir_model_swath = pr.kd_tree.resample_gauss(\n",
    "        ncep_def, ncepGFSmodelWind['wind_dir'].ravel(), swath_def,\n",
    "        radius_of_influence=2*pxlResWind.max()*1e3, neighbours=12,\n",
    "        sigmas=pxlResWind.max()*1e3, fill_value=None, nprocs=numProcs\n",
    "    )\n",
    "\n",
    "    # calculate bearing from initial lats/lons for further wind calculation\n",
    "    bearing = zeros((lons.shape[0]-1, lons.shape[1]))\n",
    "\n",
    "    for n in range(0, lons.shape[1]):\n",
    "        col = ([lats[:-1, n], lons[:-1, n]], [lats[1:, n], lons[1:, n]])\n",
    "        for m in range(0, lons.shape[0]-1):\n",
    "            bearing[m][n] = distancelib.bearing(asarray(col[0])[:, m],\n",
    "                                                asarray(col[1])[:, m])\n",
    "\n",
    "    # interpolate to raw_counts.shape\n",
    "    bearing_2 = imresize(bearing, raw_counts.shape)\n",
    "\n",
    "    # NB! WINDDIR = 0 WHEN WIND BLOWS TOWARDS RADAR!\n",
    "    wind_dir_model_swath_rel = 90 + bearing_2 - wind_dir_model_swath\n",
    "\n",
    "    if pol == 'H/H':\n",
    "        PR = PR_Mouche(incident_angle, wind_dir_model_swath_rel)\n",
    "        try:\n",
    "            from cmod_gpu import rcs2windOpenCl\n",
    "            wind_speed_asar = rcs2windOpenCl(sar=sigma0w*PR,\n",
    "                                             windir=wind_dir_model_swath_rel,\n",
    "                                             theta=incident_angle)\n",
    "        except Exception:\n",
    "            from cmod_vect import rcs2windPar\n",
    "            wind_speed_asar = rcs2windPar(sigma0w*PR, cmdv=5,\n",
    "                                          windir=wind_dir_model_swath_rel,\n",
    "                                          theta=incident_angle,\n",
    "                                          nprocs=numProcs)\n",
    "    elif pol == 'V/V':\n",
    "        try:\n",
    "            from cmod_gpu import rcs2windOpenCl\n",
    "            wind_speed_asar = rcs2windOpenCl(sar=sigma0w,\n",
    "                                             windir=wind_dir_model_swath_rel,\n",
    "                                             theta=incident_angle)\n",
    "        except Exception:\n",
    "            from cmod_vect import rcs2windPar\n",
    "            wind_speed_asar = rcs2windPar(sigma0w, cmdv=5,\n",
    "                                          windir=wind_dir_model_swath_rel,\n",
    "                                          theta=incident_angle,\n",
    "                                          nprocs=numProcs)\n",
    "\n",
    "    del lats, lons\n",
    "    del sigma0, raw_counts\n",
    "    del bearing\n",
    "    del ncepGFSmodelWind\n",
    "    gc.collect()\n",
    "\n",
    "    for proj in ['EPSG:4326', 'EPSG:3413']:\n",
    "        # for proj in ['EPSG:4326']:\n",
    "        print \"    start projection %s\" % proj\n",
    "        if proj == 'EPSG:4326':\n",
    "            oPath = oPath_4326\n",
    "            for outpath in oPath:\n",
    "                mkdirs(outpath)\n",
    "            area_def = swath_area_def(\n",
    "                name='Temporal SWATH EPSG Projection 4326', proj='eqc',\n",
    "                lonlim=(lons_2.min(), lons_2.max()),\n",
    "                latlim=(lats_2.min(), lats_2.max()),\n",
    "                ellps=\"WGS84\", res=pxlResSAR.max()\n",
    "            )\n",
    "            # Set the parameters for GSHHS masking\n",
    "            proj_ = '4326'\n",
    "            proj_name = None\n",
    "            units = 'deg'\n",
    "        elif proj == 'EPSG:3413':\n",
    "            oPath = oPath_3413\n",
    "            for outpath in oPath:\n",
    "                mkdirs(outpath)\n",
    "            area_def = swath_area_def(\n",
    "                name='Temporal SWATH EPSG Projection 3413', proj='stere',\n",
    "                lonlim=(lons_2.min(), lons_2.max()),\n",
    "                latlim=(lats_2.min(), lats_2.max()),\n",
    "                ellps=\"WGS84\", res=pxlResSAR.max(),\n",
    "                lat_ts=70, lat_0=90, lon_0=-45\n",
    "            )\n",
    "            # Set the parameters for GSHHS masking\n",
    "            proj_ = '+units=m +ellps=WGS84 +lon_0=-45 +proj=stere +lat_ts=70 +lat_0=90'\n",
    "            proj_name = '3413'\n",
    "            units = 'm'\n",
    "\n",
    "        print area_def\n",
    "        print \"roughness.shape = \", roughness.shape\n",
    "\n",
    "        roughness_res = pr.kd_tree.resample_nearest(\n",
    "            swath_def, roughness.ravel(), area_def,\n",
    "            radius_of_influence=pxlResSAR.max(),\n",
    "            epsilon=0.5, nprocs=numProcs, fill_value=None\n",
    "        )\n",
    "        print \"resample_nearest done\"\n",
    "\n",
    "#        shapefile = '/media/SOLabNFS/store/auxdata/coastline/GSHHS_shp/f/GSHHS_f_L1.shp'\n",
    "#        lonlim=(lons_2.min(),lons_2.max())\n",
    "#        latlim=(lats_2.min(),lats_2.max())\n",
    "#        lakes = True\n",
    "#\n",
    "#        mask_arr = gshhs_rasterize.gshhs_rasterize(lonlim, latlim, units, roughness_res.shape,\n",
    "#                                                   proj_, proj_name, lakes, shapefile)\n",
    "#        roughness_masked = ma.masked_where(mask_arr, roughness_res)\n",
    "        roughness_masked = roughness_res\n",
    "\n",
    "        oFileName = os.path.join(oPath[0], fileName+'.png')\n",
    "        gray()\n",
    "        print 'save roughness_masked image, %s' % oFileName\n",
    "        del roughness_res\n",
    "        gc.collect()\n",
    "        print '=>>', roughness_masked.shape\n",
    "        #imsave(oFileName, roughness_masked, vmin=0, vmax=2)\n",
    "        save_big_image(oFileName, roughness_masked, vmin=0, vmax=2,\n",
    "                       temp_dir='/tmp/save_image/'+fileName)\n",
    "        del roughness_masked\n",
    "        gc.collect()\n",
    "   #     del roughness_masked, roughness_res, roughness\n",
    "\n",
    "        print \"roughness_res done\"\n",
    "        print \"wind_speed_asar.shape = \", wind_speed_asar.shape\n",
    "        wind_speed_asar_res = pr.kd_tree.resample_nearest(\n",
    "            swath_def, wind_speed_asar.ravel(), area_def,\n",
    "            radius_of_influence=pxlResSAR.max(),\n",
    "            epsilon=0.5, nprocs=numProcs, fill_value=None\n",
    "        )\n",
    "        print \"wind_speed_asar_res done\"\n",
    "\n",
    "        # Apply Mask from GSHHS\n",
    "        # ESRI shapefile containing land polygons\n",
    "#        shapefile = '/media/SOLabNFS/store/auxdata/coastline/GSHHS_shp/f/GSHHS_f_L1.shp'\n",
    "#        lonlim=(lons_2.min(),lons_2.max())\n",
    "#        latlim=(lats_2.min(),lats_2.max())\n",
    "#        lakes = True\n",
    "\n",
    "#        mask_arr = gshhs_rasterize.gshhs_rasterize(lonlim, latlim, units, roughness_res.shape,\n",
    "#                                                   proj_, proj_name, lakes, shapefile)\n",
    "#        roughness_masked = ma.masked_where(mask_arr, roughness_res)\n",
    "#        wind_speed_asar_masked = ma.masked_where(mask_arr, wind_speed_asar_res)\n",
    "        wind_speed_asar_masked = wind_speed_asar_res\n",
    "\n",
    "#        oFileName = os.path.join(oPath[0], fileName+'.png')\n",
    "#        gray()\n",
    "#        imsave(oFileName, roughness_masked, vmin=0, vmax=2)\n",
    "\n",
    "        oFileNameWind = os.path.join(oPath[1], fileName+'.png')\n",
    "        jet()\n",
    "        print 'save wind_speed_asar_masked, image, %s' % oFileNameWind\n",
    "#        del wind_speed_asar_res\n",
    "        gc.collect()\n",
    "        # imsave(oFileNameWind, wind_speed_asar_masked, vmin=0, vmax=20)\n",
    "        save_big_image(oFileNameWind, wind_speed_asar_masked, vmin=0, vmax=20,\n",
    "                       temp_dir='/tmp/save_image/'+fileName)\n",
    "#        del wind_speed_asar_masked\n",
    "        gc.collect()\n",
    "    #    del wind_speed_asar_masked, wind_speed_asar_res, wind_speed_asar\n",
    "\n",
    "        for _path in oPath:\n",
    "            create_KML_asar(area_def.area_extent,\n",
    "                            os.path.join(_path, fileName+'.kml'))\n",
    "    del roughness  # , roughness_res\n",
    "    del wind_speed_asar_masked, wind_speed_asar  # , wind_speed_asar_res\n",
    "    product.close()\n",
    "    gc.collect()\n",
    "    return True\n",
    "    # gray()\n",
    "    # pr.plot.show_quicklook(area_def, result, vmin=0, vmax=2,\n",
    "    #  label='Test', num_meridians=45, num_parallels=10, coast_res='l')\n",
    "\n",
    "import gdal\n",
    "sys.path.append('/usr/bin')\n",
    "from gdal2tiles import GDAL2Tiles\n",
    "\n",
    "\n",
    "def create_asar_tiles(png_filename, tiles_output_dir, proj):\n",
    "    local_argv = ['/usr/bin/gdal2tiles.py', '-p', 'raster', '-r', 'cubic',\n",
    "                  '-s', proj, png_filename, tiles_output_dir]\n",
    "    argv = gdal.GeneralCmdLineProcessor(local_argv)\n",
    "    if argv:\n",
    "        gdal2tiles = GDAL2Tiles(argv[1:])\n",
    "        gdal2tiles.process()\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def resize_image(filepath, basewidth, savepath):\n",
    "    if not os.path.isfile(filepath):\n",
    "        return\n",
    "#    if os.path.isfile(savepath):\n",
    "#        return\n",
    "    img = Image.open(filepath)\n",
    "    wpercent = (basewidth / float(img.size[0]))\n",
    "    hsize = int(float(img.size[1]) * float(wpercent))\n",
    "    img = img.resize((basewidth, hsize), Image.ANTIALIAS)\n",
    "    img.save(savepath)\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "def send_redis_message(granule_name, tiles_output_dir, kml_file, output_filename, r):\n",
    "    kml_text = open(kml_file, 'r').read()\n",
    "    prog = re.compile(r'<coordinates>(\\S* \\S*)</coordinates>')\n",
    "    coords = prog.findall(kml_text)[0]\n",
    "    BBox_attrib = []\n",
    "    for cs in coords.split():\n",
    "        BBox_attrib.append(cs.split(',')[0])\n",
    "        BBox_attrib.append(cs.split(',')[1])\n",
    "\n",
    "    xml_info_file = os.path.join(tiles_output_dir, 'tilemapresource.xml')\n",
    "\n",
    "    tree = ET.parse(xml_info_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    units_list = []\n",
    "    for child in root:\n",
    "        if child.tag == 'TileSets':\n",
    "            for TSchild in child:\n",
    "                # print child.attrib\n",
    "                units_list.append(TSchild.attrib['units-per-pixel'])\n",
    "    resolution_list = [int(75 * math.pow(2, i))\n",
    "                       for i in range(len(units_list))]\n",
    "    resolution_list.reverse()\n",
    "    print '=======>', BBox_attrib\n",
    "    print resolution_list\n",
    "\n",
    "    message_text = {\n",
    "      'PRODUCT_NAME': 'ASAR',\n",
    "      'GRANULE_NAME': granule_name,\n",
    "      'PNG_PATH': output_filename,\n",
    "      'OUTPUT_DIRECTORY': tiles_output_dir,\n",
    "      'METADATA': {\n",
    "          'bbox': BBox_attrib,\n",
    "          'resolution': resolution_list\n",
    "          }\n",
    "    }\n",
    "    r.publish('NewGranule', output_filename)\n",
    "\n",
    "    r.publish('Metadata', message_text)\n",
    "\n",
    "fileName = 'ASA_WSM_1PNPDK20100927_195408_000000862093_00200_44843_3688.N1'\n",
    "# input_filename = '/nfs1/store/satellite/asar/2010/270/ASA_WSM_1PNPDK20100927_195408_000000862093_00200_44843_3688.N1'\n",
    "#~ asar_path = '/nfs1/store/satellite/asar'\n",
    "asar_path = '/media/SOLabNFS2/store/satellite/asar'\n",
    "pp_names = ['roughness', 'wind_speed']\n",
    "#out_path = '/media/SOLabNFS2/http/tiles/ASAR'\n",
    "out_path_w = '/media/SOLabNFS2/http/tiles/SOLAB_ASAR_WSPD'\n",
    "out_path_r = '/media/SOLabNFS2/http/tiles/SOLAB_ASAR_ROUGH'\n",
    "\n",
    "\n",
    "def mkdirs(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "roughness_flag = True\n",
    "\n",
    "\n",
    "#Start 538 granule 2012\n",
    "\n",
    "#Start 594 granule 2011\n",
    "\n",
    "\n",
    "count = 0\n",
    "offset = 593\n",
    "for _dir, sub_dir, _files in os.walk(asar_path):\n",
    "    if not '2011' in _dir:\n",
    "        continue\n",
    "    for fileName in _files:\n",
    "        if fileName.startswith('ASA_') and fileName.endswith('.N1'):\n",
    "            count += 1\n",
    "            if count <= offset:\n",
    "                continue\n",
    "            print \"Start %d granule\" % count\n",
    "#            if fileName in ['ASA_IMM_1PNPDK20080202_152546_000001902065_00369_30984_0214.N1']:\n",
    "#                continue\n",
    "            #_dir = '/media/SOLabNFS2/store/satellite/asar/2008/001'\n",
    "            #fileName = 'ASA_IMM_1PNPDK20080101_084943_000000812064_00408_30522_3741.N1'\n",
    "            # _dir = '/nfs1/store/satellite/asar/2010/270'\n",
    "            # fileName = 'ASA_WSM_1PNPDK20100927_195408_000000862093_00200_44843_3688.N1'\n",
    "            input_filename = os.path.join(_dir, fileName)\n",
    "            dt = datetime.datetime.strptime(\n",
    "                fileName[14:22], '%Y%m%d'\n",
    "            ).timetuple()\n",
    "            year = str(dt.tm_year)\n",
    "            day = str(dt.tm_yday)\n",
    "\n",
    "            if len(day) == 1:\n",
    "                day = '00' + day\n",
    "            elif len(day) == 2:\n",
    "                day = '0' + day\n",
    "            print year, day\n",
    "\n",
    "            # _dir = '/nfs1/store/satellite/asar/%s/%s' % (year, day)\n",
    "            oPath_4326_r = os.path.join(out_path_r, pp_names[0], 'epsg_4326',\n",
    "                                        year, day, fileName)\n",
    "            oPath_3413_r = os.path.join(out_path_r, pp_names[0], 'epsg_3413',\n",
    "                                        year, day, fileName)\n",
    "            oPath_4326_w = os.path.join(out_path_w, pp_names[1], 'epsg_4326',\n",
    "                                        year, day, fileName)\n",
    "            oPath_3413_w = os.path.join(out_path_w, pp_names[1], 'epsg_3413',\n",
    "                                        year, day, fileName)\n",
    "\n",
    "            print \"Start granule: %s\" % fileName\n",
    "\n",
    "            # create dirs\n",
    "#            mkdirs(oPath_4326_r)\n",
    "#            mkdirs(oPath_3413_r)\n",
    "#            mkdirs(oPath_4326_w)\n",
    "#            mkdirs(oPath_3413_w)\n",
    "\n",
    "            # check png file\n",
    "            png_4326_r_filename = os.path.join(oPath_4326_r, fileName+'.png')\n",
    "            png_3413_r_filename = os.path.join(oPath_3413_r, fileName+'.png')\n",
    "            png_4326_w_filename = os.path.join(oPath_4326_w, fileName+'.png')\n",
    "            png_3413_w_filename = os.path.join(oPath_3413_w, fileName+'.png')\n",
    "\n",
    "#            if roughness_flag:\n",
    "#                oPaths = [(oPath_4326_r, None),\n",
    "#                          (oPath_3413_r, None)]\n",
    "#            else:\n",
    "#                oPaths = [(None, oPath_4326_w),\n",
    "#                          (None, oPath_3413_w)]\n",
    "            oPaths = [(oPath_4326_r, oPath_4326_w),\n",
    "                      (oPath_3413_r, oPath_3413_w)]\n",
    "            pngPaths = [(png_4326_r_filename, png_4326_w_filename),\n",
    "                        (png_3413_r_filename, png_3413_w_filename)]\n",
    "\n",
    "#            if roughness_flag:\n",
    "#                if not os.path.isfile(png_4326_r_filename) or not os.path.isfile(png_3413_r_filename):\n",
    "#                    if not create_asar_image(_dir, oPaths, fileName):\n",
    "#                        continue\n",
    "#            else:\n",
    "#               if not os.path.isfile(png_4326_w_filename) or not os.path.isfile(png_3413_w_filename):\n",
    "#                   if not create_asar_image(_dir, oPaths, fileName, roughness_flag):\n",
    "#                       continue\n",
    "\n",
    "            if not os.path.isfile(png_4326_r_filename) or \\\n",
    "               not os.path.isfile(png_3413_r_filename) or \\\n",
    "               not os.path.isfile(png_4326_w_filename) or \\\n",
    "               not os.path.isfile(png_3413_w_filename):\n",
    "                print \"Start create image for %s\" % fileName\n",
    "                if not create_asar_image(_dir, oPaths, fileName):\n",
    "                    continue\n",
    "\n",
    "#            resize_image(png_3413_r_filename, 1024, os.path.join(oPath_3413_r, fileName+'_1024.png'))\n",
    "#            resize_image(png_3413_r_filename, 263, os.path.join(oPath_3413_r, fileName+'_263.png'))\n",
    "            # create thumbs\n",
    "            for num in range(len(oPaths)):\n",
    "                resize_image(pngPaths[num][0], 1024,\n",
    "                             os.path.join(oPaths[num][0],\n",
    "                             fileName+'_1024.png'))\n",
    "                resize_image(pngPaths[num][0], 263,\n",
    "                             os.path.join(oPaths[num][0],\n",
    "                             fileName+'_263.png'))\n",
    "                resize_image(pngPaths[num][1], 1024,\n",
    "                             os.path.join(oPaths[num][1],\n",
    "                             fileName+'_1024.png'))\n",
    "                resize_image(pngPaths[num][1], 263,\n",
    "                             os.path.join(oPaths[num][1],\n",
    "                             fileName+'_263.png'))\n",
    "\n",
    "            # check tiles\n",
    "            tiles_4326_r_output_dir = os.path.join(oPath_4326_r, 'tiles')\n",
    "            tiles_3413_r_output_dir = os.path.join(oPath_3413_r, 'tiles')\n",
    "            if not os.path.isdir(tiles_4326_r_output_dir):\n",
    "                print \"start create tiles for %s\" % tiles_4326_r_output_dir\n",
    "                create_asar_tiles(png_4326_r_filename,\n",
    "                                  tiles_4326_r_output_dir, 'EPSG:4326')\n",
    "                kml_file_4326 = os.path.join(oPath_4326_r, fileName+'.kml')\n",
    "                send_redis_message(input_filename, tiles_4326_r_output_dir, kml_file_4326,\n",
    "                                   png_4326_r_filename, r)\n",
    "            if not os.path.isdir(tiles_3413_r_output_dir):\n",
    "                print \"start create tiles for %s\" % tiles_3413_r_output_dir\n",
    "                create_asar_tiles(png_3413_r_filename,\n",
    "                                  tiles_3413_r_output_dir, 'EPSG:3413')\n",
    "                kml_file_3413 = os.path.join(oPath_3413_r, fileName+'.kml')\n",
    "                send_redis_message(input_filename, tiles_3413_r_output_dir, kml_file_3413,\n",
    "                                   png_3413_r_filename, r)\n",
    "\n",
    "            tiles_4326_w_output_dir = os.path.join(oPath_4326_w, 'tiles')\n",
    "            tiles_3413_w_output_dir = os.path.join(oPath_3413_w, 'tiles')\n",
    "\n",
    "            if not os.path.isdir(tiles_4326_w_output_dir):\n",
    "                print \"start create tiles for %s\" % tiles_4326_w_output_dir\n",
    "                create_asar_tiles(png_4326_w_filename,\n",
    "                                  tiles_4326_w_output_dir, 'EPSG:4326')\n",
    "                kml_file_4326 = os.path.join(oPath_4326_w, fileName+'.kml')\n",
    "                send_redis_message(input_filename, tiles_4326_w_output_dir,\n",
    "                                   kml_file_4326, png_4326_w_filename, r)\n",
    "            if not os.path.isdir(tiles_3413_w_output_dir):\n",
    "                print \"start create tiles for %s\" % tiles_3413_w_output_dir\n",
    "                create_asar_tiles(png_3413_w_filename,\n",
    "                                  tiles_3413_w_output_dir, 'EPSG:3413')\n",
    "                kml_file_3413 = os.path.join(oPath_3413_w, fileName+'.kml')\n",
    "                send_redis_message(input_filename, tiles_3413_w_output_dir,\n",
    "                                   kml_file_3413, png_3413_w_filename, r)\n",
    "\n",
    "            gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
